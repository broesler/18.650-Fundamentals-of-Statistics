\documentclass[letterpaper, reqno]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[comma, compress, numbers, square]{natbib}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}  % for indicator function
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{mleftright}
\usepackage{enumerate}
\usepackage{array, multirow}
\usepackage[section, ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage[font=small, hypcap=true]{caption}                % link to top of figures and subfigures
\usepackage[font=small, hypcap=true, list=true]{subcaption}   % use for subfigures instead of {subfigure}

\usepackage{hyperref}
\hypersetup{
    linktoc=all,    % link table of contents to sections
    colorlinks,
    allcolors=black,
    urlcolor=dblue,
  }

% Declare theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}{Proposition}[section]

\renewcommand{\qedsymbol}{\ensuremath{\blacksquare}}

% new environment for proofs of claims within proofs
\newenvironment{subproof}[1][\proofname]{%
  \renewcommand{\qedsymbol}{\ensuremath{\square}}%
  \begin{proof}[#1]%
}{%
  \end{proof}%
}

\numberwithin{equation}{section}

% lists as 1., 2., ...
\renewcommand{\labelenumi}{\theenumi.}

% Letter subsections
\renewcommand{\thesubsubsection}{\thesubsection(\alph{subsubsection})}

% number only specific equation in, say, align* environment
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}

% Algorithm
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
\algnewcommand\Assert{\State \textbf{assert} }

%-------------------------------------------------------------------------------
%     {listings + color} package options
%-------------------------------------------------------------------------------
% Define colors for code
\usepackage{xcolor}
\definecolor{gray}{rgb}{0.6, 0.6, 0.6}
\definecolor{mauve}{rgb}{0.58, 0, 0.82}
\definecolor{dblue}{HTML}{0645AD}
\definecolor{lblue}{HTML}{1F77B4}

\usepackage{listings}
\lstset{
  language = Python,
  basicstyle = \scriptsize\ttfamily,
  numbers = left,
  numberstyle = \tiny\color{gray},
  stepnumber = 1,
  numbersep = 8pt,
  breaklines = true,
  keywordstyle = \bfseries\color{lblue},
  commentstyle = \color{gray},
  % stringstyle = \color{mauve}
}

% Place loose figures at actual top of page
% \makeatletter
%   \setlength{\@fptop}{0pt}
% \makeatother

%-------------------------------------------------------------------------------
% NO HYPHENATION
%-------------------------------------------------------------------------------
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

%==============================================================================
% General macros
%==============================================================================
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\newcommand{\T}{\top} % transpose symbol
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}} % use BOTH to cover all cases

% Misc.
\newcommand{\ie}{\emph{i.e.\ }}
\newcommand{\eg}{\emph{e.g.\ }}

% Use for fractions with large terms
\newcommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}

% Derivatives
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ddx}[2]{\frac{\dd #1}{\dd #2}}
\newcommand{\dx}[1]{\,\dd #1} % for inside integration

% Probability helpers
\newcommand{\Prob}[1]{\mathbb{P}\mleft[#1\mright]}
\newcommand{\E}[1]{\mathbb{E}\mleft[#1\mright]}
\newcommand{\V}[1]{\mathbb{V}\mleft[#1\mright]}
\newcommand{\Var}[1]{\operatorname{Var}\mleft(#1\mright)}
\newcommand{\Cov}[1]{\operatorname{Cov}\mleft(#1\mright)}
\newcommand{\R}{\mathbb{R}}  % real numbers
\newcommand{\N}[2]{\mathcal{N}\mleft( #1, #2 \mright)}
\newcommand{\U}[2]{\mathcal{U}\mleft(\mleft[ #1, #2 \mright]\mright)}
\newcommand{\indep}{\perp \!\!\! \perp}  % "is independent from"
\newcommand{\nindep}{\centernot\indep}
\newcommand{\indic}[1]{\mathbbm{1}\!\mleft\{#1\mright\}} % indicator function
\newcommand{\iid}{i.i.d.}

\newcommand{\sumi}[2]{\sum_{#1=1}^{#2}}
\newcommand{\avg}[2]{\frac{1}{#2}\sumi{#1}{#2}}

\newcommand{\by}[1]{&\quad&\text{(#1)}}

\newcommand{\Alim}{\xrightarrow[n \to \infty]{\text{a.s.}}}
\newcommand{\Plim}{\xrightarrow[n \to \infty]{\mathbb{P}}}
\newcommand{\Dlim}{\xrightarrow[n \to \infty]{(d)}}

\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}

\newcommand{\Xnbar}{\overline{X}_n}
\newcommand{\Rnbar}{\overline{R}_n}
\newcommand{\Qnbar}{\overline{Q}_n}

\DeclareMathOperator{\Ber}{Ber}
% \DeclareMathOperator{\Var}{Var}
% \DeclareMathOperator{\Cov}{Cov}

%%%% TITLE ----------------------------
\title[Homework 7 -- Problem \thesection]{18.650 Fundamentals of Statistics\\{\large Homework 7}}
\author{Bernie Roesler}
\date{\today}
%%%%

%%%%%%% BEGIN DOCUMENT ----------------
\begin{document}
% \sloppy

\graphicspath{{./figures/}}

\maketitle
\section{Kolmogorov-Smirnov Test for Two Samples}
Consider two independent samples $X_1, \dots, X_n$, and $Y_1, \dots, Y_m$ of
independent, real-valued, continuous random variables, and assume that the $X_i$'s
are \iid\ with some cdf $F$ and that the $Y_i$'s are \iid\ with some cdf $G$.\footnote{Note that the two samples may have different sizes (if $n \ne m$).}
We want to test whether $F = G$.
Consider the following hypotheses:
\begin{align*}
  H_0 \colon ``F = G" \\
  H_1 \colon ``F \ne G"
\end{align*}
For simplicity, we will assume that $F$ and $G$ are continuous and increasing.

\subsection{Example Experiment}
An example experiment in which testing if two samples are from the same
distribution is of interest may be encountered in a lab setting where we have
two devices for measurement, and wish to determine if the errors have the same
distribution for our analysis.

\subsection{CDF Distributions}
Let
\begin{align*}
  U_i &= F(X_i), \quad \forall i = 1, \dots, n, \\
  V_j &= G(Y_j), \quad \forall j = 1, \dots, n.
\end{align*}

\begin{prop}
  The distribution of the cdf of a continuous random variable is uniform on $[0,
  1]$.
\end{prop}

\begin{proof}
The distributions of $U_i$ and $V_j$ can be determined by finding their cdfs.
The cdf of $U_i$ is defined by $F_U(t) \coloneqq \Prob{U_i \le t}$. Assuming that $F(X)$ and $G(Y)$ are invertible, it follows that
\begin{alignat*}{3}
  \Prob{U_i \le t} &= \Prob{F(X_i) \le t} \by{definition of $U_i$} \\
                   &= \Prob{X_i \le F^{-1}(t)} \\
                   &= F(F^{-1}(t)) \by{definition of cdf} \\
                   &= t \\
  \therefore F_U(t) &= t \\
  \implies f_U(t) &= \U{0}{1} \tag*{\qedhere}
\end{alignat*}
\end{proof}
Likewise, $f_V(t) = \U{0}{1}$.

\subsection{Empirical CDFs}
Let $F_n$ be the empirical cdf of $\{X_1, \dots, X_n\}$ and $G_m$ be the
empirical cdf of $\{Y_1, \dots, Y_m\}$.

\subsubsection{The Test Statistic}
Let
\[
  T_{n,m} = \sup_{t \in \R} \left| F_n(t) - G_m(t) \right|
\]

\begin{prop}
  The test statistic $T_{n,m}$ can be written as the maximum value of a finite set of numbers.
\end{prop}

\begin{proof}
  By definition, the cdf
  \begin{alignat*}{3}
    F(t) &= \Prob{X \le t} \quad \forall t \in \R \\
         &= \E{\indic{X \le t}}. \\
    \intertext{By the Law of Large Numbers, the expectation can be approximated
        by the sample average, so we can define the \emph{empirical cdf} as}
    F_n(t) &= \avg{i}{n} \indic{X_i \le t} \numberthis \label{eq:F_n}
    \intertext{Likewise,}
    G_m(t) &= \avg{j}{m} \indic{Y_j \le t}. \numberthis \label{eq:G_m}
  \end{alignat*}
  \[
    \therefore T_{n,m} = \sup_{t \in \R} \left| \avg{i}{n} \indic{X_i \le t} - \avg{j}{m} \indic{Y_j \le t} \right|.
  \]
  The empirical cdfs~\eqref{eq:F_n}~and~\eqref{eq:G_m} can also be written
  \begin{alignat}{3}
    F_n(t) &= \#\{i=1, \dots, n \colon X_i \le t\} \cdot \frac{1}{n} \\
    G_m(t) &= \#\{i=1, \dots, m \colon Y_j \le t\} \cdot \frac{1}{m},
  \end{alignat}
  so the only values that the empirical cdfs can take are the discrete sets
  \begin{align}
    F_n(i) &= \frac{i}{n} \quad \forall i = 1, \dots, n \\
    G_m(j) &= \frac{j}{m} \quad \forall j = 1, \dots, m.
  \end{align}
  Therefore, the test statistic can be rewritten as the maximum value of
  a finite set of numbers:
  \[
    \begin{split}
      T_{n,m} = \max_{i=0,\dots,n} \Bigg[
      &\max_{j=0,\dots,m} \left| \frac{i}{n} - \frac{j}{m} \right|
        \indic{Y^{(j)} \le X^{(i)} < Y^{(j+1)}}, \\
      &\max_{k=j+1, \dots, m} \left| \frac{i}{n} - \frac{k}{m} \right|
        \indic{Y^{(k)} \le X^{(i+1)}} \Bigg]
    \end{split}
  \]
  where $X^{(i)}$ is the $i^\text{th}$ value in the ordered set of data
  $X^{(1)} \le \cdots \le X^{(n)}$. The values $X^{(0)}, Y^{(0)} \coloneqq -\infty$
  are prepended to the otherwise finite realizations to simplify the
  computation.
\end{proof}

\clearpage
The following algorithm calculates the KS test statistic for two given samples.

\begin{algorithm}[H]
  \caption{Calculate the KS test statistic $T_{n,m}$ for two samples.}
  \label{alg:ks_stat}
  \begin{algorithmic}[1]
    \Require $X, Y$ are vectors of real numbers.
    \Ensure $0 \le T_{n,m} \le 1$.
    \Procedure{KS2Sample}{$X, Y$}
      \State $X_s \gets \{-\infty,$ \Call{Sort}{$X$}$\}$
      \State $Y_s \gets$ \Call{Sort}{$Y$}
      \State $n \gets \dim X_s$
      \State $m \gets \dim Y_s$
      \State $T_v \gets$ empty array of size $n$
      \ForAll{$i \in \{0, \dots, n\}$}
        \State $j \gets j$ + \Call{Rank}{$\{Y_s^{(\ell)}\}_{\ell=j}^m, X_s^{(i)}$} \Comment{Only search remaining $j$ values}
        \State $k \gets j$ + \Call{Rank}{$\{Y_s^{(\ell)}\}_{\ell=j}^m, X_s^{(\min(i+1, n))}$}
        \State $\displaystyle{T_v^{(i)} \gets
          \max\mleft(\left|\frac{i}{n} - \frac{j}{m}\right|,
                \left|\frac{i}{n} - \frac{k}{m}\right|\mright)}$
      \EndFor
      \State\Return $\max_i T_v$
    \EndProcedure
    \Function{Rank}{$A, k$}
      % \State {\bfseries assert} $A$ is sorted in ascending order.
      \Assert $A$ is sorted in ascending order.
      \State\Return $\#\{i=1,\dots,\dim A \colon k < A_i\}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\clearpage
The following subroutine is an implementation of Algorithm~\ref{alg:ks_stat}. It
computes an array of values $T_v(i)$ for each value of $X_i$. The test statistic
$T_{n,m}$ is the maximum of these values.
\lstinputlisting[language=python,
  rangeprefix=\#\ <<,
  rangesuffix=>>,
  includerangemarker=false,
  linerange=begin__ks_2samp-end__ks_2samp
  ]{../hw7_kstest.py}

\clearpage
An example two-sample KS-test is shown in Figure~\ref{fig:ks_test}.
\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\textwidth]{ks_test.pdf}
  \caption{The empirical cdfs of two independent random samples from $\N{0}{1}$ and $\N{0}{2}$. The test statistic $T_{n,m}$ is shown by the double arrow.}
  \label{fig:ks_test}
\end{figure}

\subsubsection{The Null Hypothesis}
\begin{prop}
  If $H_0$ is true, then the test statistic
  \[ 
    T_{n,m} = \sup_{0 \le x \le 1} \left|
        \avg{i}{n} \indic{U_i \le x}
      - \avg{j}{m} \indic{V_j \le x} 
    \right|.
  \]
\end{prop}

\begin{proof}
  By~\eqref{eq:F_n}~and~\eqref{eq:G_m},
  \begin{equation} \label{eq:Tnm_supt}
    T_{n,m} = \sup_{t \in \R} \left| \avg{i}{n} \indic{X_i \le t} - \avg{j}{m} \indic{Y_j \le t} \right|.
  \end{equation}
  To show the proposition is true, we make a change of variable. Let
    \[ x = F(t). \]
  Then,
    \[ t \in \R \implies x \in [0, 1]. \]
    Since $F$ and $G$ are continuous and monotonically increasing,
  \begin{alignat*}{3}
    X_i \le t &\iff F(X_i) \le F(t) \\
              &\iff U_i \le x \by{definition}.
  \end{alignat*}
  Similarly,
  \begin{alignat*}{3}
    Y_i \le t &\iff G(Y_i) \le G(t) \\
              &\iff G(Y_i) \le F(t) \by{under $H_0$} \\
              &\iff V_i \le x \by{definition}.
  \end{alignat*}
  Substitution of these expressions into~\eqref{eq:Tnm_supt} completes the
  proof.
\end{proof}

\subsubsection{The Joint Distribution of the Samples}
\begin{prop} \label{prop:Tnm}
  If $H_0$ is true, the joint distribution of $U_1, \dots, U_n, V_1, \dots, V_m$
  $(n+m)$ random variables is uniform on $[0, 1]$.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \Prob{U_i \le t} &= \Prob{F(X_i) \le t} \\
                     &= \Prob{F(X_1) \le t} \by{\iid} \\
                     &= \Prob{G(X_1) \le t} \by{under $H_0$} \\
                     &= \Prob{G(Y_1) \le t} \by{\iid} \\
                     &= \Prob{V_1 \le t} \by{definition} \\
    \intertext{These probabilities can be rearranged to find the cdfs of $U$ and $V$}
                     &= \Prob{X_1 \le F^{-1}(t)} \\
                     &= F(F^{-1}(t)) \by{definition of cdf} \\
                     &= t \\
    \therefore F_U(t) &= G_V(t) = t \\
    \implies f_{U,V}(t) &= \U{0}{1} \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\subsubsection{The Test Statistic is Pivotal}
Since Proposition~\ref{prop:Tnm} has been shown to be true under the null hypothesis $H_0$, and the distributions of $U_i$
and $V_j$ have been shown to be $\U{0}{1}$ independent of the distributions of
the underlying samples $X_i$, $Y_j$, we conclude that $T_{n,m}$ is
\emph{pivotal}, \ie it does not itself depend on the unknown distributions of
the samples.

\clearpage
\subsubsection{Quantiles of the Test Statistic}
Let $\alpha \in (0, 1)$ and $q_\alpha$ be the $(1 - \alpha)$-quantile of the
distribution of $T_{n,m}$ under $H_0$. The quantile $q_\alpha$ is given by
\begin{align*}
  q_\alpha &= F^{-1}(1-\alpha) \\
           &= \inf\{x \colon F(x) \ge 1 - \alpha\} \\
           &\approx \min\{x \colon F_n(x) \ge 1 - \alpha\}, \quad n < \infty \\
           \implies q_\alpha \approx \hat{q}_\alpha &= \min_i \left\{
               T_{n,m}^{(i)} \colon \tfrac{i}{M} \ge 1 - \alpha \right\}
\end{align*}
where $M \in \mathbb{N}$ is large, and $T_{n,m}^{(i)}$ is the $i^\text{th}$
value in a sorted sample of $M$ test statistics. Thus, $q_\alpha$ can be
approximated by choosing $i = \ceil{M(1 - \alpha)}$. An algorithm to approximate
$q_\alpha$ given $\alpha$ is as follows.

\begin{algorithm}[H]
  \caption{Approximate $q_\alpha$, the $(1 - \alpha)$-quantile of the distribution of $T_{n,m}$ under $H_0$.}
  \label{alg:ks_q}
  \begin{algorithmic}
    \Require $n = \dim X$, $m = \dim Y$, $M \in \mathbb{N}$, and $\alpha \in (0, 1)$.
    \Ensure $q_\alpha \in [0, 1]$.
    \Procedure{KSQuantile}{$n, m, M, \alpha$}
      \State $T_v \gets$ empty array of size $n$
      \ForAll{$i \in \{0,\dots,M\}$}
        \State $X_s \gets$ sample of size $n$ from $\N{0}{1}$.
        \State $Y_s \gets$ sample of size $m$ from $\N{0}{1}$.
        \State $T_v^{(i)} \gets$ \Call{KS2Sample}{$X_s, Y_s$} \Comment{defined in Algorithm~\ref{alg:ks_stat}}
      \EndFor
      \State $T_{vs} \gets$ \Call{Sort}{$T_v$}
      \State $j \gets \ceil*{M(1 - \alpha)}$
      \State \Return $T_{vs}^{(j)}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

A plot of the distribution of
\[ \frac{T_{n,m}^M - \overline{T}_{n,m}^M}{\sqrt{\Var{T_{n,m}^M}}} \]
is shown in Figure~\ref{fig:Tnm} in comparison to a standard normal. The test
statistic distribution is skewed to the left, and has a longer right tail than
the standard normal.
Since the asymptotic distribution of the test statistic is not readily found in
theory, we rely on simulation via Algorithm~\ref{alg:ks_q} to estimate the
quantiles.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.95\textwidth]{ks_dist.pdf}
  \caption{Empirical distribution of samples of the test statistic $T_{n,m}$.}
  \label{fig:Tnm}
\end{figure}

\subsubsection{The Hypothesis Test}
Given the aproximation for $\hat{q}_\alpha$ for $q_\alpha$ from
Algorithm~\ref{alg:ks_q}, we define a test with non-asymptotic level $\alpha$
for $H_0$ vs.\ $H_1$:
\[
  \delta_\alpha = \indic{T_{n,m} > \hat{q}_\alpha^{(n, M)}}
\]
where $T_{n,m}$ is found by Algorithm~\ref{alg:ks_stat}. The p-value for this
test is
\begin{align}
  \text{p-value} &\coloneqq \Prob{Z \ge T_{n,m}} \\
  &\approx \frac{\#\{j = 1, \dots, M \colon T_{n,m}^{(j)} \ge T_{n,m}\}}{M}
\end{align}
where $Z$ is a random variable distributed as $T_{n,m}$.

\clearpage
\end{document}
