\documentclass[letterpaper, oneside, reqno]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[comma, compress, numbers, square]{natbib}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}  % for indicator function
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{mleftright}
\usepackage{enumerate}
\usepackage{array, multirow}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage[font=small, hypcap=true]{caption}                % link to top of figures and subfigures
\usepackage[font=small, hypcap=true, list=true]{subcaption}   % use for subfigures instead of {subfigure}

\usepackage{hyperref}
\hypersetup{
    linktoc=all,    % link table of contents to sections
    colorlinks,
    allcolors=black }

% Declare theorem environments
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]

\numberwithin{equation}{section}

% lists as 1., 2., ...
\renewcommand{\labelenumi}{\theenumi.}

% Letter subsections
\renewcommand{\thesubsubsection}{\thesubsection(\alph{subsubsection})}

% Algorithm
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}

%-------------------------------------------------------------------------------
%     {listings + color} package options
%-------------------------------------------------------------------------------
% Define colors for code
\usepackage{xcolor}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}
\definecolor{mauve}{rgb}{0.58, 0, 0.82}
\definecolor{lblue}{HTML}{1F77B4}

\usepackage{listings}
\lstset{
  language = Python,
  basicstyle = \scriptsize\ttfamily,
  numbers = left,
  numberstyle = \tiny\color{gray},
  stepnumber = 1,
  numbersep = 8pt,
  breaklines = true,
  keywordstyle = \bfseries\color{lblue},
  commentstyle = \color{gray},
  % stringstyle = \color{mauve}
}

% Place loose figures at actual top of page
% \makeatletter
%   \setlength{\@fptop}{0pt}
% \makeatother

%-------------------------------------------------------------------------------
% NO HYPHENATION
%-------------------------------------------------------------------------------
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

%==============================================================================
% General macros
%==============================================================================
\renewcommand{\(}{\mleft(}
\renewcommand{\)}{\mright)}
\renewcommand{\[}{\mleft[}
\renewcommand{\]}{\mright]}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand{\T}{\top} % transpose symbol
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}} % use BOTH to cover all cases

% Misc.
\newcommand{\ie}{\emph{i.e.\ }}

% Use for fractions with large terms
\newcommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}

% Derivatives
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ddx}[2]{\frac{\dd #1}{\dd #2}}
\newcommand{\dx}[1]{\,\dd #1} % for inside integration

% Probability helpers
\newcommand{\Prob}[1]{\mathbb{P}\[#1\]}
\newcommand{\E}[1]{\mathbb{E}\[#1\]}
\newcommand{\V}[1]{\mathbb{V}\[#1\]}
\newcommand{\Var}[1]{\operatorname{Var}\(#1\)}
\newcommand{\Cov}[1]{\operatorname{Cov}\(#1\)}
\newcommand{\R}{\mathbb{R}}  % real numbers
\newcommand{\N}[2]{\mathcal{N}\( #1, #2 \)}
\newcommand{\U}[2]{\mathcal{U}\(\[ #1, #2 \]\)}
\newcommand{\indep}{\perp \!\!\! \perp}  % "is independent from"
\newcommand{\nindep}{\centernot\indep}
\newcommand{\indic}[1]{\mathbbm{1}\!\mleft\{#1\mright\}} % indicator function
\newcommand{\iid}{i.i.d.}

\newcommand{\sumi}[2]{\sum_{#1=1}^{#2}}
\newcommand{\avg}[2]{\frac{1}{#2}\sumi{#1}{#2}}

\newcommand{\by}[1]{&\quad&\text{(#1)}}

\newcommand{\Alim}{\xrightarrow[n \to \infty]{\text{a.s.}}}
\newcommand{\Plim}{\xrightarrow[n \to \infty]{\mathbb{P}}}
\newcommand{\Dlim}{\xrightarrow[n \to \infty]{(d)}}

\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}

\DeclareMathOperator{\Ber}{Ber}
% \DeclareMathOperator{\Var}{Var}
% \DeclareMathOperator{\Cov}{Cov}

%%%% TITLE ----------------------------
\title[Homework 7 -- Problem \thesection]{18.650 Fundamentals of Statistics\\{\large Homework 7}}
\author{Bernie Roesler}
\date{\today}
%%%%

%%%%%%% BEGIN DOCUMENT ----------------
\begin{document}
% \sloppy

\graphicspath{{./figures/}}

\maketitle

\section{QQ-Plots}
Consider the QQ-plots of five \iid\ random variables with the following
distributions:
\begin{enumerate}
\item Standard Normal, $\N{0}{1}$,
  \item Uniform distribution, $\U{-\sqrt{3}}{\sqrt{3}}$,
  \item Cauchy distribution $\sim g(x) = \frac{1}{\pi}\frac{2}{1+x^2}$,
  \item Exponential distribution $\sim \operatorname{Exp}(\lambda) = \lambda
    e^{-\lambda x}, \lambda = 1$,
  \item Laplace distribution $\sim \operatorname{Laplace}(\lambda)
    = \frac{\lambda}{2} e^{-\lambda x}, \lambda = \sqrt{2}$.
\end{enumerate}
Figure \ref{fig:qqplots} shows the samples labeled with the appropriate
distribution.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{qqplots.pdf}
  \caption{QQ-plots of five \iid\ random variables from different distributions.}
  \label{fig:qqplots}
\end{figure}

\clearpage
\section{Kolmogorov-Smirnov Test for Two Samples}
Consider two independent samples $X_1, \dots, X_n$, and $Y_1, \dots, Y_m$ of
independent, real-valued, continuous random variables, and assume that the $X_i$'s
are \iid\ with some cdf $F$ and that the $Y_i$'s are \iid\ with some cdf $G$. Note that the
two samples may have different sizes (if $n \ne m$). We want to test whether $F = G$.
Consider the following hypotheses:
\begin{align*}
  H_0 \colon ``F = G" \\
  H_1 \colon ``F \ne G"
\end{align*}
For simplicity, we will assume that $F$ and $G$ are continuous and increasing.

\subsection{Example Experiment}
An example experiment in which testing if two samples are from the same
distribution is of interest may be encountered in a lab setting where we have
two devices for measurement, and wish to determine if the errors have the same
distribution for our analysis. 

\subsection{CDF Distributions}
Let 
\begin{align*}
  U_i &= F(X_i), \quad \forall i = 1, \dots, n, \\
  V_j &= G(Y_j), \quad \forall j = 1, \dots, n.
\end{align*}

\begin{prop}
  The distribution of the cdf of a continuous random variable is uniform on $[0,
  1]$.
\end{prop}

\begin{proof}
The distributions of $U_i$ and $V_j$ can be determined by finding their cdfs.
The cdf of $U_i$ is defined by $F_U(t) \coloneqq \Prob{U_i \le t}$. Assuming that $F(X)$ and $G(Y)$ are invertible, it follows that
\begin{alignat*}{3}
  \Prob{U_i \le t} &= \Prob{F(X_i) \le t} \by{definition of $U_i$} \\
                   &= \Prob{X_i \le F^{-1}(t)} \\
                   &= F(F^{-1}(t)) \by{definition of cdf} \\
                   &= t \\
  \therefore F_U(t) &= t \\
  \implies f_U(t) &= \U{0}{1} \qedhere
\end{alignat*}
\end{proof}
Likewise, $f_V(t) = \U{0}{1}$.

\subsection{Empirical CDFs}
Let $F_n$ be the empirical cdf of $\{X_1, \dots, X_n\}$ and $G_m$ be the
empirical cdf of $\{Y_1, \dots, Y_m\}$.

\subsubsection{The Test Statistic}
Let
\begin{equation}
  T_{n,m} = \sup_{t \in \R} \left| F_n(t) - G_m(t) \right|
\end{equation}

\begin{prop}
  The test statistic $T_{n,m}$ can be written as the maximum value of a finite set of numbers.
\end{prop}

\begin{proof}
  By definition, the cdf
  \begin{alignat}{3}
    F(t) &= \Prob{X \le t} \quad \forall t \in \R \by{$X_i$'s are \iid} \\
         &= \E{\indic{X \le t}} \nonumber \\
    \avg{i}{n}(\cdot) & \Dlim \E{\cdot} \by{LLN} \nonumber \\
    \implies F_n(t) &= \avg{i}{n} \indic{X_i \le t} \label{eq:F_n}
    \intertext{Likewise,}
    G_m(t) &= \avg{j}{m} \indic{Y_j \le t}. \label{eq:G_m}
  \end{alignat}
  \begin{equation}
    \therefore T_{n,m} = \sup_{t \in \R} \left| \avg{i}{n} \indic{X_i \le t} - \avg{j}{m} \indic{Y_j \le t} \right|.
  \end{equation}
  The empirical cdfs~\eqref{eq:F_n}~and~\eqref{eq:G_m} can also be written
  \begin{alignat}{3}
    F_n(t) &= \#\{i=1, \dots, n \colon X_i \le t\} \cdot \frac{1}{n} \\
    G_m(t) &= \#\{i=1, \dots, m \colon Y_j \le t\} \cdot \frac{1}{m},
  \end{alignat}
  so the only values that the empirical cdfs can take are the discrete sets
  \begin{align}
    F_n(i) &= \frac{i}{n} \quad \forall i = 1, \dots, n \\
    G_m(j) &= \frac{j}{m} \quad \forall j = 1, \dots, m.
  \end{align}
  Therefore, the test statistic can be rewritten as the maximum value of
  a finite set of numbers:
  \begin{equation}
    \begin{split}
      T_{n,m} = \max_{i=0,\dots,n} \Bigg[
      &\max_{j=0,\dots,m} \left| \frac{i}{n} - \frac{j}{m} \right| \indic{Y_j \le X_i < Y_{j+1}}, \\ 
      &\max_{k=j+1, \dots, m} \left| \frac{i}{n} - \frac{k}{m} \right| \indic{Y_k \le X_{i+1}} \Bigg]
    \end{split}
  \end{equation}
  where the value $X_0 \coloneqq -\infty$ is prepended to the otherwise finite realizations to simplify the algorithm.
\end{proof}

\clearpage
The following algorithm calculates the KS test statistic for two given samples.
\begin{algorithm}[!h]
  \caption{Calculate the KS test statistic $T_{n,m}$ for two samples.}
  \label{alg:ks_stat}
  \begin{algorithmic}[1]
    \Require $X, Y$ are 1-D arrays of real numbers.
    \Ensure $0 \le T_{n,m} \le 1$.
    \Function{Rank}{$A, k$}
      \State {\bfseries assert} $A$ is sorted in ascending order.
      \State\Return $\#\{i=1,\dots,\text{length}(A) \colon k < A_i\}$
    \EndFunction
    \Procedure{KS2Sample}{$X, Y$}
    \State $X_s \gets \{-\infty, \text{sort}(X)\}$
    \State $Y_s \gets$ sort($Y$)
    \State $n \gets$ length($X_s$)
    \State $m \gets$ length($Y_s$)
    \State $T_v \gets$ empty array of length $n$
    \ForAll{$i \in \{0, \dots, n-1\}$}
      \State $j \gets$ \Call{rank}{$Y_s, X_s^{(i)}$} + 1
      \State $k \gets$ \Call{rank}{$\{Y_s^{(\ell)}\}_{\ell=j+1}^m, X_s^{(i+1)}$}
      + (j+1)
      \State $\displaystyle{T_v^{(i)} \gets 
        \max\(\left|\frac{i}{n} - \frac{j}{m}\right|,
              \left|\frac{i}{n} - \frac{k}{m}\right|\)}$
    \EndFor
    \State\Return $\displaystyle{\max_i T_v}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\clearpage
The following subroutine is an implementation of Algorithm~\ref{alg:ks_stat}. It computes an array of values $T_v(i)$ for each value of $X_i$. The test statistic $T_{n,m}$ is the maximum of these values.
\lstinputlisting[language=python, firstline=76, lastline=130]{../hw7_kstest.py}

\clearpage
An example two-sample KS-test is shown in Figure~\ref{fig:ks_test}.
\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\textwidth]{ks_test.pdf}
  \caption{The empirical cdfs of two independent random samples from $\N{0}{1}$ and $\N{0}{2}$. The test statistic $T_{n,m}$ is shown by the double arrow.}
  \label{fig:ks_test}
\end{figure}

\subsubsection{The Null Hypothesis}
\begin{prop}
  If $H_0$ is true, then
  $$ T_{n,m} = \sup_{0 \le x \le 1} \left| \avg{i}{n} \indic{U_i \le x}
- \avg{j}{m} \indic{V_j \le x} \right|. $$
\end{prop}

\begin{proof}
  By~\eqref{eq:F_n}~and~\eqref{eq:G_m},
  \begin{equation} \label{eq:Tnm_supt}
    T_{n,m} = \sup_{t \in \R} \left| \avg{i}{n} \indic{X_i \le t} - \avg{j}{m} \indic{Y_j \le t} \right|.
  \end{equation}
  To show the proposition is true, we make a change of variable. Let
    $$ x = F(t). $$
  Then,
    $$ t \in \R \implies x \in [0, 1]. $$
    Since $F$ and $G$ are continuous and monotonically increasing,
  \begin{alignat*}{3}
    X_i \le t &\iff F(X_i) \le F(t) \\
              &\iff U_i \le x \by{definition}.
  \end{alignat*}
  Similarly,
  \begin{alignat*}{3}
    Y_i \le t &\iff G(Y_i) \le G(t) \\
              &\iff G(Y_i) \le F(t) \by{under $H_0$} \\
              &\iff V_i \le x \by{definition}.
  \end{alignat*}
  Substitution of these expressions into~\eqref{eq:Tnm_supt} completes the
  proof. 
\end{proof}

\subsubsection{The Joint Distribution of the Samples}
\begin{prop} \label{prop:Tnm}
  If $H_0$ is true, the joint distribution of $U_1, \dots, U_n, V_1, \dots, V_m$
  $(n+m)$ random variables is uniform on $[0, 1]$.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \Prob{U_i \le t} &= \Prob{F(X_i) \le t} \\
                     &= \Prob{F(X_1) \le t} \by{\iid} \\
                     &= \Prob{G(X_1) \le t} \by{under $H_0$} \\
                     &= \Prob{G(Y_1) \le t} \by{\iid} \\
                     &= \Prob{V_1 \le t} \by{definition} \\
    \intertext{These probabilities can be rearranged to find the cdfs of $U$ and $V$}
                     &= \Prob{X_1 \le F^{-1}(t)} \\
                     &= F(F^{-1}(t)) \by{definition of cdf} \\
                     &= t \\
    \therefore F_U(t) &= G_V(t) = t \\
    \implies f_{U,V}(t) &= \U{0}{1} \qedhere
  \end{alignat*}
\end{proof}

\subsubsection{The Test Statistic is Pivotal}
Since Proposition~\ref{prop:Tnm} has been shown to be true under the null hypothesis $H_0$, and the distributions of $U_i$
and $V_j$ have been shown to be $\U{0}{1}$ independent of the distributions of
the underlying samples $X_i$, $Y_j$, we conclude that $T_{n,m}$ is
\emph{pivotal}, \ie it does not itself depend on the unknown distributions of
the samples.

\clearpage
\subsubsection{Quantiles of the Test Statistic}
Let $\alpha \in (0, 1)$ and $q_\alpha$ be the $(1 - \alpha)$-quantile of the
distribution of $T_{n,m}$ under $H_0$. The quantile $q_\alpha$ is given by
\begin{align}
  q_\alpha &= F^{-1}(1-\alpha) \\
           &= \inf\{x \colon F(x) \ge 1 - \alpha\}, \quad 0 \le 1 - \alpha \le
           1 \\
           \implies q_\alpha \approx \hat{q}_\alpha &= \min_i \left\{ T_{n,m}^{(i)} \colon \tfrac{i}{M} \ge 1 - \alpha \right\}
\end{align}
where $M \in \mathbb{N}$ is large, and $T_{n,m}^{(i)}$ is the $i^\text{th}$
value in a sorted sample of $M$ test statistics. Thus, $q_\alpha$ can be approximated by choosing $i = \ceil{M\alpha}$. An algorithm to approximate $q_\alpha$
given $\alpha$ is as follows.

\begin{algorithm}[!h]
  \caption{Approximate $q_\alpha$, the $(1 - \alpha)$-quantile of the
distribution of $T_{n,m}$ under $H_0$.}
  \label{alg:ks_q}
  \begin{algorithmic}
    \Require $X, Y$ are 1-D arrays of real numbers. $M \in \mathbb{N}$. $\alpha
    \in (0, 1)$.
    \Ensure $q_\alpha \in [0, 1]$.
    \Procedure{KSQuantile}{$X, Y, M, \alpha$}
      \State $n \gets$ length of $X$
      \State $m \gets$ length of $Y$
      \State $T_v \gets$ empty array of length $n$
      \ForAll{$i \in \{0,\dots,M\}$}
        \State $X_s \gets$ sample of size $n$ from $\N{0}{1}$.
        \State $Y_s \gets$ sample of size $m$ from $\N{0}{1}$.
        \State $T_v^{(i)} \gets$ \Call{KS2Sample}{$X_s, Y_s$}
      \EndFor
      \State $T_{vs} \gets$ \Call{sort}{$T_v$}
      \State $j \gets \ceil*{M\alpha}$
      \State \Return $T_{vs}^{(j)}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsubsection{The Hypothesis Test}
Given the aproximation for $\hat{q}_\alpha$ for $q_\alpha$ from
Algorithm~\ref{alg:ks_q}, we define a test with non-asymptotic level $\alpha$
for $H_0$ vs.\ $H_1$:
\begin{equation}
  \delta_\alpha = \indic{T_{n,m} > \hat{q}_\alpha^{(n, M)}}
\end{equation}
where $T_{n,m}$ is found by Algorithm~\ref{alg:ks_stat}. The p-value for this
test is
\begin{align}
  \text{p-value} &\coloneqq \Prob{Z \ge T_{n,m}} \\
  &\approx \frac{\#\{j = 1, \dots, M \colon T_{n,m}^{(j)} \ge T_{n,m}\}}{M}
\end{align}
where $Z$ is a random variable distributed as $T_{n,m}$.

\clearpage
\section{Aside: Homework 6, Problem 3 -- Test of Independence for Bernoulli Random Variables}
Let $X, Y$ be two Bernoulli random variables, not necessarily independent, and
let $p = \Prob{X=1}$, $q = \Prob{Y=1}$, and $r = \Prob{X=1, Y=1}$.

\subsection{Condition for Independence}
\begin{prop}
  $X \indep Y \iff r = pq$.
\end{prop}

\begin{proof}
  Two random variables are independent iff 
  \begin{equation}
    \Prob{X \cap Y} = \Prob{X}\Prob{Y} \label{eq:indep}
    % \iff \Prob{X} = \frac{\Prob{X,Y}}{\Prob{Y}} = \Prob{X | Y}
  \end{equation}
  By definition,
  \begin{align}
    \Prob{X \cap Y} &= \Prob{X=1, Y=1} = r \\
    \Prob{X} &= \Prob{X=1} = p \\
    \Prob{Y} &= \Prob{Y=1} = q \\
    \therefore r = pq &\iff \Prob{X=1,Y=1} = \Prob{X}\Prob{Y} \\
    &\iff X \indep Y  \qedhere
  \end{align}
\end{proof}

\subsection{Test for Independence}
Let $(X_1, Y_1), \dots, (X_n, Y_n)$ be a sample of $n$ \iid\ copies of $(X, Y)$
(\ie $X_i \indep X_j$ for $i \ne j$, but $X_i$ may not be independent of
$Y_i$). Based on this sample, we want to test whether $X \indep Y$, \ie whether $r = pq$.

\subsubsection{Estimators of $p, q, r$}
Define the estimators:
\begin{align*}
  \phat &= \avg{i}{n} X_i, \\
  \qhat &= \avg{i}{n} Y_i, \\
  \rhat &= \avg{i}{n} X_i Y_i.
\end{align*}

\begin{prop}
  These estimators $\phat$, $\qhat$, and $\rhat$ are consistent estimators of the true values $p$, $q$, and $r$.
\end{prop}

\begin{proof}
  To show that an estimator is \emph{consistent}, we must prove that it
  converges to the true value of the parameter in the limit as $n \to \infty$.
  By the Law of Large Numbers,
  \begin{alignat*}{3}
    \avg{i}{n} X_i &\Plim \E{X}, \by{LLN} \\
    \E{X} &= \E{\Ber(p)} \by{given} \\
          &= p \by{definition of Bernoulli r.v.} \\
    \therefore \avg{i}{n} X_i &\Plim p \\
    \implies \phat &\Plim p.
  \end{alignat*}
  Likewise $\qhat \Plim q$.

  To show that $\rhat$ converges to $r$, let $R \coloneqq X Y$ be
  a Bernoulli random variable with parameter $r = \Prob{X, Y}$, so that the
  estimator
  \begin{equation} \label{eq:rhat}
    \rhat = \avg{i}{n} X_i Y_i = \avg{i}{n} R_i.
  \end{equation}
  Note that the values of $R_i$ \emph{are} \iid\ since each pair $(X_i, Y_i)$
  are \iid, even though $X_i$ and $Y_j$ may not be independent for $i \ne j$.
  As before, we apply the Law of Large Numbers,
  \begin{alignat*}{3}
    \avg{i}{n} R_i &\Plim \E{R} \by{LLN} \\
    \E{R} &= \E{\Ber(r)} \by{given} \\
          &= r \by{definition of Bernoulli r.v.} \\
    \therefore \avg{i}{n} R_i &\Plim r \\
    \implies \rhat &\Plim r.
  \end{alignat*}
  Thus, each estimator $(\phat, \qhat, \rhat)$ converges to its
  respective parameter $(p, q, r)$.
\end{proof}

\subsubsection{Asymptotic Normality of the Estimators}
By the Central Limit Theorem,
\begin{equation} \label{eq:CLT}
  \sqrt{n} ((\phat, \qhat, \rhat) - (p, q, r)) \Dlim \N{\vect{0}_n}{\Sigma}
\end{equation}
where $\Sigma$ is the 3-by-3 symmetric covariance matrix, defined as
\begin{equation}
  \Sigma \coloneqq 
  \begin{bmatrix}
    \Var{\phat} & \Cov{\phat, \qhat} & \Cov{\phat, \rhat} \\
    \cdot & \Var{\qhat} & \Cov{\qhat, \rhat} \\
    \cdot & \cdot & \Var{\rhat}
  \end{bmatrix}.
\end{equation}
We now determine each of these values to show the asymptotic normality of the
vector of estimators.

\begin{prop}
The variance of $\phat$ is given by $\Var{\phat} = \frac{1}{n} p(1 - p)$.
\end{prop}

\begin{proof}
  Using the definition of $\phat$,
  \begin{alignat*}{3}
    \Var{\phat} &= \Var{\avg{i}{n} X_i} \by{definition} \\
                &= \frac{1}{n^2}\Var{\sumi{i}{n} X_i} \by{variance rule} \\
                &= \frac{1}{n^2}\sumi{i}{n}\Var{X_i} \by{\iid} \\
                &= \frac{1}{n^2}n\Var{X} \by{\iid} \\
   \therefore \Var{\phat} &= \frac{1}{n} p(1-p) \by{variance of $\Ber(p)$}
  \end{alignat*}
  Likewise, $\Var{\qhat} = \frac{1}{n} q(1 - q)$,
  and $\Var{\rhat} = \frac{1}{n} r(1 - r)$. 
\end{proof}

\clearpage
\begin{prop}
  The covariance of $\phat$ and $\qhat$ is given by $\Cov{\phat, \qhat} = r - pq$.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \Cov{\phat, \qhat} &= \Cov{\avg{i}{n} X_i, \avg{i}{n} Y_i} \\
    &= \frac{1}{n^2} \Cov{\sumi{i}{n} X_i, \sumi{i}{n} Y_i} \by{covariance property} \\
    &= \frac{1}{n^2} \sumi{i}{n} \sumi{j}{n} \Cov{X_i, Y_j} \by{bilinearity of covariance} \\
    &= \frac{1}{n^2} n^2 \Cov{X, Y} \by{identically distributed} \\
    &= \Cov{X, Y}  \\
    &= \E{XY} - \E{X}\E{Y} \by{definition of covariance} \\
    &= \E{R} - \E{X}\E{Y} \by{definition of $R$} \\
    \therefore \Cov{\phat, \qhat} &= r - pq \qedhere
  \end{alignat*}
\end{proof}

% \begin{prop}
%   The expectation of the estimator $\phat$ is $\E{\phat} = p$.
% \end{prop}
% \begin{proof}
%   \begin{alignat*}{3}
%     \E{\phat} &= \E{\avg{i}{n} X_i} \\
%               &= \frac{1}{n} \sumi{i}{n} \E{X_i} \by{linearity of expectation} \\
%               &= \frac{1}{n} n \E{X} \by{\iid} \\
%     \implies \E{\phat} &= p \by{expectation of $\Ber(p)$} \qedhere
%   \end{alignat*}
% \end{proof}
% Similarly, $\E{\qhat} = q$ and $\E{\rhat} = r$. Incidentally, this proposition
% also shows that the estimators are \emph{unbiased}, since $\E{\phat - p} = 0$,
% \emph{etc}.

\begin{prop}
  The covariance of $\phat$ and $\rhat$ is given by $\Cov{\phat, \rhat} = r(1 - p)$.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \Cov{\phat, \rhat} &= \Cov{\avg{i}{n} X_i, \avg{i}{n} R_i} \\
    &= \frac{1}{n^2} \Cov{\sumi{i}{n} X_i, \sumi{i}{n} R_i} \by{covariance property} \\
    &= \frac{1}{n^2} \sumi{i}{n} \sumi{j}{n} \Cov{X_i, R_j} \by{bilinearity of covariance} \\
    &= \frac{1}{n^2} n^2 \Cov{X, R} \by{identically distributed} \\
    &= \Cov{X, R}  \\
    &= \E{X R} - \E{X}\E{R} \by{definition of covariance} \\
    &= \E{X R} - pr \by{given} \\
    &= \E{X (X Y)} - pr \by{definition of $R$} \\
    \intertext{Since $X \sim \Ber(p) \in \{0, 1\}$, $X^2 = X$, so we have}
    &= \E{X Y} - pr \\
    &= r - pr \\
    \therefore \Cov{\phat, \rhat} &= r(1 - p)
  \end{alignat*}
Similarly, $\Cov{\qhat, \rhat} = r(1 - q)$.
\end{proof}

The entire asymptotic covariance matrix is then
\begin{equation} \label{eq:sigma}
  \Sigma =
  \begin{bmatrix}
    p(1-p) & r - pq & r(1-p) \\
    \cdot & q(1-q) & r(1-q) \\
    \cdot & \cdot & r(1-r)
  \end{bmatrix}.
\end{equation}

\subsubsection{The Delta Method}
\begin{prop}
  $$ \sqrt{n}\((\rhat - \phat\qhat) - (r - pq)\) \Dlim \N{0}{V} $$
\end{prop}

\begin{proof}
  Let $\hat{\theta}$ and $\theta$ be vectors in $\R^3$
  \begin{equation*}
    \hat{\theta} = \begin{bmatrix} \phat \\ \qhat \\ \rhat \end{bmatrix} \text{, and } 
    \theta = \begin{bmatrix} p \\ q \\ r \end{bmatrix}.
  \end{equation*}
  We have
  \begin{alignat*}{3}
    \sqrt{n}(\hat{\theta} - \theta) &\Dlim \N{0}{\Sigma} \by{CLT} \\
    \implies \sqrt{n}(g(\hat{\theta}) - g(\theta)) &\Dlim \N{0}{\nabla g(\theta)^\T \Sigma
      \nabla g(\theta)} \by{Delta method}
  \end{alignat*}
  for any differentiable function $g \colon \R^k \to \R$, and $\Sigma$ given by
  Equation~\eqref{eq:sigma}. 
  Define the function 
  \begin{equation}
    g(u, v, w) = w - uv
  \end{equation}
  such that 
  \begin{align}
    g(\hat{\theta}) &= \rhat - \phat\qhat \\
    g(\theta) &= r - pq.
  \end{align}
  The gradient of $g(\theta)$ is then
  \begin{equation}
    \nabla g(u,v,w) = \begin{bmatrix} -v \\ -u \\ 1 \end{bmatrix} 
    \implies \nabla g(\theta) = \begin{bmatrix} -q \\ -p \\ 1 \end{bmatrix}
  \end{equation}
  The asymptotic variance $V = \nabla g(\theta)^\T \Sigma \nabla g(\theta)$,
  which we will now show is a function only of the parameters $(p, q, r)$.
  \begin{alignat}{3}
    V &= \begin{bmatrix} -q & -p & 1 \end{bmatrix} 
    \begin{bmatrix}
      p(1-p) & r - pq & r(1-p) \\
      \cdot & q(1-q) & r(1-q) \\
      \cdot & \cdot & r(1-r)
    \end{bmatrix}
    \begin{bmatrix} -q \\ -p \\ 1 \end{bmatrix} \\
    &= \begin{bmatrix} -q & -p & 1 \end{bmatrix} 
    \begin{bmatrix} 
      -qp(1-p) - p(r - pq) + r(1-p) \\
      -q(r - pq) - pq(1-q) + r(1-q) \\
      -qr(1-p) - pr(1-q) + r(1-r)
    \end{bmatrix} \nonumber  \\
    &= \begin{bmatrix} -q & -p & 1 \end{bmatrix} 
    \begin{bmatrix} 
      (r - pq)(1 - 2p) \\
      (r - pq)(1 - 2q) \\
      r((1-p)(1-q) - (r-pq))
    \end{bmatrix} \nonumber \\
    \begin{split}
      &= -q(r - pq)(1 - 2p) - p(r - pq)(1 - 2q)) \\
      &\,\quad + r((1-p)(1-q) - (r-pq))
    \end{split} \\
    \therefore V &= (r - pq)[-q(1 - 2p) - p(1 - 2q) - r] + r(1-p)(1-q) \label{eq:V}
  \end{alignat}
  which is a function only of $(p, q, r)$.
\end{proof}

\subsubsection{The Null Hypothesis}
Consider the hypotheses
\begin{align*}
  H_0 \colon X \indep Y \\
  H_1 \colon X \nindep Y
\end{align*}

\begin{prop}
  If $H_0$ is true, then $V = pq(1-p)(1-q)$.
\end{prop}

\begin{proof}
  Under $H_0$, $r = pq$. Using the previous expression for $V$,
  Equation~\eqref{eq:V}, replace $r$ by $pq$ to find
  $$ V = (pq - pq)[-q(1 - 2p) - p(1 - 2q) - pq] + pq(1-p)(1-q). $$
  The first term is identically 0, so
  \begin{equation}
    V = pq(1-p)(1-q). \qedhere
  \end{equation}
\end{proof}

\subsubsection{A Hypothesis Test}
Given $\alpha \in (0, 1)$, we propose the test statistic
\begin{equation}
  T_n = \sqrt{n}\frac{\rhat - \phat\quat}{\sqrt{V}}
\end{equation}
where $V$ is given by 


% % INCLUDE CODE:
% \clearpage
% \subsection*{Code}
% \renewcommand{\baselinestretch}{1.0}
% \lstinputlisting[language=matlab]{../engs250_2_1_wallshear.m}

%===============================================================================
% References:
%===============================================================================
% \clearpage
% \lhead{References}
% \bibliographystyle{apalike}
% \bibliography{engg149_finalbib}

%===============================================================================
%   Source Code {{{
%===============================================================================
% \clearpage
% \appendix   % BEGIN APPENDIX NUMBERING
% \lhead{Roesler, Final Exam, Source Code} % clear "Problem #" header
%
% \renewcommand{\baselinestretch}{1.0}
%
% \section{Main Script for Problems 1--4} \label{app:code1}
% \ % keep '\ 'space here to include "Source Code" appendix header
% \lstinputlisting[language=matlab]{../engg149_final_mimo.m}
%
% }}}


\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
