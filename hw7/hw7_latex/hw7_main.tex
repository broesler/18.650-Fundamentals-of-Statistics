\documentclass[letterpaper, reqno]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[comma, compress, numbers, square]{natbib}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}  % for indicator function
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{mleftright}
\usepackage{enumerate}
\usepackage{array, multirow}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage[font=small, hypcap=true]{caption}                % link to top of figures and subfigures
\usepackage[font=small, hypcap=true, list=true]{subcaption}   % use for subfigures instead of {subfigure}

\usepackage{hyperref}
\usepackage{nameref}  % refer to unnumbered sections
\hypersetup{
    linktoc=all,    % link table of contents to sections
    colorlinks,
    allcolors=black,
    urlcolor=dblue,
  }

% Declare theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}{Proposition}[section]

\renewcommand{\qedsymbol}{\ensuremath{\blacksquare}}

% new environment for proofs of claims within proofs
\newenvironment{subproof}[1][\proofname]{%
  \renewcommand{\qedsymbol}{\ensuremath{\square}}%
  \begin{proof}[#1]%
}{%
  \end{proof}%
}

\numberwithin{equation}{section}

% lists as 1., 2., ...
\renewcommand{\labelenumi}{\theenumi.}

% Letter subsections
\renewcommand{\thesubsubsection}{\thesubsection(\alph{subsubsection})}

% number only specific equation in, say, align* environment
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}

% Algorithm
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}

%-------------------------------------------------------------------------------
%     {listings + color} package options
%-------------------------------------------------------------------------------
% Define colors for code
\usepackage{xcolor}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}
\definecolor{mauve}{rgb}{0.58, 0, 0.82}
\definecolor{dblue}{HTML}{0645AD}
\definecolor{lblue}{HTML}{1F77B4}

\usepackage{listings}
\lstset{
  language = Python,
  basicstyle = \scriptsize\ttfamily,
  numbers = left,
  numberstyle = \tiny\color{gray},
  stepnumber = 1,
  numbersep = 8pt,
  breaklines = true,
  keywordstyle = \bfseries\color{lblue},
  commentstyle = \color{gray},
  % stringstyle = \color{mauve}
}

% Place loose figures at actual top of page
% \makeatletter
%   \setlength{\@fptop}{0pt}
% \makeatother

%-------------------------------------------------------------------------------
% NO HYPHENATION
%-------------------------------------------------------------------------------
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

%==============================================================================
% General macros
%==============================================================================
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\newcommand{\T}{\top} % transpose symbol
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}} % use BOTH to cover all cases

% Misc.
\newcommand{\ie}{\emph{i.e.\ }}
\newcommand{\eg}{\emph{e.g.\ }}

% Use for fractions with large terms
\newcommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}

% Derivatives
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ddx}[2]{\frac{\dd #1}{\dd #2}}
\newcommand{\dx}[1]{\,\dd #1} % for inside integration

% Probability helpers
\newcommand{\Prob}[1]{\mathbb{P}\mleft[#1\mright]}
\newcommand{\E}[1]{\mathbb{E}\mleft[#1\mright]}
\newcommand{\V}[1]{\mathbb{V}\mleft[#1\mright]}
\newcommand{\Var}[1]{\operatorname{Var}\mleft(#1\mright)}
\newcommand{\Cov}[1]{\operatorname{Cov}\mleft(#1\mright)}
\newcommand{\R}{\mathbb{R}}  % real numbers
\newcommand{\N}[2]{\mathcal{N}\mleft( #1, #2 \mright)}
\newcommand{\U}[2]{\mathcal{U}\mleft(\mleft[ #1, #2 \mright]\mright)}
\newcommand{\indep}{\perp \!\!\! \perp}  % "is independent from"
\newcommand{\nindep}{\centernot\indep}
\newcommand{\indic}[1]{\mathbbm{1}\!\mleft\{#1\mright\}} % indicator function
\newcommand{\iid}{i.i.d.}

\newcommand{\sumi}[2]{\sum_{#1=1}^{#2}}
\newcommand{\avg}[2]{\frac{1}{#2}\sumi{#1}{#2}}

\newcommand{\by}[1]{&\quad&\text{(#1)}}

\newcommand{\Alim}{\xrightarrow[n \to \infty]{\text{a.s.}}}
\newcommand{\Plim}{\xrightarrow[n \to \infty]{\mathbb{P}}}
\newcommand{\Dlim}{\xrightarrow[n \to \infty]{(d)}}

\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}

\newcommand{\Xnbar}{\overline{X}_n}
\newcommand{\Rnbar}{\overline{R}_n}
\newcommand{\Qnbar}{\overline{Q}_n}

\DeclareMathOperator{\Ber}{Ber}
% \DeclareMathOperator{\Var}{Var}
% \DeclareMathOperator{\Cov}{Cov}

%%%% TITLE ----------------------------
\title[Homework 7 -- Problem \thesection]{18.650 Fundamentals of Statistics\\{\large Homework 7}}
\author{Bernie Roesler}
\date{\today}
%%%%

%%%%%%% BEGIN DOCUMENT ----------------
\begin{document}
% \sloppy

\graphicspath{{./figures/}}

\maketitle

\section{QQ-Plots}
Consider the QQ-plots of five \iid\ random variables with the following
distributions:
\begin{enumerate}
\item Standard Normal, $\N{0}{1}$,
  \item Uniform distribution, $\U{-\sqrt{3}}{\sqrt{3}}$,
  \item Cauchy distribution $\sim g(x) = \frac{1}{\pi}\frac{2}{1+x^2}$,
  \item Exponential distribution $\sim \operatorname{Exp}(\lambda) = \lambda
    e^{-\lambda x}, \lambda = 1$,
  \item Laplace distribution $\sim \operatorname{Laplace}(\lambda)
    = \frac{\lambda}{2} e^{-\lambda x}, \lambda = \sqrt{2}$.
\end{enumerate}
Figure \ref{fig:qqplots} shows the samples labeled with the appropriate
distribution.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{qqplots.pdf}
  \caption{QQ-plots of five \iid\ random variables from different distributions.}
  \label{fig:qqplots}
\end{figure}

\clearpage
\section{Kolmogorov-Smirnov Test for Two Samples}
Consider two independent samples $X_1, \dots, X_n$, and $Y_1, \dots, Y_m$ of
independent, real-valued, continuous random variables, and assume that the $X_i$'s
are \iid\ with some cdf $F$ and that the $Y_i$'s are \iid\ with some cdf $G$. Note that the
two samples may have different sizes (if $n \ne m$). We want to test whether $F = G$.
Consider the following hypotheses:
\begin{align*}
  H_0 \colon ``F = G" \\
  H_1 \colon ``F \ne G"
\end{align*}
For simplicity, we will assume that $F$ and $G$ are continuous and increasing.

\subsection{Example Experiment}
An example experiment in which testing if two samples are from the same
distribution is of interest may be encountered in a lab setting where we have
two devices for measurement, and wish to determine if the errors have the same
distribution for our analysis. 

\subsection{CDF Distributions}
Let 
\begin{align*}
  U_i &= F(X_i), \quad \forall i = 1, \dots, n, \\
  V_j &= G(Y_j), \quad \forall j = 1, \dots, n.
\end{align*}

\begin{prop}
  The distribution of the cdf of a continuous random variable is uniform on $[0,
  1]$.
\end{prop}

\begin{proof}
The distributions of $U_i$ and $V_j$ can be determined by finding their cdfs.
The cdf of $U_i$ is defined by $F_U(t) \coloneqq \Prob{U_i \le t}$. Assuming that $F(X)$ and $G(Y)$ are invertible, it follows that
\begin{alignat*}{3}
  \Prob{U_i \le t} &= \Prob{F(X_i) \le t} \by{definition of $U_i$} \\
                   &= \Prob{X_i \le F^{-1}(t)} \\
                   &= F(F^{-1}(t)) \by{definition of cdf} \\
                   &= t \\
  \therefore F_U(t) &= t \\
  \implies f_U(t) &= \U{0}{1} \tag*{\qedhere}
\end{alignat*}
\end{proof}
Likewise, $f_V(t) = \U{0}{1}$.

\subsection{Empirical CDFs}
Let $F_n$ be the empirical cdf of $\{X_1, \dots, X_n\}$ and $G_m$ be the
empirical cdf of $\{Y_1, \dots, Y_m\}$.

\subsubsection{The Test Statistic}
Let
\begin{equation}
  T_{n,m} = \sup_{t \in \R} \left| F_n(t) - G_m(t) \right|
\end{equation}

\begin{prop}
  The test statistic $T_{n,m}$ can be written as the maximum value of a finite set of numbers.
\end{prop}

\begin{proof}
  By definition, the cdf
  \begin{alignat*}{3}
    F(t) &= \Prob{X \le t} \quad \forall t \in \R \\
         &= \E{\indic{X \le t}}. \\
    \intertext{By the Law of Large Numbers, the expectation can be approximated
        by the sample average, so we can define the \emph{empirical cdf} as}
    F_n(t) &= \avg{i}{n} \indic{X_i \le t} \numberthis \label{eq:F_n}
    \intertext{Likewise,}
    G_m(t) &= \avg{j}{m} \indic{Y_j \le t}. \numberthis \label{eq:G_m}
  \end{alignat*}
  \begin{equation}
    \therefore T_{n,m} = \sup_{t \in \R} \left| \avg{i}{n} \indic{X_i \le t} - \avg{j}{m} \indic{Y_j \le t} \right|.
  \end{equation}
  The empirical cdfs~\eqref{eq:F_n}~and~\eqref{eq:G_m} can also be written
  \begin{alignat}{3}
    F_n(t) &= \#\{i=1, \dots, n \colon X_i \le t\} \cdot \frac{1}{n} \\
    G_m(t) &= \#\{i=1, \dots, m \colon Y_j \le t\} \cdot \frac{1}{m},
  \end{alignat}
  so the only values that the empirical cdfs can take are the discrete sets
  \begin{align}
    F_n(i) &= \frac{i}{n} \quad \forall i = 1, \dots, n \\
    G_m(j) &= \frac{j}{m} \quad \forall j = 1, \dots, m.
  \end{align}
  Therefore, the test statistic can be rewritten as the maximum value of
  a finite set of numbers:
  \begin{equation}
    \begin{split}
      T_{n,m} = \max_{i=0,\dots,n} \Bigg[
      &\max_{j=0,\dots,m} \left| \frac{i}{n} - \frac{j}{m} \right| 
        \indic{Y^{(j)} \le X^{(i)} < Y^{(j+1)}}, \\ 
      &\max_{k=j+1, \dots, m} \left| \frac{i}{n} - \frac{k}{m} \right| 
        \indic{Y^{(k)} \le X^{(i+1)}} \Bigg]
    \end{split}
  \end{equation}
  where $X^{(i)}$ is the $i^\text{th}$ value in the ordered set of data 
  $X^{(1)} \le \cdots \le X^{(n)}$. The values $X^{(0)}, Y^{(0)} \coloneqq -\infty$
  are prepended to the otherwise finite realizations to simplify the
  computation.
\end{proof}

\clearpage
The following algorithm calculates the KS test statistic for two given samples.
\begin{algorithm}[!h]
  \caption{Calculate the KS test statistic $T_{n,m}$ for two samples.}
  \label{alg:ks_stat}
  \begin{algorithmic}[1]
    \Require $X, Y$ are vectors of real numbers.
    \Ensure $0 \le T_{n,m} \le 1$.
    \Function{Rank}{$A, k$}
      \State {\bfseries assert} $A$ is sorted in ascending order.
      \State\Return $\#\{i=1,\dots,\dim A \colon k < A_i\}$
    \EndFunction
    \Procedure{KS2Sample}{$X, Y$}
    \State $X_s \gets \{-\infty,$ \Call{Sort}{$X$}$\}$
    \State $Y_s \gets$ \Call{Sort}{$Y$}
    \State $n \gets \dim X_s$
    \State $m \gets \dim Y_s$
    \State $T_v \gets$ empty array of size $n$
    \ForAll{$i \in \{0, \dots, n-1\}$}
      \State $j \gets$ \Call{Rank}{$Y_s, X_s^{(i)}$} + 1
      \State $k \gets$ \Call{Rank}{$\{Y_s^{(\ell)}\}_{\ell=j+1}^m, X_s^{(i+1)}$}
      + (j+1)
      \State $\displaystyle{T_v^{(i)} \gets 
        \max\mleft(\left|\frac{i}{n} - \frac{j}{m}\right|,
              \left|\frac{i}{n} - \frac{k}{m}\right|\mright)}$
    \EndFor
    \State\Return $\max\limits_i T_v$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\clearpage
The following subroutine is an implementation of Algorithm~\ref{alg:ks_stat}. It computes an array of values $T_v(i)$ for each value of $X_i$. The test statistic $T_{n,m}$ is the maximum of these values.
\lstinputlisting[language=python,
  rangeprefix=\#<<,
  rangesuffix=>>,
  includerangemarker=false,
  linerange=begin__ks_2samp-end__ks_2samp
  ]{../hw7_kstest.py}

\clearpage
An example two-sample KS-test is shown in Figure~\ref{fig:ks_test}.
\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\textwidth]{ks_test.pdf}
  \caption{The empirical cdfs of two independent random samples from $\N{0}{1}$ and $\N{0}{2}$. The test statistic $T_{n,m}$ is shown by the double arrow.}
  \label{fig:ks_test}
\end{figure}

\subsubsection{The Null Hypothesis}
\begin{prop}
  If $H_0$ is true, then
  \[ T_{n,m} = \sup_{0 \le x \le 1} \left| \avg{i}{n} \indic{U_i \le x}
- \avg{j}{m} \indic{V_j \le x} \right|. \]
\end{prop}

\begin{proof}
  By~\eqref{eq:F_n}~and~\eqref{eq:G_m},
  \begin{equation} \label{eq:Tnm_supt}
    T_{n,m} = \sup_{t \in \R} \left| \avg{i}{n} \indic{X_i \le t} - \avg{j}{m} \indic{Y_j \le t} \right|.
  \end{equation}
  To show the proposition is true, we make a change of variable. Let
    \[ x = F(t). \]
  Then,
    \[ t \in \R \implies x \in [0, 1]. \]
    Since $F$ and $G$ are continuous and monotonically increasing,
  \begin{alignat*}{3}
    X_i \le t &\iff F(X_i) \le F(t) \\
              &\iff U_i \le x \by{definition}.
  \end{alignat*}
  Similarly,
  \begin{alignat*}{3}
    Y_i \le t &\iff G(Y_i) \le G(t) \\
              &\iff G(Y_i) \le F(t) \by{under $H_0$} \\
              &\iff V_i \le x \by{definition}.
  \end{alignat*}
  Substitution of these expressions into~\eqref{eq:Tnm_supt} completes the
  proof. 
\end{proof}

\subsubsection{The Joint Distribution of the Samples}
\begin{prop} \label{prop:Tnm}
  If $H_0$ is true, the joint distribution of $U_1, \dots, U_n, V_1, \dots, V_m$
  $(n+m)$ random variables is uniform on $[0, 1]$.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \Prob{U_i \le t} &= \Prob{F(X_i) \le t} \\
                     &= \Prob{F(X_1) \le t} \by{\iid} \\
                     &= \Prob{G(X_1) \le t} \by{under $H_0$} \\
                     &= \Prob{G(Y_1) \le t} \by{\iid} \\
                     &= \Prob{V_1 \le t} \by{definition} \\
    \intertext{These probabilities can be rearranged to find the cdfs of $U$ and $V$}
                     &= \Prob{X_1 \le F^{-1}(t)} \\
                     &= F(F^{-1}(t)) \by{definition of cdf} \\
                     &= t \\
    \therefore F_U(t) &= G_V(t) = t \\
    \implies f_{U,V}(t) &= \U{0}{1} \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\subsubsection{The Test Statistic is Pivotal}
Since Proposition~\ref{prop:Tnm} has been shown to be true under the null hypothesis $H_0$, and the distributions of $U_i$
and $V_j$ have been shown to be $\U{0}{1}$ independent of the distributions of
the underlying samples $X_i$, $Y_j$, we conclude that $T_{n,m}$ is
\emph{pivotal}, \ie it does not itself depend on the unknown distributions of
the samples.

\clearpage
\subsubsection{Quantiles of the Test Statistic}
Let $\alpha \in (0, 1)$ and $q_\alpha$ be the $(1 - \alpha)$-quantile of the
distribution of $T_{n,m}$ under $H_0$. The quantile $q_\alpha$ is given by
\begin{align*}
  q_\alpha &= F^{-1}(1-\alpha) \\
           &= \inf\{x \colon F(x) \ge 1 - \alpha\} \\
           &\approx \min\{x \colon F_n(x) \ge 1 - \alpha\}, \quad n < \infty \\
           \implies q_\alpha \approx \hat{q}_\alpha &= \min_i \left\{
               T_{n,m}^{(i)} \colon \tfrac{i}{M} \ge 1 - \alpha \right\}
\end{align*}
where $M \in \mathbb{N}$ is large, and $T_{n,m}^{(i)}$ is the $i^\text{th}$
value in a sorted sample of $M$ test statistics. Thus, $q_\alpha$ can be
approximated by choosing $i = \ceil{M\alpha}$. An algorithm to approximate
$q_\alpha$ given $\alpha$ is as follows.

\begin{algorithm}[!h]
  \caption{Approximate $q_\alpha$, the $(1 - \alpha)$-quantile of the
distribution of $T_{n,m}$ under $H_0$.}
  \label{alg:ks_q}
  \begin{algorithmic}
    \Require $X, Y$ are vectors of real numbers. $M \in \mathbb{N}$. $\alpha
    \in (0, 1)$.
    \Ensure $q_\alpha \in [0, 1]$.
    \Procedure{KSQuantile}{$X, Y, M, \alpha$}
      \State $n \gets \dim X$
      \State $m \gets \dim Y$
      \State $T_v \gets$ empty array of size $n$
      \ForAll{$i \in \{0,\dots,M\}$}
        \State $X_s \gets$ sample of size $n$ from $\N{0}{1}$.
        \State $Y_s \gets$ sample of size $m$ from $\N{0}{1}$.
        \State $T_v^{(i)} \gets$ \Call{KS2Sample}{$X_s, Y_s$}
      \EndFor
      \State $T_{vs} \gets$ \Call{Sort}{$T_v$}
      \State $j \gets \ceil*{M\alpha}$
      \State \Return $T_{vs}^{(j)}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

% TODO show plot of this distribution

\subsubsection{The Hypothesis Test}
Given the aproximation for $\hat{q}_\alpha$ for $q_\alpha$ from
Algorithm~\ref{alg:ks_q}, we define a test with non-asymptotic level $\alpha$
for $H_0$ vs.\ $H_1$:
\begin{equation}
  \delta_\alpha = \indic{T_{n,m} > \hat{q}_\alpha^{(n, M)}}
\end{equation}
where $T_{n,m}$ is found by Algorithm~\ref{alg:ks_stat}. The p-value for this
test is
\begin{align}
  \text{p-value} &\coloneqq \Prob{Z \ge T_{n,m}} \\
  &\approx \frac{\#\{j = 1, \dots, M \colon T_{n,m}^{(j)} \ge T_{n,m}\}}{M}
\end{align}
where $Z$ is a random variable distributed as $T_{n,m}$.

\clearpage
\section{Aside: Homework 6, Problem 3 -- Test of Independence for Bernoulli Random Variables}
Let $X, Y$ be two Bernoulli random variables, not necessarily independent, and
let $p = \Prob{X=1}$, $q = \Prob{Y=1}$, and $r = \Prob{X=1, Y=1}$.

\subsection{Condition for Independence}
\begin{prop}
  $X \indep Y \iff r = pq$.
\end{prop}

\begin{proof}
  Two random variables are independent iff 
  \begin{equation}
    \Prob{X \cap Y} = \Prob{X}\Prob{Y} \label{eq:indep}
    % \iff \Prob{X} = \frac{\Prob{X,Y}}{\Prob{Y}} = \Prob{X | Y}
  \end{equation}
  By the given definition of the Bernoulli random variables,
  \begin{align*}
    \Prob{X \cap Y} &= \Prob{X, Y} = \Prob{X=1, Y=1} = r \\
    \Prob{X} &= \Prob{X=1} = p \\
    \Prob{Y} &= \Prob{Y=1} = q \\
    \therefore r = pq &\iff \Prob{X,Y} = \Prob{X}\Prob{Y} \\
    &\iff X \indep Y  \qedhere
  \end{align*}
\end{proof}

\subsection{Test for Independence}
Let $(X_1, Y_1), \dots, (X_n, Y_n)$ be a sample of $n$ \iid\ copies of $(X, Y)$
(\ie $X_i \indep X_j$ for $i \ne j$, but $X_i$ may not be independent of $Y_i$).
Based on this sample, we want to test whether $X \indep Y$, \ie whether $r = pq$.

\subsubsection{Estimators of $p, q, r$}
Define the estimators:
\begin{align*}
  \phat &= \avg{i}{n} X_i, \\
  \qhat &= \avg{i}{n} Y_i, \\
  \rhat &= \avg{i}{n} X_i Y_i.
\end{align*}

\begin{prop}
  These estimators $\phat$, $\qhat$, and $\rhat$ are consistent estimators of
  the true values $p$, $q$, and $r$.
\end{prop}

\begin{proof}
  To show that an estimator is \emph{consistent}, we must prove that it
  converges to the true value of the parameter in the limit as $n \to \infty$.
  Since the sequence of $X_i$'s are \iid, we can use the Weak Law of Large
  Numbers (LLN) to prove that $\phat$ converges to $p$.

  \begin{theorem}[Weak Law of Large Numbers] \label{eq:LLN}
    If the sequence of random variables $X_1, \dots, X_n$ are \iid, then
    \[ \avg{i}{n} X_i \Plim \E{X}. \]
  \end{theorem}

  The expectation of $X$ is given by
  \begin{alignat*}{3}
    \E{X} &= \E{\Ber(p)} \by{given} \\
          &= p \by{definition of Bernoulli r.v.} \\
    \therefore \avg{i}{n} X_i &\Plim p \by{LLN} \\
    \implies \phat &\Plim p.
  \end{alignat*}
  Likewise $\qhat \Plim q$.

  To show that $\rhat$ converges to $r$, let $R \coloneqq X Y$ be
  a Bernoulli random variable with parameter $r = \Prob{X=1, Y=1}$, so that the
  estimator
  \begin{equation} \label{eq:rhat}
    \rhat = \avg{i}{n} X_i Y_i = \avg{i}{n} R_i.
  \end{equation}
  Note that the values of $R_i$ \emph{are} \iid\ since each pair $(X_i, Y_i)$
  are \iid, even though $X_i$ and $Y_j$ may not be independent for $i \ne j$.
  As before, we apply the Law of Large Numbers to the average of $R_i$'s. The
  expectation of $R$ is 
  \begin{alignat*}{3}
    \E{R} &= \E{\Ber(r)} \by{definition} \\
          &= r \by{definition of Bernoulli r.v.} \\
    \therefore \avg{i}{n} R_i &\Plim r \by{LLN} \\
    \implies \rhat &\Plim r.
  \end{alignat*}
  Thus, each estimator $(\phat, \qhat, \rhat)$ converges to its
  respective parameter $(p, q, r)$.
\end{proof}

\subsubsection{Asymptotic Normality of the Estimators}
\begin{prop} \label{prop:normality_pqr}
    The vector of estimators $(\phat, \qhat, \rhat)$ is asymptotically normal, \ie
    \[ \sqrt{n} ((\phat, \qhat, \rhat) - (p, q, r)) 
        \Dlim \N{0}{\Cov{(\phat, \qhat, \rhat)}}. \]
\end{prop}
\emph{Proof.} To prove that the vector of estimators is asymptotically normal, we employ the
Central Limit Theorem (CLT).

\begin{theorem}[Central Limit Theorem]
    Let $X_1, \dots, X_n$ be a sequence of \iid\ random vectors $X_i \in \R^k$, 
    and $\Xnbar = \avg{i}{n} X_i$.
    Then,
    \[ \sqrt{n}(\Xnbar - \E{X}) \Dlim \N{0}{\Sigma}. \]
    where $\Sigma$ is the $k$-by-$k$ matrix $\Cov{X}$.
\end{theorem}

By the CLT,
\begin{equation} \label{eq:CLT}
  \sqrt{n} ((\phat, \qhat, \rhat) - \E{(\phat, \qhat, \rhat)}) \Dlim \N{0}{\Sigma}
\end{equation}
where $\Sigma$ is the 3-by-3 symmetric covariance matrix, defined as
\begin{equation}
  \Sigma \coloneqq 
  \begin{bmatrix}
    \Var{\phat} & \Cov{\phat, \qhat} & \Cov{\phat, \rhat} \\
    \cdot & \Var{\qhat} & \Cov{\qhat, \rhat} \\
    \cdot & \cdot & \Var{\rhat}
  \end{bmatrix}.
\end{equation}

We first need to determine the expectations of the estimators.

\begin{prop}
  The expectation of the estimator $\phat$ is $\E{\phat} = p$.
\end{prop}
\begin{subproof}
  \begin{alignat*}{3}
    \E{\phat} &= \E{\avg{i}{n} X_i} \by{definition} \\
              &= \frac{1}{n} \sumi{i}{n} \E{X_i} \by{linearity of expectation} \\
              &= \frac{1}{n} n \E{X} \by{\iid} \\
              &= \E{\Ber(p)} \by{definition} \\
    \implies \E{\phat} &= p \by{definition} \tag*{\qedhere}
  \end{alignat*}
\end{subproof}
\noindent Similarly, $\E{\qhat} = q$ and $\E{\rhat} = r$. This proposition
also shows that the estimators are \emph{unbiased}, since $\E{\phat - p} = 0$,
\emph{etc}.

We now determine the entries in the covariance matrix to complete the proof of
asymptotic normality.
\begin{prop}
The variance of $\phat$ is given by $\Var{\phat} = \frac{1}{n} p(1 - p)$.
\end{prop}
\begin{subproof}
  Using the definition of $\phat$,
  \begin{alignat*}{3}
    \Var{\phat} &= \Var{\avg{i}{n} X_i} \by{definition} \\
                &= \frac{1}{n^2}\Var{\sumi{i}{n} X_i} \by{variance rule} \\
                &= \frac{1}{n^2}\sumi{i}{n}\Var{X_i} \by{\iid} \\
                &= \frac{1}{n^2}n\Var{X} \by{\iid} \\
   \therefore \Var{\phat} &= \frac{1}{n} p(1-p) \by{variance of $\Ber(p)$}
  \end{alignat*}
  Likewise, $\Var{\qhat} = \frac{1}{n} q(1 - q)$,
  and $\Var{\rhat} = \frac{1}{n} r(1 - r)$. 
\end{subproof}

\begin{prop}
  The covariance of $\phat$ and $\qhat$ is given by $\Cov{\phat, \qhat} = r - pq$.
\end{prop}
\begin{subproof}
  \begin{alignat*}{3}
    \Cov{\phat, \qhat} &= \Cov{\avg{i}{n} X_i, \avg{i}{n} Y_i} \\
    &= \frac{1}{n^2} \Cov{\sumi{i}{n} X_i, \sumi{i}{n} Y_i} \by{covariance property} \\
    &= \frac{1}{n^2} \sumi{i}{n} \sumi{j}{n} \Cov{X_i, Y_j} \by{bilinearity of covariance} \\
    &= \frac{1}{n^2} n^2 \Cov{X, Y} \by{identically distributed} \\
    &= \Cov{X, Y}  \\
    &= \E{XY} - \E{X}\E{Y} \by{definition of covariance} \\
    &= \E{R} - \E{X}\E{Y} \by{definition of $R$} \\
    \therefore \Cov{\phat, \qhat} &= r - pq \tag*{\qedhere}
  \end{alignat*}
\end{subproof}

\begin{prop}
  The covariance of $\phat$ and $\rhat$ is given by $\Cov{\phat, \rhat} = r(1 - p)$.
\end{prop}
\begin{subproof}
  \begin{alignat*}{3}
    \Cov{\phat, \rhat} &= \Cov{\avg{i}{n} X_i, \avg{i}{n} R_i} \\
    &= \frac{1}{n^2} \Cov{\sumi{i}{n} X_i, \sumi{i}{n} R_i} \by{covariance property} \\
    &= \frac{1}{n^2} \sumi{i}{n} \sumi{j}{n} \Cov{X_i, R_j} \by{bilinearity of covariance} \\
    &= \frac{1}{n^2} n^2 \Cov{X, R} \by{identically distributed} \\
    &= \Cov{X, R}  \\
    &= \E{X R} - \E{X}\E{R} \by{definition of covariance} \\
    &= \E{X R} - pr \by{given} \\
    &= \E{X (X Y)} - pr \by{definition of $R$} \\
    \intertext{Since $X \sim \Ber(p) \in \{0, 1\}$, $X^2 = X$, so we have}
    &= \E{X Y} - pr \\
    &= r - pr \\
    \therefore \Cov{\phat, \rhat} &= r(1 - p) \tag*{\qedhere}
  \end{alignat*}
\end{subproof}
\noindent Similarly, $\Cov{\qhat, \rhat} = r(1 - q)$.

The entire asymptotic covariance matrix is then
\begin{equation} \label{eq:sigma}
  \Sigma =
  \begin{bmatrix}
    p(1-p) & r - pq & r(1-p) \\
    \cdot & q(1-q) & r(1-q) \\
    \cdot & \cdot & r(1-r)
  \end{bmatrix}.
\end{equation}

Since we have determined the expectation $\E{(\phat, \qhat, \rhat)} = (p, q,
r)$, and the covariance matrix $\Sigma$ in terms of $p$, $q$, and $r$, we
conclude that Proposition~\ref{prop:normality_pqr} is true, and the vector of
estimators $(\phat, \qhat, \rhat)$ is asymptotically normal. \qed

\subsubsection{The Delta Method}
\begin{prop} \label{prop:delta}
  \[ \sqrt{n}\mleft((\rhat - \phat\qhat) - (r - pq)\mright) \Dlim \N{0}{V} \]
  where $V$ depends only on $p$, $q$, and $r$.
\end{prop}

\begin{proof}
  Let $\hat{\theta}$ and $\theta$ be vectors in $\R^3$
  \begin{equation*}
    \hat{\theta} = \begin{bmatrix} \phat \\ \qhat \\ \rhat \end{bmatrix} \text{, and } 
    \theta = \begin{bmatrix} p \\ q \\ r \end{bmatrix}.
  \end{equation*}
  From our proof of Proposition~\ref{prop:normality_pqr}, we have
  \begin{alignat*}{3}
    \sqrt{n}(\hat{\theta} - \theta) &\Dlim \N{0}{\Sigma} \by{CLT} \\
    \implies \sqrt{n}(g(\hat{\theta}) - g(\theta)) &\Dlim \N{0}{\nabla g(\theta)^\T \Sigma
      \nabla g(\theta)} \by{Delta method}
  \end{alignat*}
  for any differentiable function $g \colon \R^k \to \R$, and $\Sigma$ given by
  Equation~\eqref{eq:sigma}. 
  Define the function 
  \begin{equation} \label{eq:g}
    g(u, v, w) = w - uv
  \end{equation}
  such that 
  \begin{align*}
    g(\hat{\theta}) &= \rhat - \phat\qhat, \\
    g(\theta) &= r - pq.
  \end{align*}
  The gradient of $g(\theta)$ is then
  \begin{equation}
    \nabla g(u,v,w) = \begin{bmatrix} -v \\ -u \\ 1 \end{bmatrix} 
    \implies \nabla g(\theta) = \begin{bmatrix} -q \\ -p \\ 1 \end{bmatrix}
  \end{equation}
  The asymptotic variance $V = \nabla g(\theta)^\T \Sigma \nabla g(\theta)$,
  which we will now show is a function only of the parameters $(p, q, r)$.
  \begin{alignat*}{3}
    V &= \begin{bmatrix} -q & -p & 1 \end{bmatrix} 
    \begin{bmatrix}
      p(1-p) & r - pq & r(1-p) \\
      \cdot & q(1-q) & r(1-q) \\
      \cdot & \cdot & r(1-r)
    \end{bmatrix}
    \begin{bmatrix} -q \\ -p \\ 1 \end{bmatrix} \numberthis \label{eq:V_mat} \\
    &= \begin{bmatrix} -q & -p & 1 \end{bmatrix} 
    \begin{bmatrix} 
      -qp(1-p) - p(r - pq) + r(1-p) \\
      -q(r - pq) - pq(1-q) + r(1-q) \\
      -qr(1-p) - pr(1-q) + r(1-r)
    \end{bmatrix} \\
    &= \begin{bmatrix} -q & -p & 1 \end{bmatrix} 
    \begin{bmatrix} 
      (r - pq)(1 - 2p) \\
      (r - pq)(1 - 2q) \\
      r((1-p)(1-q) - (r-pq))
    \end{bmatrix} \\
    \begin{split}
      &= -q(r - pq)(1 - 2p) - p(r - pq)(1 - 2q)) \\
      &\,\quad + r((1-p)(1-q) - (r-pq))
    \end{split} \\
    \therefore V &= (r - pq)[-q(1 - 2p) - p(1 - 2q) - r] + r(1-p)(1-q) 
    \numberthis \label{eq:V}
  \end{alignat*}
  which is a function only of $(p, q, r)$.
\end{proof}

\subsubsection{The Null Hypothesis}
Consider the hypotheses
\begin{align*}
  H_0 \colon X \indep Y \\
  H_1 \colon X \nindep Y
\end{align*}

\begin{prop} \label{prop:V_H0}
  If $H_0$ is true, then $V = pq(1-p)(1-q)$.
\end{prop}

\begin{proof}
  Under $H_0$, $r = pq$. Using the previous expression for $V$,
  Equation~\eqref{eq:V}, replace $r$ by $pq$ to find
  \[ V = (pq - pq)[-q(1 - 2p) - p(1 - 2q) - pq] + pq(1-p)(1-q). \]
  The first term is identically 0, so
  \[ V = pq(1-p)(1-q). \qedhere \]
\end{proof}

\begin{prop} \label{prop:Vhat}
Given
  \[ V = pq(1-p)(1-q), \]
a consistent estimator is given by
  \[ \hat{V} = \phat\qhat(1 - \phat)(1 - \qhat). \]
\end{prop}

\begin{proof}
  To prove that $\hat{V}$ converges to $V$, we employ the Continuous Mapping
  Theorem.
  \begin{theorem}[Continuous Mapping Theorem]
    Let $X \in \R^n$ be a vector of random variables, and $g \colon \R^n \to \R$
    be a continuous function. Let $X_n = X_1, X_2, \dots$ be a sequence of
    random vectors. If $X_n \Plim X$, then $g(X_n) \Plim g(X)$. 
  \end{theorem}

  Since $\phat \Plim p$ and $\qhat \Plim q$,
  $\hat{V}(\phat, \qhat) \Plim V(p, q)$.
\end{proof}

\subsubsection{A Hypothesis Test}
\begin{prop}
  Given $\alpha \in (0, 1)$, we propose the test statistic
  \begin{equation}
    T_n \coloneqq \frac{\sqrt{n}(\rhat - \phat\qhat)}{\sqrt{\hat{V}}} \Dlim \N{0}{1}
  \end{equation}
  where $\hat{V}$ is given by Proposition~\ref{prop:Vhat}, and $t_{n-1}$ is
  Student's $t$-distribution with $n-1$ degrees of freedom.
\end{prop}

\begin{proof}
  Proposition~\ref{prop:delta} gives the distribution of $g(\theta)$ (given by
  Equation~\eqref{eq:g}) under $H_0$. 
  Assume that $p, q \in (0, 1)$ s.t.\ $V > 0$.
  \begin{alignat*}{3}
  \sqrt{n}\mleft((\rhat - \phat\qhat) - (r - pq)\mright) &\Dlim \N{0}{V}
  \by{Proposition~\ref{prop:delta}} \\
  \sqrt{n}(\rhat - \phat\qhat) &\Dlim \N{0}{V} \by{$r = pq$ under $H_0$} \\
  \sqrt{n}\frac{(\rhat - \phat\qhat)}{\sqrt{V}} &\Dlim \N{0}{1} \numberthis
  \label{eq:Tn_norm}
\end{alignat*}
The asymptotic variance $V$, however, is unknown, so we divide the estimator by
$\sqrt{\frac{\hat{V}}{V}}$ to get an expression that will evaluate to our
desired test statistic
\begin{equation} \label{eq:t1}
  T_n = \ddfrac{\sqrt{n}\frac{(\rhat - \phat\qhat)}{\sqrt{V}}}
               {\sqrt{\frac{\hat{V}}{V}}} 
\end{equation}
Given this expression, we can determine the distribution of $T_n$.
Equation~\eqref{eq:Tn_norm} shows that the numerator is a standard normal random
variable. 
\href{https://en.wikipedia.org/wiki/Cochran's_theorem}{\underline{Cochran's theorem}}
gives the distribution of the denominator.
\begin{lemma}[Result of Cochran's Theorem]
  If $X_1, \dots, X_n$ are \iid\ random variables drawn from the distribution
  $\N{\mu}{\sigma^2}$, and $S_n^2 \coloneqq \sumi{i}{n} (X_i - \Xnbar)^2$,
  then
  \[ \Xnbar \indep S_n, \]
  and
  \[ \frac{n S_n^2}{\sigma^2} \sim \chi^2_{n-1}. \]
\end{lemma}
Since $\hat{V}$ and $V$ describe the sample variance and variance of a
(asymptotically) normal distribution, $T_n$ is asymptotically characterized by
\begin{equation}
  T_n \Dlim \ddfrac{\N{0}{1}}{\sqrt{\frac{\chi^2_{n-1}}{n}}}
\end{equation}
which is the definition of a random variable drawn from \emph{Student's
T-distribution} with $n-1$ degrees of freedom. In this case, however, the
normality of the underlying random variables is asymptotic, so the $t_{n-1}$
distribution approaches a standard normal distribution
\begin{align*}
  T_n &\Dlim t_{n-1} \\
  t_{n-1} &\Dlim \N{0}{1} \numberthis \label{eq:t_to_N} \\
  \implies T_n &\Dlim \N{0}{1} \qedhere
\end{align*}
A proof of Equation~\eqref{eq:t_to_N} is given in \nameref{app:t_to_N}.
\end{proof}

Given the test statistic $T_n$, define the rejection region
\begin{equation}
  R_\psi = \left\{ \hat{\theta} \colon |T_n| > q_{\alpha/2} \right\}
\end{equation}
where
\begin{equation}
  q_{\alpha/2} = \Phi^{-1}\mleft(1 - \frac{\alpha}{2}\mright)
\end{equation}
is the $\mleft(1-\frac{\alpha}{2}\mright)$-quantile of the standard normal $\N{0}{1}$
distribution.

We would like to know whether the facts of being happy and being in
a relationship are independent of each other. In a given population, 1000 people
(aged at least 21 years old) are sampled and asked two questions: ``Do you
consider yourself as happy?'' and ``Are you involved in a relationship?''. The
answers are summarized in Table~\ref{tab:tab1}.

\begin{table}[!h]
  \setlength{\tabcolsep}{8pt}
  \def\arraystretch{1.1}
  \caption{}
  \label{tab:tab1}
    \centering
    \begin{tabular}{|r|c c|c|}
      \firsthline
                                  & {\bf Happy} & {\bf Not Happy} & {\bf Total} \\ 
      \hline
      {\bf In a Relationship}     & 205         & 301             & 506 \\
      {\bf Not in a Relationship} & 179         & 315             & 494 \\
      \hline
      {\bf Total}                 & 384         & 616             & 1000 \\
      \lasthline
    \end{tabular}
\end{table}

The values of our estimators are as follows:
\begin{align*}
  \phat &= \frac{\text{\# Happy}}{N} = \frac{384}{1000} = 0.384 \\
  \qhat &= \frac{\text{\# In a Relationship}}{N} = \frac{506}{1000} = 0.506 \\
  \rhat &= \frac{\text{\# Happy} \cap \text{\# In a Relationship}}{N} = \frac{205}{1000} = 0.205.
\end{align*}
The estimate of the asymptotic variance of the test statistic is
\[ \hat{V} = \phat\qhat(1-\phat)(1-\qhat) = (0.384)(0.506)(1 - 0.384)(1 - 0.506)
  = 0.05913, \]
giving the test statistic
\[ T_n = \frac{\sqrt{n}(\rhat - \phat\qhat)}{\sqrt{\hat{V}}}
  = \frac{\sqrt{1000}(0.205 - 0.384\cdot0.506)}{\sqrt{0.05913}} = 1.391. \]
The standard normal quantile at $\alpha = 0.05$ is $q_{\alpha/2}
= \Phi^{-1}\mleft(1
- \frac{\alpha}{2}\mright) = 1.96$, so the test result is
\[ |T_n| = 1.391 < q_{\alpha/2} = 1.96 \]
so we \emph{fail to reject $H_0$ at the 5\% level}. The $p$-value of the test is
\begin{align*}
  \text{$p$-value} &\coloneqq \Prob{Z > |T_n|} \by{$Z \sim \N{0}{1}$}  \\
                   &= \Prob{Z \le |T_n|} \by{symmetry} \\
                   &= \Prob{Z \le -T_n} + \Prob{Z > T_n} \\
                   &= 2\Phi(-|T_n|) \\
  \implies \text{$p$-value} &= 0.1642.
\end{align*}
In other words, the lowest level at which we could reject the null hypothesis is
at $\alpha = \text{$p$-value} = 0.1642 = 16.42\%$.

\clearpage
\subsection*{Appendix A: Additional Proofs} \label{app:t_to_N}
\begin{prop}
  A $t$-distribution with $n$ degrees of freedrom approaches a standard normal
  distribution as $n$ approaches infinity:
  \[ t_n \Dlim \N{0}{1}. \]
\end{prop}

\emph{Proof.}
Student's $t$-distribution with $\nu$ degrees of freedom is defined as the
distribution of the random variable $T$ such that
\[ t_{\nu} \sim T = \ddfrac{Z}{\sqrt{\frac{V}{\nu}}} \]
where $Z \sim \N{0}{1}$, $V \sim \chi^2_{\nu}$, and $Z \indep V$.

Let $X_1, \dots, X_n \sim \N{\mu}{\sigma^2}$ be a sequence of \iid\ random
variables. Define the sample mean and sample variance
\begin{align*}
  \Xnbar &\coloneqq \avg{i}{n} X_i \\
  S_n^2 &\coloneqq \avg{i}{n} (X_i - \Xnbar)^2.
\end{align*}
Let the random variables
\begin{align*}
  Z &= \frac{\sqrt{n}(\Xnbar - \mu)}{\sigma} \\
  V &= \frac{n S_n^2}{\sigma^2}.
\end{align*}
such that $Z \sim \N{0}{1}$ by the Central Limit Theorem, and $V \sim
\chi^2_{n-1}$ by Cochran's Theorem (which also shows that $Z \indep V$).
Then, the $t$-distribution is \emph{pivotal}
\[  t_{n-1} = \ddfrac{Z}{\sqrt{\frac{V}{n-1}}}. \]

\begin{lemma}
  The sample variance converges in probability to the variance,
  \[ S_n^2 \Plim \sigma^2. \]
\end{lemma}

\begin{subproof}
  \begin{alignat*}{3}
    S_n^2 &\coloneqq \avg{i}{n}(X_i - \Xnbar)^2 \\
          &= \avg{i}{n}(X_i^2 - 2 \Xnbar X_i + \Xnbar^2) \\
          &= \avg{i}{n} X_i^2 - \avg{i}{n} 2 \Xnbar X_i + \avg{i}{n} \Xnbar^2  \\
          &= \avg{i}{n} X_i^2 - 2 \Xnbar \avg{i}{n} X_i + \Xnbar^2  \\
          &= \avg{i}{n} X_i^2 - 2 \Xnbar^2  + \Xnbar^2  \\
          &= \avg{i}{n} X_i^2 - \Xnbar^2.
  \end{alignat*}
  The second term in the expression for $S_n^2$ is determined by
  \begin{alignat*}{3}
    \Xnbar &\Plim \E{X} \by{LLN} \\
    \E{X} &= \mu \by{given}. \\
    g(\Xnbar) &\Plim g(\mu) \by{CMT} \\
    \implies \Xnbar^2 &\Plim \mu^2.
  \end{alignat*}
  The first term in the expression for $S_n^2$ is then determined by
  \begin{alignat*}{3}
    \avg{i}{n} X_i^2 &\Plim \E{X^2} \by{LLN} \\
    \Var{X} &= \E{X^2} - \E{X}^2 \by{definition} \\
    \implies \E{X^2} &= \Var{X} + \E{X}^2 \\
                     &= \sigma^2 + \mu^2. \by{given} \\
    \therefore S_n^2 &\Plim \sigma^2 + \mu^2 - \mu^2 \\
    \implies S_n^2 &\Plim \sigma^2 \tag*{\qedhere}
  \end{alignat*}
\end{subproof}

Thus, $V \Plim \frac{n \sigma^2}{\sigma^2} = n$, a constant. 

\begin{theorem}[Slutsky's Theorem]
  If the sequences of random variables $X_n~\Dlim~X$, and $Y_n~\Dlim~c$,
  a constant, then
  \begin{alignat*}{3}
    X_n + Y_n &\Dlim X + c \text{, and} \\
    X_n Y_n &\Dlim cX.
  \end{alignat*}
\end{theorem}

% TODO rewrite all in terms of unbiased sample variance
Since convergence in probability implies convergence in distribution, and $Z
\Dlim \N{0}{1}$, Slutsky's theorem implies that
\begin{align*}
  t_{n-1} = \ddfrac{Z}{\sqrt{\frac{V}{n-1}}} 
        &\Dlim \ddfrac{\N{0}{1}}{\sqrt{\frac{n}{n-1}}} \\
        \implies t_{n-1} &\Dlim \N{0}{1}.  \qed
\end{align*}

\clearpage
\section{Test of Independence for Samples with Continuous CDF}
Consider the \iid\ pairs of random variables $(X_1, Y_1), \dots, (X_n, Y_n)$
with some continuous distribution. While each pair is independent, $X_i \indep
X_j$ for $i \ne j$, we would like to test if $X_i \indep Y_i$ for all $i$.

Define the hypotheses
\begin{align*}
  H_0 &\colon X_1 \indep Y_1 \\
  H_1 &\colon X_1 \nindep Y_1.
\end{align*}
For $i = 1, \dots, n$, let $R_i$ be the \emph{rank} of $X_i$ in the sample $X_1,
\dots, X_n$. The rank function is defined as
\begin{equation} \label{eq:R_i}
  R_i = \operatorname{rank}(X_i) \coloneqq \#\{j \colon X_j \le X_i, \; i \ne j\} 
\end{equation}
\ie if $X_i = \min\limits_j X_j$, then $R_i = 1$, and if $X_i = \max\limits_j
X_j$, then $R_j = n$. 
Similarly, let $Q_i$ be the rank of $Y_i$ in $Y_1, \dots, Y_n$.

\subsection{Example Experiment}
An example experiment in which testing for independence of two continuous random
variables is important is in a scientific experiment in which two devices are
measured using sensors powered by the same circuitry. We would like to ensure
that the measurements are not correlated.

\subsection{Dependence of the Ranks}
The ranks $R_1, \dots, R_n$ are \emph{not} independent because the rank of $X_i$
in the sample is unique. Therefore, if you know that $R_1 = 1$, \emph{e.g.}, then
$R_2,\dots,R_n \ne 1$.

\subsection{Proof of Distribution of Ranks} \label{subsec:3}
\begin{prop}
  The distribution of the vector $(R_1, \dots, R_n)$ does \emph{not} depend on
  the distribution of $X_i$'s (and, similarly, the distribution of $(Q_1,
  \dots, Q_n)$ does not depend on the distribution of $Y_i$'s).
\end{prop}

\begin{proof}
  Given the definition of $R_i$
  \[ R_i = \operatorname{rank}(X_i) \coloneqq \#\{j \colon X_j \le X_i, \; i \ne j\}, \tag{\ref{eq:R_i}} \]
  we can determine its distribution. 
  Let 
  \[ F_n(t) = \avg{i}{n} \indic{X_i \le t} \] 
  for some $t \in \R$ be the empirical cdf of the sample $X_i$'s. 
  We can then rewrite the definition of the rank as
  \[ R_i = \sumi{j}{n} \indic{X_j \le X_i} = n F_n(X_i). \]
  The cdf of $R_i$, $F_R(t)$ is then
  \begin{align*}{3}
    \Prob{R_i \le t} &= \Prob{n F_n(X_i) \le t} \\
                     &= \Prob{X_i \le F_n^{-1}\mleft(\frac{t}{n}\mright)} \\
                     &= F_n\mleft(F_n^{-1}\mleft(\frac{t}{n}\mright)\mright)
  \end{align*}
  Since $F_n$ is a piecewise-constant function, $F_n \in \frac{1}{n} \{0, \dots, n\}$,
  it only takes discrete values
  \[ \frac{\floor{t}}{n} \quad t \in [0, n], \]
  regardless of the sample on which it is based.
  Therefore,
  \begin{align*}
    \Prob{R_i \le t} &= \frac{\floor{t}}{n} \\
    \implies F_R(t) &= \frac{\floor{t}}{n} \\
    \implies f_R(t) &= \mathcal{U}(\{0,\dots,n\}) \\
    \implies F_R(t) &\indep F_X(t) \qedhere
  \end{align*}
\end{proof}
An interpretation of this result is that $(R_1, \dots, R_n)$ is a permutation of
the vector $(1, \dots, n)$, with equal probability of any permutation since the
$X_i$'s are \iid.

\subsection{The Null Hypothesis} \label{subsec:4}
\begin{prop}
  If $H_0$ is true, then $(R_1, \dots, R_n) \indep (Q_1, \dots, Q_n)$.
\end{prop}

\begin{proof}
  Under $H_0$, $X_i \indep Y_i$, so
  \[ \Prob{X = x \land Y = y} = \Prob{X=x}\Prob{Y=y} \]
  Since the empirical cdfs $F_n$ and $G_n$ of $X_i$ and $Y_j$, respectively, are
  monotonic and increasing, we can apply them to every value to get the expression
  \[ \begin{split}
      &\Prob{nF_n(X) = nF_n(x) \land nG_n(Y) = nG_n(y)} \\
      &\quad = \Prob{nF_n(X)=nF_n(x)}\Prob{nG_n(Y)=nG_n(y)}.
      \end{split}
  \]
  The rank $R_i = n F_n(X_i)$, and, similarly, $Q_j = n G_n(Y_j)$, therefore
  \begin{align*}
    \Prob{R_i = i \land Q_j = j} &= \Prob{R_i = i}\Prob{Q_j = j} \\
    \implies R &\indep Q. \qedhere
  \end{align*}
\end{proof}

\subsection{Conclusion Under the Null Hypothesis} \label{subsec:5}
Under $H_0$, we have just shown that $R \indep Q$, so the joint distribution of
the $2n$ random variables $R_1, \dots, R_n, Q_1, \dots, Q_n$ is a discrete
uniform distribution on $[0, n]$, and does not depend on the distributions of
the $X$'s or $Y$'s.

\subsection{The Test Statistic} \label{subsec:6}
Consider the test statistic
\[ T_n \coloneqq \ddfrac{\sumi{i}{n}(R_i - \Rnbar)(Q_i - \Qnbar)}
                        {\sqrt{\sumi{i}{n} (R_i - \Rnbar)^2 \sumi{i}{n} (Q_i - \Qnbar)^2}}. \]
This test statistic is the empirical correlation between the two samples. If
$H_0$ is true, then $T_n \to 0$. We will now show that $T_n$ has a much simpler
expression.

\subsubsection{}
\begin{prop} \label{prop:rn1}
  \[ \Rnbar = \Qnbar = \frac{n+1}{2} \]
\end{prop}

\begin{proof}
  Because $R \indep Q$ and $R, Q$ are each permutations of $(1, \dots, n)$,
  $\Rnbar = \Qnbar$.
  \begin{alignat*}{3}
    \Rnbar &\coloneqq \avg{i}{n} R_i \\
           &= \avg{i}{n} i \by{range of $R_i$} \\
           &= \frac{1}{n} \frac{n(n+1)}{2} \by{geometric series} \\
   \implies \Rnbar &= \frac{n+1}{2} \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\begin{prop} \label{prop:rn2}
  \[  \sumi{i}{n} (R_i - \Rnbar)^2 = \sumi{i}{n} (Q_i - \Qnbar)^2 = \frac{n(n^2 - 1)}{12} \]
\end{prop}

\begin{proof}
  The first equality is true because $R, Q$ are each permutations of $(1, \dots, n)$.
  The sum evaluates to
  \begin{alignat*}{3}
    \sumi{i}{n} (R_i - \Rnbar)^2 &= \sumi{i}{n} (R_i^2 - 2\Rnbar R_i + \Rnbar^2) \\
    &= \sumi{i}{n} \mleft(i^2 - 2 i \frac{n+1}{2} + \mleft(\frac{n+1}{2}\mright)^2 \mright) \\
    &= \sumi{i}{n} i^2 - (n+1)\sumi{i}{n} i + \frac{(n+1)^2}{4} \sumi{i}{n} 1 \\
    \intertext{Using the fact that $\sumi{i}{n} i^2 = \frac{n(n+1)(2n+1)}{6}$,
      and the previous result,}
    &= \frac{n(n+1)(2n+1)}{6} - (n+1) \frac{n(n+1)}{2} + \frac{n(n+1)^2}{4} \\
    &= \frac{2n^3 + 3n^2 + n}{6} - \frac{n^3 + 2n^2 + n}{2} + \frac{n^3 + 2n^2 + n}{4}  \\
    &= \frac{1}{12} (4n^3 + 6n^2 + 2n - 6n^3 - 12n^2 - 6n + 3n^3 + 6n^2 + 3n) \\
    &= \frac{1}{12} (n^3 - n) \\
    &= \frac{n(n^2 - 1)}{12} \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\subsubsection{}
\begin{prop}
  The test statistic can be written as
  \[ T_n = \frac{12}{n(n^2 - 1)} \sumi{i}{n}R_iQ_i - \frac{3(n+1)}{n-1}. \]
\end{prop}

\begin{proof}
Plugging the expressions from Propositions~\ref{prop:rn1} and~\ref{prop:rn2}
into the definition of $T_n$,
\begin{align*}
  T_n &\coloneqq \ddfrac{\sumi{i}{n}(R_i - \Rnbar)(Q_i - \Qnbar)}
                            {\sqrt{\sumi{i}{n} (R_i - \Rnbar)^2 \sumi{i}{n} (Q_i - \Qnbar)^2}} \\
  &= \ddfrac{\sumi{i}{n}\mleft(R_i - \frac{n+1}{2}\mright)\mleft(Q_i - \frac{n+1}{2}\mright)}
                            {\sqrt{\frac{n(n^2 - 1)}{12} \frac{n(n^2 - 1)}{12}}} \\
  &= \ddfrac{\sumi{i}{n}\mleft(R_iQ_i 
                              - \frac{n+1}{2}(R_i + Q_i) 
                              + \mleft(\frac{n+1}{2}\mright)^2 \mright)}
                            {\frac{n(n^2 - 1)}{12}} \\
  &= \frac{12}{n(n^2 - 1)} \mleft[ \sumi{i}{n}R_iQ_i 
  + \mleft(\frac{n+1}{2}\mright)^2  \sumi{i}{n} 1
  - \frac{n+1}{2}\sumi{i}{n}(R_i + Q_i) \mright]
\end{align*}
The first term is as desired. The remaining two terms simplify to
\begin{align*}
  &\phantom{=} \frac{12}{n(n^2 - 1)} \mleft[ \mleft(\frac{n+1}{2}\mright)^2  \sumi{i}{n} 1
  - \frac{n+1}{2}\sumi{i}{n}(R_i + Q_i) \mright]  \\
  &= \frac{12}{n(n^2 - 1)} \mleft[ \frac{n(n+1)^2}{4} - \frac{n+1}{2}\sumi{i}{n}2i \mright] \\
  &= \frac{12}{n(n^2 - 1)} \mleft[ \frac{n(n+1)^2}{4} - (n+1)\frac{n(n+1)}{2} \mright] \\
  &= \frac{12}{n(n+1)(n-1)} \mleft[ \frac{n(n+1)^2}{4} - \frac{n(n+1)^2}{2} \mright] \\
  &= \frac{12}{n-1} \mleft[ \frac{n+1}{4} - \frac{n+1}{2} \mright] \\
  &= \frac{12}{n-1} \mleft[ -\frac{n+1}{4} \mright] \\
  &= \frac{3(n+1)}{n-1}
\end{align*}
so the entire expression is proven.
\end{proof}

\subsection{The Test Statistic is Distributed as the Sample Variance}
\begin{prop}
  If $H_0 \colon X \indep Y$ is true, then
  \[ T_n \sim S_n = \frac{12}{n(n^2 - 1)} \sumi{i}{n} R_i' Q_i' - \frac{3(n+1)}{n-1}, \]
  where $(R_1', \dots, R_n')$ and $(Q_1', \dots, Q_n')$ are ranks of two \iid\
  samples of $\U{0}{1}$.
\end{prop}

\begin{proof}
  Using the previous arguments, the proof is as follows:
  \begin{itemize}
    \item By the argument in \S\ref{subsec:3}, the distributions of $R_i', Q_i'$ do not depend on the distributions of the underlying random variables.
    \item By the argument in \S\ref{subsec:4}, the vector $(R_1', \dots, R_n')
      \indep (Q_1', \dots, Q_n')$ under $H_0$, so their distribution is known,
      and is the same discrete uniform distribution as $R, Q$.
    \item By the argument in \S\ref{subsec:6}, $T_n = S_n(R, Q)$.
  \end{itemize}
  Therefore, by the Continuous Mapping Theorem, $T_n \sim S_n$.
\end{proof}

\subsection{Computing Quantiles of the Test Statistic}
Let $\alpha \in (0, 1)$. Algorithm~\ref{alg:sn_q} approximates the $(1-\alpha)$-quantile
of $S_n$, $q_\alpha$.

\begin{algorithm}[!h]
  \caption{Approximate $q_\alpha$, the $(1 - \alpha)$-quantile of the
distribution of $S_n$ under $H_0$.}
  \label{alg:sn_q}
  \begin{algorithmic}
    \Require $M, n \in \mathbb{N}$. $\alpha \in (0, 1)$.
    \Ensure $q_\alpha \in [0, 1]$.
    \Procedure{SnQuantile}{$n, M, \alpha$}
      \State $S_v \gets$ empty array of size $M$
      \ForAll{$i \in \{1,\dots,M\}$}
        \State $R \gets$ random permutation of $(1, \dots, n)$.
        \State $Q \gets$ random permutation of $(1, \dots, n)$.
        \State $S_v^{(i)} \gets \frac{12}{n(n^2 - 1)} \langle R, Q \rangle - \frac{3(n+1)}{n-1}$
      \EndFor
      \State $S_{vs} \gets$ \Call{Sort}{$S_v$}
      \State $j \gets \ceil*{M\alpha}$
      \State \Return $S_{vs}^{(j)}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{A Non-Asymptotic Hypothesis Test}
Redefine the hypotheses in terms of the test statistic
\begin{align*}
  H_0 &\colon T_n = 0 \\
  H_1 &\colon T_n \ne 0
\end{align*}
Let $\alpha \in (0, 1)$. The hypothesis test is then
\[ \delta_\alpha = \indic{|T_n| \ge \hat{q}_{\alpha/2}^{(n, M)}} \]
where $\hat{q}_{\alpha/2}^{(n, M)}$ is the estimated $(1 - \alpha)$-quantile
of $T_n$. The $p$-value for this test is
\begin{align*}
  \text{$p$-value} &\coloneqq \Prob{|Z| \ge |T_n|} \\
  &\approx \frac{\#\{j=1,\dots,M \colon |S_v^{(j)}| \ge |T_n|\}}{M}
\end{align*}
where $S_v^{(j)}$ is the $j^{\text{th}}$ sample of $S_n$, as computed in
Algorithm~\ref{alg:sn_q}.

% TODO include subroutine code, plot of the distribution


% % INCLUDE CODE:
% \clearpage
% \subsection*{Code}
% \renewcommand{\baselinestretch}{1.0}
% \lstinputlisting[language=matlab]{../engs250_2_1_wallshear.m}

%===============================================================================
% References:
%===============================================================================
% \clearpage
% \lhead{References}
% \bibliographystyle{apalike}
% \bibliography{engg149_finalbib}

%===============================================================================
%   Source Code {{{
%===============================================================================
% \clearpage
% \appendix   % BEGIN APPENDIX NUMBERING
% \lhead{Roesler, Final Exam, Source Code} % clear "Problem #" header
%
% \renewcommand{\baselinestretch}{1.0}
%
% \section{Main Script for Problems 1--4} \label{app:code1}
% \ % keep '\ 'space here to include "Source Code" appendix header
% \lstinputlisting[language=matlab]{../engg149_final_mimo.m}
%
% }}}


\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
