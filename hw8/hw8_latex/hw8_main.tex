\documentclass[letterpaper, reqno]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[comma, compress, numbers, square]{natbib}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}  % for indicator function
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{mathrsfs}  % mathscr
\usepackage{mleftright}
\usepackage{enumerate}
\usepackage{array, multirow}
\usepackage[section, ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage[font=small, hypcap=true]{caption}                % link to top of figures and subfigures
\usepackage[font=small, hypcap=true, list=true]{subcaption}   % use for subfigures instead of {subfigure}

\usepackage{hyperref}
% \usepackage{nameref}  % refer to unnumbered sections
\hypersetup{
    linktoc=all,    % link table of contents to sections
    colorlinks,
    allcolors=black,
    urlcolor=dblue,
  }
\usepackage{cleveref}

% Declare theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}{Proposition}[section]

\renewcommand{\qedsymbol}{\ensuremath{\blacksquare}}

% new environment for proofs of claims within proofs
\newenvironment{subproof}[1][\proofname]{%
  \renewcommand{\qedsymbol}{\ensuremath{\square}}%
  \begin{proof}[#1]%
}{%
  \end{proof}%
}

\numberwithin{equation}{section}

% lists as 1., 2., ...
\renewcommand{\labelenumi}{\theenumi.}

% Letter subsections
\renewcommand{\thesubsubsection}{\thesubsection(\alph{subsubsection})}

% number only specific equation in, say, align* environment
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}

% Algorithm
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}

%-------------------------------------------------------------------------------
%     {listings + color} package options
%-------------------------------------------------------------------------------
% Define colors for code
\usepackage{xcolor}
\definecolor{gray}{rgb}{0.6, 0.6, 0.6}
\definecolor{mauve}{rgb}{0.58, 0, 0.82}
\definecolor{dblue}{HTML}{0645AD}
\definecolor{lblue}{HTML}{1F77B4}

\usepackage{listings}
\lstset{
  language = Python,
  basicstyle = \scriptsize\ttfamily,
  numbers = left,
  numberstyle = \tiny\color{gray},
  stepnumber = 1,
  numbersep = 8pt,
  breaklines = true,
  keywordstyle = \bfseries\color{lblue},
  commentstyle = \color{gray},
  % stringstyle = \color{mauve}
}

% Place loose figures at actual top of page
% \makeatletter
%   \setlength{\@fptop}{0pt}
% \makeatother

%-------------------------------------------------------------------------------
% NO HYPHENATION
%-------------------------------------------------------------------------------
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

%==============================================================================
% General macros
%==============================================================================
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand{\T}{\top} % transpose symbol
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}} % use BOTH to cover all cases
\newcommand{\ith}{$i^\mathrm{th}$}
\newcommand{\jth}{$j^\mathrm{th}$}
% \newcommand{\norm}[1]{{\lVert {#1} \rVert}_2}

% Misc.
\newcommand{\ie}{\emph{i.e.\ }}
\newcommand{\eg}{\emph{e.g.\ }}

% Use for fractions with large terms
\newcommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}

% Derivatives
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ddx}[2]{\frac{\dd #1}{\dd #2}}
\newcommand{\dx}[1]{\,\dd #1} % for inside integration

% Probability helpers
\newcommand{\Prob}[1]{\mathbb{P}\mleft[#1\mright]}
\newcommand{\E}[1]{\mathbb{E}\mleft[#1\mright]}
\newcommand{\V}[1]{\mathbb{V}\mleft[#1\mright]}
\newcommand{\Var}[1]{\operatorname{Var}\mleft(#1\mright)}
\newcommand{\Cov}[1]{\operatorname{Cov}\mleft(#1\mright)}
\newcommand{\Li}[1]{\mathcal{L}\mleft( #1 \mright)}  % likelihood
\newcommand{\R}{\mathbb{R}}  % real numbers
\newcommand{\N}[2]{\mathcal{N}\mleft( #1, #2 \mright)}
\newcommand{\U}[2]{\mathcal{U}\mleft(\mleft[ #1, #2 \mright]\mright)}
\newcommand{\indep}{\perp \!\!\! \perp}  % "is independent from"
\newcommand{\nindep}{\centernot\indep}
\newcommand{\indic}[1]{\mathbbm{1}\!\mleft\{#1\mright\}} % indicator function
\newcommand{\iid}{i.i.d.}

\newcommand{\sumi}[2]{\sum_{#1=1}^{#2}}
\newcommand{\avg}[2]{\frac{1}{#2}\sumi{#1}{#2}}

\newcommand{\by}[1]{&\quad&\text{(#1)}}

\newcommand{\Alim}{\xrightarrow[n \to \infty]{\text{a.s.}}}
\newcommand{\Plim}{\xrightarrow[n \to \infty]{\mathbb{P}}}
\newcommand{\Dlim}{\xrightarrow[n \to \infty]{(d)}}

\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}

\newcommand{\Xnbar}{\overline{X}_n}
\newcommand{\Rnbar}{\overline{R}_n}
\newcommand{\Qnbar}{\overline{Q}_n}

% Vectors/Matrices
\newcommand{\Xm}{\vect{X}}
\newcommand{\Yv}{\vect{Y}}
\newcommand{\Bv}{\beta}
\newcommand{\Bvh}{\hat{\beta}}
\newcommand{\ve}{\varepsilon}
\newcommand{\Ebh}{\mathbb{E}[\Bvh]}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Ber}{Ber}


%%%% TITLE ----------------------------
\title[Homework 8 -- Problem \thesection]{18.650 Fundamentals of Statistics\\{\large Homework 8}}
\author{Bernie Roesler}
\date{\today}
%%%%

%%%%%%% BEGIN DOCUMENT ----------------
\begin{document}

\graphicspath{{./figures/}}

\maketitle

\section{Heteroscedastic Regression}
Let the characteristics $(X_i, y_i)$ of $n$ individuals $(i = 1, \dots, n)$ be
observed, where $y_i \in \R$ is the dependent variable, and $X_i \in \R^p$ is
the vector of deterministic explanatory variables. The goal is to estimate the
vector of coefficients $\Bv = (\beta_1, \dots, \beta_n)^\T$ in the linear
regression
\[ y_i = X_i^\T \Bv + \ve_i, \quad i = 1, \dots, n. \]
Assume that the model is \emph{heteroscedastic}, \ie\ the error terms $\ve_i$
are not \iid, but $\ve = (\ve_1, \dots, \ve_n)^\T \in \R^n$ is Gaussian,
centered, with known invertible covariance matrix $\Sigma \in \R^{n \times n}$.
Often, the errors are assumed to be independent, but not identically
distributed. In this case, the covariance matrix is diagonal,
\[ \Sigma = \begin{bmatrix} \sigma_1^2 && \\ & \ddots & \\ && \sigma_n^2 \end{bmatrix}. \]
In the case where errors are also not independent, $\Sigma$ will have non-zero
off-diagonal entries, but is valid as long as it is symmetric and invertible. 
Let $\Xm \in \R^{n \times p}$ be the matrix
\[ \Xm = \begin{bmatrix} 
            \text{---} X_1^\T \text{---} \\
            \vdots \\ 
            \text{---} X_n^\T \text{---} 
          \end{bmatrix} 
\]
and $\Yv = (y_1, \dots, y_n)^\T \in \R^n$.
The linear regression is then
\[ \Yv = \Xm\Bv + \ve. \]

Consider the estimator
\[ \Bvh = \argmin_{\Bv \in \R^p} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv). \]

\subsection{The Estimator Under Homoscedasticity}
\begin{prop}
  If the errors are homoscedastic, \ie\ $\Sigma = \sigma^2 \vect{I}_n$, where $\sigma^2 > 0$, and
  $\vect{I}_n$ is the $n \times n$ identity matrix, then $\Bvh$ reduces to the least
  squares estimator
  \[ \beta_\mathrm{LSE} = (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv). \]
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \Bvh &\coloneqq \argmin_\Bv (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \\
         &= \argmin_\Bv (\Yv - \Xm\Bv)^\T (\sigma^2 \vect{I}_n)^{-1} (\Yv - \Xm\Bv) \\
         &= \argmin_\Bv \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T \vect{I}_n (\Yv - \Xm\Bv) \by{inverse properties} \\
         &= \argmin_\Bv \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T (\Yv - \Xm\Bv) \by{definition of identity} \\
    \implies \Bvh &= \argmin_\Bv (\Yv - \Xm\Bv)^\T (\Yv - \Xm\Bv) = \beta_\mathrm{LSE} \by{$\sigma^2 > 0$ and constant} \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\subsection{The Maximum Likelihood Estimator}
\begin{prop}
  The estimator $\Bvh$ is the maximum likelihood estimator.
\end{prop}

\begin{proof}
  If $\ve \sim \N{0}{\Sigma}$, then
  \[ f_Y(\Yv) = \frac{1}{\sqrt{(2\pi)^n \det \Sigma}}
      \exp \mleft[-\frac{1}{2} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright]. \]
  The likelihood is defined as
  \begin{align*}
    L(\Bv; \Sigma) &\coloneqq \prod_{i=1}^m f_Y(\Yv_i)  \\
    &= \frac{1}{\sqrt{(2\pi)^n \det \Sigma}}
      \exp \mleft[-\frac{1}{2} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright]
  \end{align*}
  To maximize the likelihood, we take the log-likelihood
  \begin{align*}
    \ell(\Bv; \Sigma) &= \log L(\Bv, \Sigma) \\
    &= \log \frac{1}{\sqrt{(2\pi)^n \det \Sigma}}
      \exp \mleft[-\frac{1}{2} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright] \\
    &= -\frac{1}{2} \mleft[n \log 2\pi + \log \det \Sigma 
        + (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright] \\
    \intertext{When maximizing with respect to $\Bv$, we ignore terms that are
      not functions of $\Bv$,}
    \therefore \Bvh_\mathrm{MSE} &\coloneqq \argmax_\Bv \ell(\Bv; \Sigma) \\
    &= \argmax_\Bv -\frac{1}{2} \mleft[n \log 2\pi + \log \det \Sigma 
        + (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright] \\
    \implies \Bvh = \Bvh_\mathrm{MSE} &= \argmin_\Bv (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv)   \qedhere
  \end{align*}
\end{proof}

\subsection{A Sufficient Condition on the Dependent Matrix}
\begin{prop}
  A sufficient condition on the dependent matrix $\Xm$ for the estimator $\Bv$ to
  be uniquely defined is
  \[ \rank \Xm = p \le n. \]
\end{prop}

\begin{proof}
  Given the data, the error in the linear regression
  \[ \Yv = \Xm\Bv + \ve \]
  is minimized by projecting $\Yv$ onto the column space of $\Xm$,
  $\mathscr{C}(\Xm)$, by multiplying each side by $\Xm^\T$,
  \[ \Xm^\T\Yv = \Xm^\T\Xm\Bvh \]
  such that
  \[ \Bvh = (\Xm^\T\Xm)^{-1} \Xm^\T \Yv. \]
  This expression is unique if the inverse $(\Xm^\T\Xm)^{-1}$ is unique and non-singular.
  Since
  \[ \rank \Xm^\T\Xm = \rank \Xm, \]
  for any matrix $X$, and $\Xm^\T \Xm \in \R^{p \times p}$, $\rank \Xm = p$ is
  a sufficient condition for the existence of the inverse, and thus for the
  uniqueness of $\Bvh$.
\end{proof}

\subsection{The Heteroscedastic Estimator}
\begin{prop}
  The optimal heteroscedastic estimator is
  \[ \Bvh = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Yv. \]
\end{prop}

\begin{proof}
  \begin{align*}
    \Bvh &\coloneqq \argmin_t (\Yv - \Xm t)^\T \Sigma^{-1} (\Yv - \Xm t) \\
    &= \argmin_t \Yv^\T \Sigma^{-1} \Yv + t^\T \Xm^\T \Sigma^{-1} \Xm t - 2t^\T
    \Xm^\T \Sigma^{-1} \Yv.
  \end{align*}
  The minimum occurs when the gradient of the argument is 0 with respect to the
  variable over which we are optimizing. The gradient with respect to $t$ is
  \begin{align*}
    \nabla_t &= 2 \Xm^\T \Sigma^{-1} \Xm t - 2 \Xm^\T \Sigma^{-1} \Yv \\
             &= 0 \quad \text{for } t = \Bvh \\
    \implies& \Xm^\T \Sigma^{-1} \Xm \Bvh = \Xm^\T \Sigma^{-1} \Yv \\
    \implies& \Bvh = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Yv.
    \qedhere
  \end{align*}
\end{proof}

\begin{prop} \label{prop:BvN}
  The estimator is normally distributed, with mean and variance
  \[ \Bvh \sim \N{\Bv}{(\Xm^\T \Sigma^{-1} \Xm)^{-1}}. \]
\end{prop}

\begin{proof}
  From our expression for $\Bvh$, insert the expression for the data $\Yv
  = \Xm\Bv + \ve$
  \begin{align*}
    \Bvh &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Yv \\
         &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} (\Xm\Bv + \ve) \\
         &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} (\Xm^\T \Sigma^{-1} \Xm)\Bv 
          + (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \ve \\
         &= \Bv + (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \ve.
  \end{align*}
  Let
  \[ B = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \]
  be the error coefficient matrix, such that
  \[ \Bvh = \Bv + B\ve. \]

  \begin{theorem} \label{thm:Bx}
    If $X \sim \N{0}{\Sigma}$, then $a + BX \sim \N{a}{B \Sigma B^\T}$,
    for any constant vector $a$, matrix $B$ and invertible covariance matrix
    $\Sigma$.
  \end{theorem}

  Since the error is assumed to be normally distributed,
  $\ve \sim \N{0}{\Sigma}$, by \Cref{thm:Bx},
  \[ \Bvh \sim \N{\Bv}{B \Sigma B^\T}. \]
  The covariance matrix is then
  \begin{alignat*}{3}
    B \Sigma B^\T &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Sigma
                    ((\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1})^\T \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T 
                    ((\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1})^\T \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T 
                    {\Sigma^{-1}}^\T \Xm {(\Xm^\T \Sigma^{-1} \Xm)^{-1}}^\T \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T 
                    {\Sigma^\T}^{-1} \Xm {(\Xm^\T \Sigma^{-1} \Xm)^\T}^{-1}
                    \by{${(A^{-1})}^\T = {(A^\T)}^{-1}$} \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} 
                     (\Xm^\T \Sigma^{-1} \Xm) (\Xm^\T \Sigma^{-1} \Xm)^{-1}
                    \by{$\Sigma^\T = \Sigma$} \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} 
  \end{alignat*}
  so the estimator $\Bvh$ is indeed distributed normally
  \[ \Bvh \sim \N{\Bv}{(\Xm^\T \Sigma^{-1} \Xm)^{-1}}. \qedhere \]
\end{proof}

\subsection{Bias and Quadratic Risk of the Estimator}
\begin{prop} \label{prop:bhat_nobias}
  The estimator $\Bvh$ is unbiased.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Bias}_\Bv(\Bvh) &\coloneqq \E{\Bvh - \Bv} \\
    &= \Ebh - \E{\Bv} \by{linearity of expectation} \\
    &= \Ebh - \Bv \\
    &= \Bv - \Bv \by{\Cref{prop:BvN}} \\
    &= 0. \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\begin{prop}
  The risk of the estimator $\Bvh$ is the trace of its covariance matrix.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Risk}_\Bv(\Bvh) &\coloneqq \E{(\Bvh - \Bv)^\T(\Bvh - \Bv)} \\
    &= \E{\norm*{\Bvh - \Bv}_2^2} \\
    &= \E{\norm*{\Bvh - \Ebh + \Ebh - \Bv}_2^2} \\
    &= \E{\norm*{\Bvh - \Ebh}_2^2 
      + 2(\Bvh - \Ebh)^\T (\Ebh - \Bv)
      + \norm*{\Ebh - \Bv}_2^2} \\
    &= \E{\norm*{\Bvh - \Ebh}_2^2}
      + 2\E{(\Bvh - \Ebh)^\T (\Ebh - \Bv)} 
      + \E{\norm*{\Ebh - \Bv}_2^2}.
  \end{alignat*}
  The second term is identically zero.
  \[ 2\E{(\Bvh - \Ebh)^\T (\Ebh - \Bv)} = 2\E{(\Bvh - \Ebh)^\T}(\Ebh - \Bv) \]
  because $\Ebh - \Bv$ is deterministic, and 
  \[ \E{(\Bvh - \Ebh)^\T} = 0 \]
  by definition. The risk is then
  \[ \mathrm{Risk}_\Bv(\Bvh) = \E{\norm*{\Bvh - \Ebh}_2^2} + \E{\norm*{\Ebh - \Bv}_2^2}. \]
  The second term becomes
  \[ \E{\norm*{\Ebh - \Bv}_2^2} = \norm*{\Ebh - \Bv}_2^2 \]
  because the argument is deterministic. We recognize this expression as the
  norm of the bias.
  \[ \norm*{\Ebh - \Bv}_2^2 = \norm*{\mathrm{Bias}_\Bv(\Bvh)}_2^2. \]
  To evaluate the first term we apply the following theorem.
  \begin{theorem} \label{thm:normnorm}
    If $X \sim \N{0}{\Sigma} \in \R^n$, then $X^\T X = \norm*{X}_2^2 = \tr \Sigma$.
  \end{theorem}
  Since 
  \[ \Bvh - \E{\Bvh} \sim \N{0}{(\Xm^\T \Sigma^{-1} \Xm)^{-1}}, \]
  \begin{align*}
    \E{\norm*{\Bvh - \Ebh}_2^2} &= \E{\tr (\Xm^\T \Sigma^{-1} \Xm)^{-1}} \\
    &= \tr (\Xm^\T \Sigma^{-1} \Xm)^{-1} \\
    &= \tr \Var{\Bvh}.
  \end{align*}
  \[ \therefore \mathrm{Risk}_\Bv(\Bvh) = \tr \Var{\Bvh} + \norm*{\mathrm{Bias}_\Bv(\Bvh)}_2^2. \]
  As shown by the proof of \Cref{prop:bhat_nobias}, $\Bvh$ is
  unbiased, so
  \begin{align*}
    \mathrm{Risk}_\Bv(\Bvh) &= \tr \Var{\Bvh}  \\
    \implies \mathrm{Risk}_\Bv(\Bvh) &= \tr (\Xm^\T \Sigma^{-1} \Xm)^{-1}.
    \qedhere
  \end{align*}
\end{proof}

\subsection*{Appendix} \label{app:prediction_error}
\begin{quote}
  Q: What is the prediction error of the estimator?
\end{quote}

\begin{prop}
  The prediction error, defined as the expected value of the sum of squared
  residuals, is
  \[ \E{\norm*{\Yv - \Xm\Bvh}_2^2} = \tr M \Sigma M, \]
  where $M$ is the projection matrix onto the null space of $\Xm^\T$,
  $\mathscr{N}(\Xm^\T)$.
\end{prop}

\begin{proof}
  The residuals can be expressed as
  \begin{align*}
    \Yv - \Xm\Bvh &= \Yv - \Xm B \Yv \\
    &= (\vect{I}_n - \Xm B) \Yv
  \end{align*}
  where
  \[ B = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1}. \]
  Let $H \in \R^{n \times n}$ be the projection matrix that maps onto the column
  space of $\Xm$, $\mathscr{C}(\Xm)$:
  \[ H = \Xm B =  \Xm (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1}. \]
  Then, let $M \in \R^{n \times n}$ be the projection matrix onto
  $\mathscr{N}(\Xm^\T)$, which is orthogonal to $\mathscr{C}(\Xm)$.
  \[ M = \vect{I}_n - \Xm B 
        = \vect{I}_n - \Xm (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1}. \]
  The residuals are then
  \[ \Yv - \Xm\Bvh = (\vect{I}_n - H)\Yv = M\Yv. \]
  We can substitute the expression for the $\Yv$ to obtain
  \begin{align*}
    \Yv - \Xm\Bvh &= M \Yv \\
    &= M (\Xm\Bv + \ve) \\
    &= M\Xm\Bv + M\ve.
  \end{align*}
  The projection of $\Xm$ onto the null space of its own transpose $M\Xm\Bv$ is
  always zero,
  \[ \implies \Yv - \Xm\Bvh = M\ve. \]
  The norm of the residuals is then
  \begin{alignat*}{3}
    \norm*{\Yv - \Xm\Bvh}_2^2 = \norm*{M\Yv}_2^2 &= \norm*{M\ve}_2^2 \\
    &= \norm*{\N{0}{M\Sigma M^\T}}_2^2 \by{\Cref{thm:normnorm}} \\
    &= \norm*{\N{0}{M\Sigma M}}_2^2 \by{$M^\T = M$} \\
    &= \tr M\Sigma M \by{\Cref{thm:Bx}} \\
    \implies \norm*{\Yv - \Xm\Bvh}_2^2 &= \tr M\Sigma M \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\begin{prop}
  In the homoscedastic case, where $\Sigma = \sigma^2 \vect{I}_n$, the norm of
  the residuals is  
  \[ \norm*{\Yv - \Xm\Bvh}_2^2 = \sigma^2 (n - p). \]
\end{prop}

\begin{proof}
  In the homoscedastic case, where $\Sigma = \sigma^2 \vect{I}_n$, this expression
  evaluates to 
  \begin{alignat*}{3}
    \norm*{\Yv - \Xm\Bvh}_2^2 &= \tr M(\sigma^2 \vect{I}_n) M \\
    &= \sigma^2 \tr M M \by{trace property} \\
    &= \sigma^2 \tr M \by {$M$ is idempotent}
  \end{alignat*}
  It is property of projection matrices that their eigenvalues $\lambda_i \in
  \{0, 1\}$ only.
  The trace of $M$ is then equal to the number of its non-zero eigenvalues.
  Since $M$ is a projection onto the null space of $\Xm^\T$, it has non-zero
  eigenvalues equal to the dimension of that null space.
  For $\Xm \in \R^{n \times p}$,
  \[ \dim\mathscr{N}(\Xm^\T) = n - \dim \mathscr{C}(\Xm). \]
  The dimension of the column space is the rank
  \[ \dim \mathscr{C}(\Xm) = \rank \Xm = p, \]
  as required for the uniqueness of $\Bvh$. Thus,
  \[ \tr M = \dim\mathscr{N}(\Xm^\T) = n - p, \]
  \[ \implies \norm*{\Yv - \Xm\Bvh}_2^2 = \sigma^2 (n - p). \qedhere \]
\end{proof}

\clearpage
\section{Linear Regression with Random Design}
Consider the \iid\ pairs of random variables $(X_i, Y_i)$, $i = 1, \dots, n$,
where $X_i~\in~\R^p$ $(p \ge 1)$, and $Y_i \in \R$. For each $i$, write
\[ Y_i = X_i^\T \Bv + \ve_i, \]
where $\E{\ve_i = 0}$, $\Cov{X_i, \ve_i} = 0$, and $\Bv \in \R^p$ is an unknown
vector that we want to estimate. Assume $\forall x \in \R^p$, $\ve_1$ has
a density conditional on $X_1 = x$, denoted $f_x$, and $X_1$ has a density $g$.
In other words,
\begin{align*}
  f_x(t) &= \Prob{\ve_1 = t | X_1 = x}, \\
  g(t) &= \Prob{X_1 = t}.
\end{align*}

\subsection{The Likelihood}
The likelihood function is the product of the probabilities of the data, given
the parameters:
\begin{alignat*}{3}
  \Li{X, Y; \Bv} &\coloneqq \prod_{i=1}^n f(X_i, Y_i) \\
  &= \prod_{i=1}^n f_X(X_i) f_{Y|X}(Y_i | X_i) \by{law of total probability} \\
  &= \prod_{i=1}^n f_X(X_i) \times \prod_{i=1}^n f_{Y|X}(Y_i | X_i) \\
  &= \prod_{i=1}^n g(X_i) \times \prod_{i=1}^n f_{Y|X}(Y_i | X_i) \by{definition of $g$}.
\end{alignat*}

\begin{prop}
  The probability of $Y_i$ given $X_i$ is equal to the probability of the error
  $\ve_i$ given $X_i$:
  \[ f_{Y|X}(Y_i | X_i) = f_x\mleft(Y_i - X_i^\T\Bv\mright). \]
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    f_{Y|X}(Y_i | X_i) &= f_{Y|X}(X_i^\T \Bv + \ve_i | X_i) \\
    &= \Prob{Y_i = X_i^\T \Bv + \ve_i | X_i} \by{density definition} \\
    &= \Prob{\ve_i = Y_i - X_i^\T\Bv | X_i} \by{algebraic manipulation} \\
    &= f_x\mleft(Y_i - X_i^\T\Bv\mright). \by{definition of $f_x$} \tag*{\qedhere}
  \end{alignat*}
\end{proof}

Therefore, the likelihood is
  \[ \Li{X, Y; \Bv} = \prod_{i=1}^n g(X_i) \times \prod_{i=1}^n f_x\mleft(Y_i - X_i^\T\Bv\mright). \]

\subsection{The Maximum Likelihood Estimator}
\begin{prop}
  The maximum likelihood estimator, $\Bvh$, does not depend on the distribution
  of the design random variables $X$, $g$.
\end{prop}

\begin{proof}
  The maximum likelihood estimator is the $\Bvh$ that maximizes the likelihood
  function
  \[ \Bvh \coloneqq \argmax_{\Bv \in \R^p} \prod_{i=1}^n g(X_i) 
    \times \prod_{i=1}^n f_x\mleft(Y_i - X_i^\T\Bv\mright). \]
  Since $g(x) = \Prob{X_1 = x}$ does not depend on $\Bv$, the definition of
  $\Bvh$ is equivalent to
  \[ \iff \Bvh = \argmax_\Bv \prod_{i=1}^n f_x\mleft(Y_i - X_i^\T\Bv\mright) \]
  which itself does not depend on $g$.
\end{proof}

\subsection{Gaussian Error}
Assume that $\ve_1 \indep X_1$ and $\ve_1 \sim \N{0}{\sigma^2}$,
with $0~<~\sigma^2~<~\infty$.

\subsubsection{The Conditional Error Density}
Given $X_1$, $\ve_1 = Y_1 - X_1^\T\Bv$, so
\[ f_x = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \mleft[\frac{-(Y_1 - X_1^\T\Bv)^2}{2\sigma^2}\mright]. \]

\subsubsection{The Least Square Estimator}
Since $X_1, \dots, X_n \in \R^p$ are \iid, the rank of
\[ \Xm = \begin{bmatrix} 
            \text{---} X_1^\T \text{---} \\
            \vdots \\ 
            \text{---} X_n^\T \text{---} 
          \end{bmatrix} 
\]
is almost surely $p$.

\begin{theorem}
  Under the assumption of normal error $\ve_1 \sim \N{0}{\sigma^2}$, the maximum
  likelihood estimator is equal to the least squares estimator.
\end{theorem}

\begin{proof}
  First, we write down the least squares estimator
  \begin{align*}
    \Bvh_\mathrm{LSE} &\coloneqq \argmin_{\Bv \in \R^p} \ve^\T \ve \\
                      &= \argmin_{\Bv \in \R^p} (\Yv - \Xm\Bv)^\T (\Yv - \Xm\Bv).
  \end{align*}
  Under the assumption of normality, we can write down the log-likelihood.
  \begin{align*}
    \Li{X, Y; \Bv, \sigma^2} &\propto \prod_{i=1}^n f_x\mleft(Y_i - X_i^\T\Bv\mright) \\
    &= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} 
       \exp \mleft[\frac{-(Y_i - X_i^\T\Bv)^2}{2\sigma^2}\mright] \\
    &= \frac{1}{(2\pi\sigma^2)^\frac{n}{2}} 
       \exp \mleft[\frac{-(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv)}{2\sigma^2}\mright] \\
    \implies \ell(X, Y; \Bv, \sigma^2) &= \log\mleft( \frac{1}{(2\pi\sigma^2)^\frac{n}{2}} 
       \exp \mleft[\frac{-(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv)}{2\sigma^2}\mright]
       \mright) \\
    \implies \ell(X, Y; \Bv, \sigma^2) &= -\frac{1}{2}\mleft[ n\log 2\pi + n \log\sigma^2 
          + \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \mright]. \numberthis \label{eq:likelihood}
  \end{align*}
  The maximum likelihood estimator is found by
  \begin{align*}
    \Bvh_\mathrm{MLE} &= \argmax_\Bv \ell\mleft(X, Y; \Bv, \sigma^2\mright) \\
    &\iff \argmax_\Bv -\frac{1}{2}\mleft[ n\log 2\pi + n\log\sigma^2 
          + \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \mright] \\
    &\iff \argmin_\Bv \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \\
    &\iff \argmin_\Bv (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv). \\
    \therefore \Bvh_\mathrm{MLE} &= \Bvh_\mathrm{LSE} = \argmin_\Bv (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv). \qedhere
  \end{align*}
\end{proof}

The maximum likelihood estimator occurs at the location where the partial
derivative with respect to $\Bv$.
\begin{alignat*}{3}
  \nabla_\Bv &= -2\Xm^\T(\Yv - \Xm\Bvh) = 0 \\
  &= \Xm^\T\Yv - \Xm^\T\Xm\Bvh = 0 \\
  \implies &\Xm^\T\Yv = \Xm^\T\Xm\Bvh \\
  \implies &\Bvh = (\Xm^\T\Xm)^{-1}\Xm^\T\Yv.
\end{alignat*}
Since $\rank \Xm = p$, $(\Xm^\T\Xm)^{-1} \in \R^{p \times p}$ exists and is
unique, so $\Bvh$ is a unique estimator. We have also shown, incidentally, that
$\Bvh$ is the maximum likelihood estimator regardless of the value of
$\sigma^2$. The only requirement on the errors $\ve_i$ is that they be \iid, such
that $\sigma^2$ is finite and constant.

\subsection{The Distribution of the MLE}
\begin{prop}
  Given $\Xm$, the MLE of the regression coefficients is normally distributed,
  centered about the true parameter, with variance proportional to the error
  variance:
  \[ \Bvh|\Xm \sim \N{\Bv}{ \sigma^2 (\Xm^\T \Xm)^{-1}}. \]
\end{prop}

\begin{proof}
  The distribution of $\Bvh$ can be determined from the distribution of the
  residuals.
  \begin{align*}
    \Bvh &= (\Xm^\T \Xm)^{-1} \Xm^\T \Yv \\
        &= (\Xm^\T \Xm)^{-1} \Xm^\T (\Xm \Bv + \ve) \\
        &= (\Xm^\T \Xm)^{-1} (\Xm^\T \Xm) \Bv 
          + (\Xm^\T \Xm)^{-1} \Xm^\T \ve \\
        &= \Bv + (\Xm^\T \Xm)^{-1} \Xm^\T \ve.
  \end{align*}
  By \Cref{thm:Bx} and the assumption of normality of errors,
  \begin{align*}
    \ve &\sim \N{0}{\sigma^2 \vect{I}_n} \\
    \implies B\ve &\sim \N{0}{\sigma^2 B B^\T}.
  \end{align*}
  Thus,
    \[ \Bvh|\Xm \sim \N{\Bv}{\sigma^2 B B^\T} \]
  where
  \[ B = (\Xm^\T \Xm)^{-1} \Xm^\T. \]
  The variance of the estimator is then proportional to
  \begin{alignat*}{3}
    B B^\T &= (\Xm^\T \Xm)^{-1} \Xm^\T ((\Xm^\T \Xm)^{-1} \Xm^\T)^\T  \\
          &= (\Xm^\T \Xm)^{-1} \Xm^\T \Xm ((\Xm^\T \Xm)^{-1})^\T  \by{transpose properties} \\
          &= ((\Xm^\T \Xm)^{-1})^\T \by{multiplicative inverse} \\
          &= ((\Xm^\T \Xm)^\T)^{-1} \by{$\Xm^\T\Xm$ square and invertible} \\
          &= (\Xm^\T \Xm)^{-1}, \by{$\Xm^\T\Xm$ symmetric}
  \end{alignat*}
  so its distribution is
  \[ \Bvh|\Xm \sim \N{\Bv}{ \sigma^2 (\Xm^\T \Xm)^{-1}}. \qedhere \]
\end{proof}

\subsection{Bias of the MLE}
\begin{prop}
  The MLE is unbiased.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Bias}_\Bv(\Bvh) &= \E{\Bvh - \Bv} \\
    &= \Ebh - \E{\Bv} \by{linearity of expectation} \\
    &= \Ebh - \Bv \by{$\Bv$ is constant} \\
    &= \E{\mathbb{E}[\Bvh|\Xm]} - \Bv \by{law of total expectation} \\
    &= \E{\Bv} - \Bv \by{distribution of $\Bvh|\Xm$ is normal} \\
    &= \Bv - \Bv \by{$\Bv$ is constant} \\
    &= 0. \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\subsection{MLE of the Variance}
\begin{prop}
  The maximum likelihood estimator of the variance is
  \[ \hat{\sigma}^2_\mathrm{MLE} = \tfrac{1}{n} \hat{\ve}^\T \hat{\ve} 
    = \tfrac{1}{n} (\Yv - \Xm\Bvh)^\T (\Yv - \Xm\Bvh). \]
\end{prop}

\begin{proof}
  To maximize the likelihood with respect to the variance, we take the gradient
  of~\Cref{eq:likelihood} w.r.t.\ $\sigma^2$
  \begin{align*}
    \nabla_{\sigma^2} &= -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \\
    \implies \nabla_{\sigma^2}(\hat{\sigma}^2) &= -\frac{n}{2\hat{\sigma}^2} 
      + \frac{1}{2(\hat{\sigma}^2)^2}(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) = 0 \\
    \implies \frac{n}{2\hat{\sigma}^2} &= \frac{1}{2(\hat{\sigma}^2)^2}(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \\
    \implies \hat{\sigma}^2 &= \tfrac{1}{n}(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv).
  \end{align*}
  The value of $\Bv$ can be estimated by the maximum likelihood estimator for
  $\Bv$, $\Bvh$, since we have already shown that $\Bvh$ is optimal, independent
  of the value of $\sigma^2$. Thus,
  \[ \hat{\sigma}^2_\mathrm{MLE} = \tfrac{1}{n}(\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh). \qedhere \]
\end{proof}

\subsection{An Unbiased Estimator of the Variance}
\begin{prop}
  The maximum likelihood estimator of the variance is biased, given the design.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Bias}_{\sigma^2}(\hat{\sigma}_\mathrm{MLE}^2) &= \E{\hat{\sigma}_\mathrm{MLE}^2 - \sigma^2} \\
    &= \E{\hat{\sigma}_\mathrm{MLE}^2} - \E{\sigma^2} \by{linearity of expectation} \\
    &= \E{\hat{\sigma}_\mathrm{MLE}^2} - \sigma^2 \by{$\sigma^2$ is constant} \\
    &= \E{\E{\tfrac{1}{n}(\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh) | \Xm}} - \sigma^2. \by{law of total expectation}
  \end{alignat*}
  We have already shown in the~\nameref{app:prediction_error} that the norm of
  the residuals, under the assumption of normality, is 
  \[ (\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh) = \sigma^2 (n - p) \]
  where $\rank \Xm = p$. Thus,
  \begin{align*}
    \mathrm{Bias}_{\sigma^2}(\hat{\sigma}_\mathrm{MLE}^2) &= \E{\E{\tfrac{1}{n}
        \sigma^2 (n - p) | \Xm}} - \sigma^2 \\
    &= \frac{n-p}{n} \E{\E{\sigma^2 | \Xm}} - \sigma^2 \\
    &= \frac{n-p}{n} \E{\sigma^2} - \sigma^2 \\
    &= \frac{n-p}{n} \sigma^2 - \sigma^2 \numberthis \label{eq:Es2} \\
    &= \frac{(n-p)\sigma^2 - n\sigma^2}{n} \\
    \implies \mathrm{Bias}_{\sigma^2}(\hat{\sigma}_\mathrm{MLE}^2) &= -\frac{p}{n} \sigma^2 \ne 0. \qedhere
  \end{align*}
\end{proof}
Although the MLE of $\sigma^2$ is \emph{consistent} ($\mathrm{Bias} \to 0$ as $n
\to \infty$), it has non-zero bias for finite $\sigma^2$, $p$, $n$ and $p \le n$.

\begin{prop}
  An unbiased estimator of the variance is
  \[ \hat{\sigma}^2 = \frac{1}{n-p} (\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh). \]
\end{prop}

\begin{proof}
  To eliminate the bias in our estimator, we would like the expectation of the MLE
  estimator to be equal to $\sigma^2$. We see from~\Cref{eq:Es2} that the
  expectation of the MLE estimator is
  \[ \E{\hat{\sigma}^2_\mathrm{MLE}} = \frac{n-p}{n} \sigma^2. \]
  To remove the bias, define the estimator
  \[ \hat{\sigma}^2 = \lambda \hat{\sigma}^2_\mathrm{MLE}, \]
  where $\lambda$ is a constant parameter to be determined, such that
  \[ \E{\hat{\sigma}^2} = \sigma^2. \]
  The bias is then
  \begin{align*}
    \mathrm{Bias}_{\sigma^2}(\hat{\sigma}^2) &= \E{\hat{\sigma}^2} - \sigma^2 \\
    &= \sigma^2 - \sigma^2 \\
    &= 0.
  \end{align*}
  The parameter $\lambda$ is found by computing the expectation
  \begin{align*}
    \E{\hat{\sigma}^2} &= \E{\E{\hat{\sigma}^2 | \Xm}} \\
    &= \E{\E{\lambda \hat{\sigma}^2_\mathrm{MLE} | \Xm}} \\
    &= \lambda \E{\E{\hat{\sigma}^2_\mathrm{MLE} | \Xm}} \\
    &= \lambda \frac{n-p}{n} \sigma^2 = \sigma^2 \\
    \implies \lambda &= \frac{n}{n-p}.
  \end{align*}
  Thus,
  \begin{align*}
    \hat{\sigma}^2 &= \frac{n}{n-p} \hat{\sigma}^2_\mathrm{MLE} \\
    &= \E{\hat{\sigma}^2_\mathrm{MLE}}\E{\hat{\sigma}^2_\mathrm{MLE}}\frac{n}{n-p} \frac{1}{n}(\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh) \\
  \implies \hat{\sigma}^2  &\coloneqq \frac{1}{n-p} (\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh).
  \end{align*}
\end{proof}

Given $\Xm$, the conditional distribution of the unbiased estimator is
\[ \frac{(n-p)\hat{\sigma}^2}{\sigma^2} \Big| \Xm \sim \chi^2_{n-p} \]
by Cochran's theorem, under the assumption that the errors are \iid\ Gaussian.


\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
