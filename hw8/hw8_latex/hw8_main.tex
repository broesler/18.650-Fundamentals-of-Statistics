\documentclass[letterpaper, reqno]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[comma, compress, numbers, square]{natbib}
\usepackage[sectionbib]{chapterbib}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}  % for indicator function
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{mathrsfs}  % mathscr
\usepackage{mleftright}
\usepackage{enumerate}
\usepackage{array, multirow}
\usepackage{tcolorbox}
\usepackage[section, ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage[font=small, hypcap=true]{caption}                % link to top of figures and subfigures
\usepackage[font=small, hypcap=true, list=true]{subcaption}   % use for subfigures instead of {subfigure}

\usepackage{hyperref}
\hypersetup{
    linktoc=all,    % link table of contents to sections
    colorlinks,
    allcolors=black,
    urlcolor=dblue,
  }
\usepackage{cleveref}

% Declare theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}{Proposition}[section]
\newtheorem{definition}{Definition}[section]

\renewcommand{\qedsymbol}{\ensuremath{\blacksquare}}

% new environment for proofs of claims within proofs
\newenvironment{subproof}[1][\proofname]{%
  \renewcommand{\qedsymbol}{\ensuremath{\square}}%
  \begin{proof}[#1]%
}{%
  \end{proof}%
}

\numberwithin{equation}{section}

% lists as 1., 2., ...
\renewcommand{\labelenumi}{\theenumi.}

% Letter subsections
\renewcommand{\thesubsubsection}{\thesubsection(\alph{subsubsection})}

% number only specific equation in, say, align* environment
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}

% Algorithm
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}

%-------------------------------------------------------------------------------
%     {listings + color} package options
%-------------------------------------------------------------------------------
% Define colors for code
\usepackage{xcolor}
\definecolor{gray}{rgb}{0.6, 0.6, 0.6}
\definecolor{mauve}{rgb}{0.58, 0, 0.82}
\definecolor{dblue}{HTML}{0645AD}
\definecolor{lblue}{HTML}{1F77B4}

\usepackage{listings}
\lstset{
  language = Python,
  basicstyle = \scriptsize\ttfamily,
  numbers = left,
  numberstyle = \tiny\color{gray},
  stepnumber = 1,
  numbersep = 8pt,
  breaklines = true,
  keywordstyle = \bfseries\color{lblue},
  commentstyle = \color{gray},
  % stringstyle = \color{mauve}
}

% Place loose figures at actual top of page
% \makeatletter
%   \setlength{\@fptop}{0pt}
% \makeatother

%-------------------------------------------------------------------------------
% NO HYPHENATION
%-------------------------------------------------------------------------------
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

%==============================================================================
% General macros
%==============================================================================
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand{\T}{\top} % transpose symbol
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}} % use BOTH to cover all cases
\newcommand{\ith}{$i^\mathrm{th}$}
\newcommand{\jth}{$j^\mathrm{th}$}
% \newcommand{\norm}[1]{{\lVert {#1} \rVert}_2}

% Misc.
\newcommand{\ie}{\emph{i.e.\ }}
\newcommand{\eg}{\emph{e.g.\ }}

% Use for fractions with large terms
\newcommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}

% Derivatives
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ddx}[2]{\frac{\dd #1}{\dd #2}}
\newcommand{\ppx}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dx}[1]{\,\dd #1} % for inside integration

% Probability helpers
\newcommand{\Prob}[1]{\mathbb{P}\mleft[#1\mright]}
\newcommand{\E}[1]{\mathbb{E}\mleft[#1\mright]}
\newcommand{\V}[1]{\mathbb{V}\mleft[#1\mright]}
\newcommand{\Var}[1]{\operatorname{Var}\mleft(#1\mright)}
\newcommand{\Cov}[1]{\operatorname{Cov}\mleft(#1\mright)}
\newcommand{\Li}[1]{\mathcal{L}\mleft( #1 \mright)}  % likelihood
\newcommand{\R}{\mathbb{R}}  % real numbers
\newcommand{\N}[2]{\mathcal{N}\mleft( #1, #2 \mright)}
\newcommand{\U}[2]{\mathcal{U}\mleft(\mleft[ #1, #2 \mright]\mright)}
\newcommand{\indep}{\perp \!\!\! \perp}  % "is independent from"
\newcommand{\nindep}{\centernot\indep}
\newcommand{\indic}[1]{\mathbbm{1}\mleft\{#1\mright\}} % indicator function
\newcommand{\iid}{i.i.d.}

\newcommand{\sumi}[2]{\sum_{#1=1}^{#2}}
\newcommand{\avg}[2]{\frac{1}{#2}\sumi{#1}{#2}}

\newcommand{\by}[1]{&\quad&\text{(#1)}}

\newcommand{\Alim}{\xrightarrow[n \to \infty]{\text{a.s.}}}
\newcommand{\Plim}{\xrightarrow[n \to \infty]{\mathbb{P}}}
\newcommand{\Dlim}{\xrightarrow[n \to \infty]{(d)}}

\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}

\newcommand{\Xnbar}{\overline{X}_n}
\newcommand{\Xsqbar}{\overline{X_n^2}}
\newcommand{\Rnbar}{\overline{R}_n}
\newcommand{\Qnbar}{\overline{Q}_n}

% Vectors/Matrices
\newcommand{\Xm}{\vect{X}}
\newcommand{\Yv}{\vect{Y}}
\newcommand{\Xv}{\vect{X}}
\newcommand{\Bv}{\beta}
\newcommand{\Bvh}{\hat{\beta}}
\newcommand{\ve}{\varepsilon}
\newcommand{\Ebh}{\mathbb{E}[\Bvh]}

\newcommand{\thh}{\hat{\theta}}
\newcommand{\xs}{x_\star}
\newcommand{\Xs}{\Xv_\star}
\newcommand{\Ys}{Y_\star}
\newcommand{\Yhs}{\hat{Y}_\star}
\newcommand{\Vth}{\mathbb{V}[\hat{\theta}]}
\newcommand{\za}{z_\alpha}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Ber}{Ber}


%%%% TITLE ----------------------------
\title[Homework 8 -- Problem \thesection]{18.650 Fundamentals of Statistics\\{\large Homework 8}}
\author{Bernie Roesler}
\date{\today}
%%%%

%%%%%%% BEGIN DOCUMENT ----------------
\begin{document}

\graphicspath{{./figures/}}

\maketitle

\section{Heteroscedastic Regression}
Let the characteristics $(X_i, y_i)$ of $n$ individuals $(i = 1, \dots, n)$ be
observed, where $y_i \in \R$ is the dependent variable, and $X_i \in \R^p$ is
the vector of deterministic explanatory variables. The goal is to estimate the
vector of coefficients $\Bv = (\beta_1, \dots, \beta_n)^\T$ in the linear
regression
\[ y_i = X_i^\T \Bv + \ve_i, \quad i = 1, \dots, n. \]
Assume that the model is \emph{heteroscedastic}, \ie\ the error terms $\ve_i$
are not \iid, but $\ve = (\ve_1, \dots, \ve_n)^\T \in \R^n$ is Gaussian,
centered, with known invertible covariance matrix $\Sigma \in \R^{n \times n}$.
Often, the errors are assumed to be independent, but not identically
distributed. In this case, the covariance matrix is diagonal,
\[ \Sigma = \begin{bmatrix} \sigma_1^2 && \\ & \ddots & \\ && \sigma_n^2 \end{bmatrix}. \]
In the case where errors are also not independent, $\Sigma$ will have non-zero
off-diagonal entries, but is valid as long as it is symmetric and invertible. 
Let $\Xm \in \R^{n \times p}$ be the matrix
\[ \Xm = \begin{bmatrix} 
            \text{---} X_1^\T \text{---} \\
            \vdots \\ 
            \text{---} X_n^\T \text{---} 
          \end{bmatrix} 
\]
and $\Yv = (y_1, \dots, y_n)^\T \in \R^n$.
The linear regression is then
\[ \Yv = \Xm\Bv + \ve. \]

Consider the estimator
\[ \Bvh = \argmin_{\Bv \in \R^p} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv). \]

\subsection{The Estimator Under Homoscedasticity}
\begin{prop}
  If the errors are homoscedastic, \ie\ $\Sigma = \sigma^2 \vect{I}_n$, where $\sigma^2 > 0$, and
  $\vect{I}_n$ is the $n \times n$ identity matrix, then $\Bvh$ reduces to the least
  squares estimator
  \[ \beta_\mathrm{LSE} = (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv). \]
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \Bvh &\coloneqq \argmin_\Bv (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \\
         &= \argmin_\Bv (\Yv - \Xm\Bv)^\T (\sigma^2 \vect{I}_n)^{-1} (\Yv - \Xm\Bv) \\
         &= \argmin_\Bv \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T \vect{I}_n (\Yv - \Xm\Bv) \by{inverse properties} \\
         &= \argmin_\Bv \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T (\Yv - \Xm\Bv) \by{definition of identity} \\
    \implies \Bvh &= \argmin_\Bv (\Yv - \Xm\Bv)^\T (\Yv - \Xm\Bv) = \beta_\mathrm{LSE} \by{$\sigma^2 > 0$ and constant} \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\subsection{The Maximum Likelihood Estimator}
\begin{prop}
  The estimator $\Bvh$ is the maximum likelihood estimator.
\end{prop}

\begin{proof}
  If $\ve \sim \N{0}{\Sigma}$, then
  \[ f_Y(\Yv) = \frac{1}{\sqrt{(2\pi)^n \det \Sigma}}
      \exp \mleft[-\frac{1}{2} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright]. \]
  The likelihood is defined as
  \begin{align*}
    L(\Bv; \Sigma) &\coloneqq \prod_{i=1}^m f_Y(\Yv_i)  \\
    &= \frac{1}{\sqrt{(2\pi)^n \det \Sigma}}
      \exp \mleft[-\frac{1}{2} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright]
  \end{align*}
  To maximize the likelihood, we take the log-likelihood
  \begin{align*}
    \ell(\Bv; \Sigma) &= \log L(\Bv, \Sigma) \\
    &= \log \frac{1}{\sqrt{(2\pi)^n \det \Sigma}}
      \exp \mleft[-\frac{1}{2} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright] \\
    &= -\frac{1}{2} \mleft[n \log 2\pi + \log \det \Sigma 
        + (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright] \\
    \intertext{When maximizing with respect to $\Bv$, we ignore terms that are
      not functions of $\Bv$,}
    \therefore \Bvh_\mathrm{MSE} &\coloneqq \argmax_\Bv \ell(\Bv; \Sigma) \\
    &= \argmax_\Bv -\frac{1}{2} \mleft[n \log 2\pi + \log \det \Sigma 
        + (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright] \\
    \implies \Bvh = \Bvh_\mathrm{MSE} &= \argmin_\Bv (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv)   \qedhere
  \end{align*}
\end{proof}

\subsection{A Sufficient Condition on the Dependent Matrix}
\begin{prop}
  A sufficient condition on the dependent matrix $\Xm$ for the estimator $\Bv$ to
  be uniquely defined is
  \[ \rank \Xm = p \le n. \]
\end{prop}

\begin{proof}
  Given the data, the error in the linear regression
  \[ \Yv = \Xm\Bv + \ve \]
  is minimized by projecting $\Yv$ onto the column space of $\Xm$,
  $\mathscr{C}(\Xm)$, by multiplying each side by $\Xm^\T$,
  \[ \Xm^\T\Yv = \Xm^\T\Xm\Bvh \]
  such that
  \[ \Bvh = (\Xm^\T\Xm)^{-1} \Xm^\T \Yv. \]
  This expression is unique if the inverse $(\Xm^\T\Xm)^{-1}$ is unique and non-singular.
  Since
  \[ \rank \Xm^\T\Xm = \rank \Xm, \]
  for any matrix $X$, and $\Xm^\T \Xm \in \R^{p \times p}$, $\rank \Xm = p$ is
  a sufficient condition for the existence of the inverse, and thus for the
  uniqueness of $\Bvh$.
\end{proof}

\subsection{The Heteroscedastic Estimator}
\begin{prop}
  The optimal heteroscedastic estimator is
  \[ \Bvh = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Yv. \]
\end{prop}

\begin{proof}
  \begin{align*}
    \Bvh &\coloneqq \argmin_t (\Yv - \Xm t)^\T \Sigma^{-1} (\Yv - \Xm t) \\
    &= \argmin_t \Yv^\T \Sigma^{-1} \Yv + t^\T \Xm^\T \Sigma^{-1} \Xm t - 2t^\T
    \Xm^\T \Sigma^{-1} \Yv.
  \end{align*}
  The minimum occurs when the gradient of the argument is 0 with respect to the
  variable over which we are optimizing. The gradient with respect to $t$ is
  \begin{align*}
    \nabla_t &= 2 \Xm^\T \Sigma^{-1} \Xm t - 2 \Xm^\T \Sigma^{-1} \Yv \\
             &= 0 \quad \text{for } t = \Bvh \\
    \implies& \Xm^\T \Sigma^{-1} \Xm \Bvh = \Xm^\T \Sigma^{-1} \Yv \\
    \implies& \Bvh = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Yv.
    \qedhere
  \end{align*}
\end{proof}

\begin{prop} \label{prop:BvN}
  The estimator is normally distributed, with mean and variance
  \[ \Bvh \sim \N{\Bv}{(\Xm^\T \Sigma^{-1} \Xm)^{-1}}. \]
\end{prop}

\begin{proof}
  From our expression for $\Bvh$, insert the expression for the data $\Yv
  = \Xm\Bv + \ve$
  \begin{align*}
    \Bvh &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Yv \\
         &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} (\Xm\Bv + \ve) \\
         &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} (\Xm^\T \Sigma^{-1} \Xm)\Bv 
          + (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \ve \\
         &= \Bv + (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \ve.
  \end{align*}
  Let
  \[ B = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \]
  be the error coefficient matrix, such that
  \[ \Bvh = \Bv + B\ve. \]

  \begin{theorem} \label{thm:Bx}
    If $X \sim \N{\mu}{\Sigma}$, then $a + BX \sim \N{a + B\mu}{B \Sigma B^\T}$,
    for any constant vector $a$, matrix $B$ and invertible covariance matrix
    $\Sigma$.
  \end{theorem}

  Since the error is assumed to be normally distributed,
  $\ve \sim \N{0}{\Sigma}$, by \Cref{thm:Bx},
  \[ \Bvh \sim \N{\Bv}{B \Sigma B^\T}. \]
  The covariance matrix is then
  \begin{alignat*}{3}
    B \Sigma B^\T &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Sigma
                    ((\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1})^\T \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T 
                    ((\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1})^\T \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T 
                    {\Sigma^{-1}}^\T \Xm {(\Xm^\T \Sigma^{-1} \Xm)^{-1}}^\T \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T 
                    {\Sigma^\T}^{-1} \Xm {(\Xm^\T \Sigma^{-1} \Xm)^\T}^{-1}
                    \by{${(A^{-1})}^\T = {(A^\T)}^{-1}$} \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} 
                     (\Xm^\T \Sigma^{-1} \Xm) (\Xm^\T \Sigma^{-1} \Xm)^{-1}
                    \by{$\Sigma^\T = \Sigma$} \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} 
  \end{alignat*}
  so the estimator $\Bvh$ is indeed distributed normally
  \[ \Bvh \sim \N{\Bv}{(\Xm^\T \Sigma^{-1} \Xm)^{-1}}. \qedhere \]
\end{proof}

\subsection{Bias and Quadratic Risk of the Estimator}
\begin{prop} \label{prop:bhat_nobias}
  The estimator $\Bvh$ is unbiased.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Bias}_\Bv(\Bvh) &\coloneqq \E{\Bvh - \Bv} \\
    &= \Ebh - \E{\Bv} \by{linearity of expectation} \\
    &= \Ebh - \Bv \\
    &= \Bv - \Bv \by{\Cref{prop:BvN}} \\
    &= 0. \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\begin{prop}
  The risk of the estimator $\Bvh$ is the trace of its covariance matrix.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Risk}_\Bv(\Bvh) &\coloneqq \E{(\Bvh - \Bv)^\T(\Bvh - \Bv)} \\
    &= \E{\norm*{\Bvh - \Bv}_2^2} \\
    &= \E{\norm*{\Bvh - \Ebh + \Ebh - \Bv}_2^2} \\
    &= \E{\norm*{\Bvh - \Ebh}_2^2 
      + 2(\Bvh - \Ebh)^\T (\Ebh - \Bv)
      + \norm*{\Ebh - \Bv}_2^2} \\
    &= \E{\norm*{\Bvh - \Ebh}_2^2}
      + 2\E{(\Bvh - \Ebh)^\T (\Ebh - \Bv)} 
      + \E{\norm*{\Ebh - \Bv}_2^2}.
  \end{alignat*}
  The second term is identically zero.
  \[ 2\E{(\Bvh - \Ebh)^\T (\Ebh - \Bv)} = 2\E{(\Bvh - \Ebh)^\T}(\Ebh - \Bv) \]
  because $\Ebh - \Bv$ is deterministic, and 
  \[ \E{(\Bvh - \Ebh)^\T} = 0 \]
  by definition. The risk is then
  \[ \mathrm{Risk}_\Bv(\Bvh) = \E{\norm*{\Bvh - \Ebh}_2^2} + \E{\norm*{\Ebh - \Bv}_2^2}. \]
  The second term becomes
  \[ \E{\norm*{\Ebh - \Bv}_2^2} = \norm*{\Ebh - \Bv}_2^2 \]
  because the argument is deterministic. We recognize this expression as the
  norm of the bias.
  \[ \norm*{\Ebh - \Bv}_2^2 = \norm*{\mathrm{Bias}_\Bv(\Bvh)}_2^2. \]
  To evaluate the first term we apply the following theorem.
  \begin{theorem} \label{thm:normnorm}
    If $X \sim \N{0}{\Sigma} \in \R^n$, then $X^\T X = \norm*{X}_2^2 = \tr \Sigma$.
  \end{theorem}
  Since 
  \[ \Bvh - \E{\Bvh} \sim \N{0}{(\Xm^\T \Sigma^{-1} \Xm)^{-1}}, \]
  \begin{align*}
    \E{\norm*{\Bvh - \Ebh}_2^2} &= \E{\tr (\Xm^\T \Sigma^{-1} \Xm)^{-1}} \\
    &= \tr (\Xm^\T \Sigma^{-1} \Xm)^{-1} \\
    &= \tr \Var{\Bvh}.
  \end{align*}
  \[ \therefore \mathrm{Risk}_\Bv(\Bvh) = \tr \Var{\Bvh} + \norm*{\mathrm{Bias}_\Bv(\Bvh)}_2^2. \]
  As shown by the proof of \Cref{prop:bhat_nobias}, $\Bvh$ is
  unbiased, so
  \begin{align*}
    \mathrm{Risk}_\Bv(\Bvh) &= \tr \Var{\Bvh}  \\
    \implies \mathrm{Risk}_\Bv(\Bvh) &= \tr (\Xm^\T \Sigma^{-1} \Xm)^{-1}.
    \qedhere
  \end{align*}
\end{proof}

\subsection*{Appendix} \label{app:prediction_error}
\begin{quote}
  Q: What is the prediction error of the estimator?
\end{quote}

\begin{prop} \label{prop:pred_err}
  The prediction error, defined as the expected value of the sum of squared
  residuals, is
  \[ \E{\norm*{\Yv - \Xm\Bvh}_2^2} = \tr M \Sigma M, \]
  where $M$ is the projection matrix onto the null space of $\Xm^\T$,
  $\mathscr{N}(\Xm^\T)$.
\end{prop}

\begin{proof}
  The residuals can be expressed as
  \begin{align*}
    \Yv - \Xm\Bvh &= \Yv - \Xm B \Yv \\
    &= (\vect{I}_n - \Xm B) \Yv
  \end{align*}
  where
  \[ B = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1}. \]
  Let $H \in \R^{n \times n}$ be the projection matrix that maps onto the column
  space of $\Xm$, $\mathscr{C}(\Xm)$:
  \[ H = \Xm B =  \Xm (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1}. \]
  Then, let $M \in \R^{n \times n}$ be the projection matrix onto
  $\mathscr{N}(\Xm^\T)$, which is orthogonal to $\mathscr{C}(\Xm)$.
  \[ M = \vect{I}_n - \Xm B 
        = \vect{I}_n - \Xm (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1}. \]
  The residuals are then
  \[ \Yv - \Xm\Bvh = (\vect{I}_n - H)\Yv = M\Yv. \]
  We can substitute the expression for the $\Yv$ to obtain
  \begin{align*}
    \Yv - \Xm\Bvh &= M \Yv \\
    &= M (\Xm\Bv + \ve) \\
    &= M\Xm\Bv + M\ve.
  \end{align*}
  The projection of $\Xm$ onto the null space of its own transpose $M\Xm\Bv$ is
  always zero,
  \[ \implies \Yv - \Xm\Bvh = M\ve. \]
  The norm of the residuals is then
  \begin{alignat*}{3}
    \norm*{\Yv - \Xm\Bvh}_2^2 = \norm*{M\Yv}_2^2 &= \norm*{M\ve}_2^2 \\
    &= \norm*{\N{0}{M\Sigma M^\T}}_2^2 \by{\Cref{thm:normnorm}} \\
    &= \norm*{\N{0}{M\Sigma M}}_2^2 \by{$M^\T = M$} \\
    &= \tr M\Sigma M \by{\Cref{thm:Bx}} \\
    \implies \norm*{\Yv - \Xm\Bvh}_2^2 &= \tr M\Sigma M \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\begin{prop}
  In the homoscedastic case, where $\Sigma = \sigma^2 \vect{I}_n$, the norm of
  the residuals is  
  \[ \norm*{\Yv - \Xm\Bvh}_2^2 = \sigma^2 (n - p). \]
\end{prop}

\begin{proof}
  In the homoscedastic case, where $\Sigma = \sigma^2 \vect{I}_n$, this expression
  evaluates to 
  \begin{alignat*}{3}
    \norm*{\Yv - \Xm\Bvh}_2^2 &= \tr M(\sigma^2 \vect{I}_n) M \\
    &= \sigma^2 \tr M M \by{trace property} \\
    &= \sigma^2 \tr M \by {$M$ is idempotent}
  \end{alignat*}
  It is property of projection matrices that their eigenvalues $\lambda_i \in
  \{0, 1\}$ only.
  The trace of $M$ is then equal to the number of its non-zero eigenvalues.
  Since $M$ is a projection onto the null space of $\Xm^\T$, it has non-zero
  eigenvalues equal to the dimension of that null space.
  For $\Xm \in \R^{n \times p}$, $n \ge p$,
  \[ \dim\mathscr{N}(\Xm^\T) = n - \dim \mathscr{C}(\Xm). \]
  The dimension of the column space is the rank
  \[ \dim \mathscr{C}(\Xm) = \rank \Xm = p, \]
  as required for the uniqueness of $\Bvh$. Thus,
  \[ \tr M = \dim\mathscr{N}(\Xm^\T) = n - p, \]
  \[ \implies \norm*{\Yv - \Xm\Bvh}_2^2 = \sigma^2 (n - p). \qedhere \]
\end{proof}
Further discussion of the geometry of these subspaces can be found in
\cite{davidson2004econometric}.

\subsection{Out of Sample Prediction}
The derivation of prediction error given in the proof of \Cref{prop:pred_err}
applies only to predictions made from inputs \emph{in the fixed design}
variables. What if we observe $\Xm = \Xs$,
where $\Xs \notin \{\Xv_1, \dots, \Xv_n \}$,
and would like to make a prediction?
What is the confidence interval on our prediction? The following example is
based on Exercise 13.10 in \cite{wasserman2013all}.

Let our desired prediction matrix be $\Xs \in \R^{m \times p}$, $\rank \Xs = p$, 
such that our prediction $\Yhs \in \R^m$ is
\[ \Yhs = \Xs \Bvh. \]
The variance of this prediction is
\begin{align*}
   \Var{\Yhs} &= \Var{\Xs \Bvh}  \\
   &= \Xs \Var{\Bvh} \Xs^\T \\
   &= \Xs (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xs^\T.
\end{align*}
% TODO fully work this out in multiple dimensions, and/or with heteroscedasticity.
For the following arguments, we let $m = 1$, so that our prediction $\Yhs$ is
a singleton, and assume that the error is homoscedastic, so that 
$\Sigma = \sigma^2 \vect{I}_n$.

\begin{theorem}
  The probability that the out-of-sample prediction $\Yhs$ lies in the interval
  \[ \Yhs \pm \za \sqrt{\Var{\Yhs}} \]
  where $\za = \Phi\mleft(1 - \frac{\alpha}{2}\mright)$, is \emph{not} equal to $1 - \frac{\alpha}{2}$, \ie
  \[ \Prob{ \Yhs - \za \sqrt{\Var{\Yhs}} 
           < \Ys 
           < \Yhs + \za \sqrt{\Var{\Yhs}}} 
      < 1 - \frac{\alpha}{2} \]
\end{theorem}

\begin{proof}
  Define the variables
  \begin{alignat*}{3}
    \theta &= \Xs \beta &&\implies \Ys = \theta + \ve, \\
    \thh &= \Xs \Bvh &&\implies \Yhs = \thh,
  \end{alignat*}
  where $\thh \sim \N{\theta}{s^2}$ and $s^2 = \Vth = \Var{\Yhs}$.
  Then, the probability that $\Yhs$ is in the interval is given by
  \begin{align*}
    &\Prob{\Yhs - \za s < \Ys < \Yhs + \za s}  \\
    \iff &\Prob{\thh - \za s < \theta + \ve < \thh + \za s} \\
    \iff &\Prob{\frac{\thh - \za s - \E{\thh}}{\sqrt{\Vth}}
              < \frac{\theta + \ve - \E{\thh}}{\sqrt{\Vth}}
              < \frac{\thh - \za s - \E{\thh}}{\sqrt{\Vth}}} \\
    \iff &\Prob{\frac{\thh - \za s - \theta}{s}
              < \frac{\theta + \ve - \theta}{s}
              < \frac{\thh - \za s - \theta}{s}} \\
    \iff &\Prob{\frac{\thh - \theta}{s} - \za
              < \frac{\ve}{s}
              < \frac{\thh - \theta}{s} + \za} \\
    \iff &\Prob{-\za < \frac{\ve - (\thh - \theta)}{s} < \za}.
  \end{align*}
  To evaluate the middle expression, recall the definitions
  \begin{align*}
    \ve \sim \N{0}{\sigma^2}, \\
    \thh - \theta \sim \N{0}{s^2}.
  \end{align*}
  Assuming $\Xv \indep \ve$, $\Cov{\Xv, \ve} = 0$, so
  \[ \Var{\ve - (\thh - \theta)} = \Var{\ve} + \Var{\thh - \theta}. \]
  Their joint distribution is then
  \begin{align*}
     \frac{1}{s}(\ve - (\thh - \theta)) &\sim \frac{1}{s}\N{0}{s^2 + \sigma^2} \\
     &\sim \N{0}{\frac{s^2 + \sigma^2}{s^2}} \\
     &\sim \N{0}{1 + \frac{\sigma^2}{s^2}}.
  \end{align*}
  The probability expression then becomes
  \begin{align*}
    &\Prob{\za < \frac{\ve - (\thh - \theta)}{s} < \za} \\
    \iff &\Prob{-\za < \N{0}{1 + \frac{\sigma^2}{s^2}} < \za}
    < 1 - \frac{\alpha}{2}.
  \end{align*}
  because the normal distribution has a larger variance than a standard normal.
\end{proof}

The question remains: what \emph{is} a proper confidence interval for an
out-of-sample prediction? 
\begin{prop}
  The interval defined by $\Yhs \pm \za \xi_n^2$,
  where $\xi_n^2 = s^2 + \sigma^2$, ensures that
  \[ \Prob{ \Yhs - \za \xi_n^2 < \Ys < \Yhs + \za \xi_n^2}
    = \Prob{-\za < \N{0}{1} < \za} = 1 - \frac{\alpha}{2}, \]
  giving a proper confidence interval.
\end{prop}

\begin{proof}
  We would like to find a parameter $\lambda \in \R$
  ($\lambda \ne s$) such that 
  \[ \Prob{\Yhs - \za\lambda < \Ys < \Yhs + \za\lambda} 
      = \Prob{ -\za < \N{0}{1} < \za} = 1 - \frac{\alpha}{2}. \]
  Following the proof above, this probability statement becomes
  \begin{align*}
    &\Prob{\Yhs - \za\lambda < \Ys < \Yhs + \za\lambda}  \\
    \iff &\Prob{\thh - \za\lambda < \theta + \ve < \thh + \za\lambda}  \\
    \iff &\Prob{\frac{\thh - \theta}{s} - \za\frac{\lambda}{s} 
              < \frac{\ve}{s}
              < \frac{\thh - \theta}{s} + \za\frac{\lambda}{s}} \\
    \iff &\Prob{- \za < \frac{s}{\lambda} \cdot \frac{\ve - (\thh - \theta)}{s} < \za} \\
    \iff &\Prob{- \za < \frac{1}{\lambda} (\ve - (\thh - \theta)) < \za},
    \intertext{which, given our normality assumptions,}
    \iff &\Prob{- \za < \frac{1}{\lambda} \N{0}{s^2 + \sigma^2} < \za} \\
    \iff &\Prob{- \za < \N{0}{\frac{s^2 + \sigma^2}{\lambda^2}} < \za}.
  \end{align*}
  This expression implores us to define
  \[ \xi_n^2 = \lambda^2 \coloneqq s^2 + \sigma^2, \]
  so that
  \begin{align*}
    &\Prob{- \za < \N{0}{\frac{s^2 + \sigma^2}{s^2 + \sigma^2}} < \za} \\
    \iff &\Prob{-\za < \N{0}{1} < \za} = 1 - \frac{\alpha}{2}. \qedhere
  \end{align*}
\end{proof}
In practice, we do not know the value of $\sigma^2$, so we use the unbiased
estimator $\hat{\sigma}^2$, and let
\[ \hat{\xi}_n^2 = \hat{\sigma}^2 + s^2. \]
When $p = 2$, this expression evaluates to
\[ \hat{\xi}_n^2 = \hat{\sigma}^2 + \frac{\hat{\sigma}^2}{n}
  \mleft( \frac{\sumi{i}{n} (X_i - \Xs)^2}{\sumi{i}{n}{(X_i - \Xnbar)^2}} \mright). \]
\emph{See} \Cref{sec:2dnormal} for a full derivation of the 2-D covariance
matrix $\Var{\Bvh}$. We can see from this expression that if our desired
out-of-sample input $\Xs$ is close to $\Xnbar$, the additional term is
minimized. We can also observe that if $\Xs$ is far from $\Xnbar$, even if $n$
is large, we will have large uncertainty in our prediction.


% References:
\bibliographystyle{apalike}
\bibliography{hw8_main}


\clearpage
\section{Linear Regression with Random Design}
Consider the \iid\ pairs of random variables $(X_i, Y_i)$, $i = 1, \dots, n$,
where $X_i~\in~\R^p$ $(p \ge 1)$, and $Y_i \in \R$. For each $i$, write
\[ Y_i = X_i^\T \Bv + \ve_i, \]
where $\E{\ve_i = 0}$, $\Cov{X_i, \ve_i} = 0$, and $\Bv \in \R^p$ is an unknown
vector that we want to estimate. Assume $\forall x \in \R^p$, $\ve_1$ has
a density conditional on $X_1 = x$, denoted $f_x$, and $X_1$ has a density $g$.
In other words,
\begin{align*}
  f_x(t) &= \Prob{\ve_1 = t | X_1 = x}, \\
  g(t) &= \Prob{X_1 = t}.
\end{align*}

\subsection{The Likelihood}
The likelihood function is the product of the probabilities of the data, given
the parameters:
\begin{alignat*}{3}
  \Li{X, Y; \Bv} &\coloneqq \prod_{i=1}^n f(X_i, Y_i) \\
  &= \prod_{i=1}^n f_X(X_i) f_{Y|X}(Y_i | X_i) \by{law of total probability} \\
  &= \prod_{i=1}^n f_X(X_i) \times \prod_{i=1}^n f_{Y|X}(Y_i | X_i) \\
  &= \prod_{i=1}^n g(X_i) \times \prod_{i=1}^n f_{Y|X}(Y_i | X_i) \by{definition of $g$}.
\end{alignat*}

\begin{prop}
  The probability of $Y_i$ given $X_i$ is equal to the probability of the error
  $\ve_i$ given $X_i$:
  \[ f_{Y|X}(Y_i | X_i) = f_x\mleft(Y_i - X_i^\T\Bv\mright). \]
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    f_{Y|X}(Y_i | X_i) &= f_{Y|X}(X_i^\T \Bv + \ve_i | X_i) \\
    &= \Prob{Y_i = X_i^\T \Bv + \ve_i | X_i} \by{density definition} \\
    &= \Prob{\ve_i = Y_i - X_i^\T\Bv | X_i} \by{algebraic manipulation} \\
    &= f_x\mleft(Y_i - X_i^\T\Bv\mright). \by{definition of $f_x$} \tag*{\qedhere}
  \end{alignat*}
\end{proof}

Therefore, the likelihood is
  \[ \Li{X, Y; \Bv} = \prod_{i=1}^n g(X_i) \times \prod_{i=1}^n f_x\mleft(Y_i - X_i^\T\Bv\mright). \]

\subsection{The Maximum Likelihood Estimator}
\begin{prop}
  The maximum likelihood estimator, $\Bvh$, does not depend on the distribution
  of the design random variables $X$, $g$.
\end{prop}

\begin{proof}
  The maximum likelihood estimator is the $\Bvh$ that maximizes the likelihood
  function
  \[ \Bvh \coloneqq \argmax_{\Bv \in \R^p} \prod_{i=1}^n g(X_i) 
    \times \prod_{i=1}^n f_x\mleft(Y_i - X_i^\T\Bv\mright). \]
  Since $g(x) = \Prob{X_1 = x}$ does not depend on $\Bv$, the definition of
  $\Bvh$ is equivalent to
  \[ \iff \Bvh = \argmax_\Bv \prod_{i=1}^n f_x\mleft(Y_i - X_i^\T\Bv\mright) \]
  which itself does not depend on $g$.
\end{proof}

\subsection{Gaussian Error}
Assume that $\ve_1 \indep X_1$ and $\ve_1 \sim \N{0}{\sigma^2}$,
with $0~<~\sigma^2~<~\infty$.

\subsubsection{The Conditional Error Density}
Given $X_1 = x_1$, $\ve_1 = y_1 - x_1^\T\Bv$, so
\[ f_x = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \mleft[\frac{-(y_1 - x_1^\T\Bv)^2}{2\sigma^2}\mright]. \]

\subsubsection{The Least Square Estimator}
In this section, we will use the following theorem without proof.
\begin{theorem}
Since $X_1, \dots, X_n \in \R^p$ are \iid, the rank of
\[ \Xm = \begin{bmatrix} 
            \text{---} X_1^\T \text{---} \\
            \vdots \\ 
            \text{---} X_n^\T \text{---} 
          \end{bmatrix} 
\]
is almost surely $p$.
\end{theorem}

\begin{theorem}
  Under the assumption of normal error, $\ve_1 \sim \N{0}{\sigma^2}$, the maximum
  likelihood estimator is equal to the least squares estimator.
\end{theorem}

\begin{proof}
  First, we write down the least squares estimator
  \begin{align*}
    \Bvh_\mathrm{LSE} &\coloneqq \argmin_{\Bv \in \R^p} \ve^\T \ve \\
                      &= \argmin_{\Bv \in \R^p} (\Yv - \Xm\Bv)^\T (\Yv - \Xm\Bv).
  \end{align*}
  Under the assumption of normality, we can write down the conditional log-likelihood.
  \begin{align*}
    \Li{x, y; \Bv, \sigma^2} &\propto \prod_{i=1}^n f_x\mleft(y_i - x_i^\T\Bv\mright) \\
    &= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} 
       \exp \mleft[\frac{-(y_i - x_i^\T\Bv)^2}{2\sigma^2}\mright] \\
    &= \frac{1}{(2\pi\sigma^2)^\frac{n}{2}} 
       \exp \mleft[\frac{-(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv)}{2\sigma^2}\mright] \\
    \implies \ell(x, y; \Bv, \sigma^2) &= \log\mleft( \frac{1}{(2\pi\sigma^2)^\frac{n}{2}} 
       \exp \mleft[\frac{-(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv)}{2\sigma^2}\mright]
       \mright) \\
    \implies \ell(x, y; \Bv, \sigma^2) &= -\frac{1}{2}\mleft[ n\log 2\pi + n \log\sigma^2 
          + \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \mright]. \numberthis \label{eq:likelihood}
  \end{align*}
  The maximum likelihood estimator is found by
  \begin{align*}
    \Bvh_\mathrm{MLE} &= \argmax_\Bv \ell\mleft(x, y; \Bv, \sigma^2\mright) \\
    &\iff \argmax_\Bv -\frac{1}{2}\mleft[ n\log 2\pi + n\log\sigma^2 
          + \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \mright] \\
    &\iff \argmin_\Bv \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \\
    &\iff \argmin_\Bv (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv). \\
    \therefore \Bvh_\mathrm{MLE} &= \Bvh_\mathrm{LSE} = \argmin_\Bv (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv). \qedhere
  \end{align*}
\end{proof}

The maximum likelihood estimator occurs at the location where the partial
derivative of the log-likelihood with respect to $\Bv$ is zero.%
\footnote{The classification of the critical point as a ``maximum'' is confirmed
  by inspecting the second derivative, $\nabla_\Bv^2 = 2\Xm^\T \Xm$. Since
  $\rank \Xm = p$, the matrix $\Xm^\T \Xm$ is positive definite, so the critical
  point is indeed a maximum.}
\begin{alignat*}{3}
  \nabla_\Bv &= -2\Xm^\T(\Yv - \Xm\Bvh) = 0 \\
  &= \Xm^\T\Yv - \Xm^\T\Xm\Bvh = 0 \\
  \implies &\Xm^\T\Yv = \Xm^\T\Xm\Bvh \\
  \implies &\Bvh = (\Xm^\T\Xm)^{-1}\Xm^\T\Yv.
\end{alignat*}
Since $\rank \Xm = p$, $(\Xm^\T\Xm)^{-1} \in \R^{p \times p}$ exists and is
unique, so $\Bvh$ is a unique estimator. We have also shown, incidentally, that
$\Bvh$ is the maximum likelihood estimator regardless of the value of
$\sigma^2$. The only requirement on the errors $\ve_i$ is that they be \iid, such
that $\sigma^2$ is finite and constant.

\subsection{The Distribution of the MLE} \label{sec:dist_mle}
\begin{prop}
  Given the design, the MLE of the regression coefficients is normally
  distributed, centered about the true parameter, with variance proportional to
  the error variance:
  \[ \Bvh|\Xm \sim \N{\Bv}{ \sigma^2 (\Xm^\T \Xm)^{-1}}. \]
\end{prop}

\begin{proof}
  The distribution of $\Bvh$ can be determined from the distribution of the
  residuals.
  \begin{align*}
    \Bvh &= (\Xm^\T \Xm)^{-1} \Xm^\T \Yv \\
        &= (\Xm^\T \Xm)^{-1} \Xm^\T (\Xm \Bv + \ve) \\
        &= (\Xm^\T \Xm)^{-1} (\Xm^\T \Xm) \Bv 
          + (\Xm^\T \Xm)^{-1} \Xm^\T \ve \\
        &= \Bv + (\Xm^\T \Xm)^{-1} \Xm^\T \ve.
  \end{align*}
  By \Cref{thm:Bx} and the assumption of normality of errors,
  \begin{align*}
    \ve &\sim \N{0}{\sigma^2 \vect{I}_n} \\
    \implies \Bv + B\ve &\sim \N{\Bv}{\sigma^2 B B^\T}.
  \end{align*}
  Thus,
    \[ \Bvh|\Xm \sim \N{\Bv}{\sigma^2 B B^\T} \]
  where
  \[ B = (\Xm^\T \Xm)^{-1} \Xm^\T. \]
  The variance of the estimator is then proportional to
  \begin{alignat*}{3}
    B B^\T &= (\Xm^\T \Xm)^{-1} \Xm^\T ((\Xm^\T \Xm)^{-1} \Xm^\T)^\T  \\
          &= (\Xm^\T \Xm)^{-1} \Xm^\T \Xm ((\Xm^\T \Xm)^{-1})^\T  \by{transpose properties} \\
          &= ((\Xm^\T \Xm)^{-1})^\T \by{multiplicative inverse} \\
          &= ((\Xm^\T \Xm)^\T)^{-1} \by{$\Xm^\T\Xm$ square and invertible} \\
          &= (\Xm^\T \Xm)^{-1}, \by{$\Xm^\T\Xm$ symmetric}
  \end{alignat*}
  so its conditional distribution is
  \[ \Bvh|\Xm \sim \N{\Bv}{ \sigma^2 (\Xm^\T \Xm)^{-1}}. \qedhere \]
\end{proof}
% NOTE we can write this proof by finding the expectation and variance of \Bvh
% conditional on X *without* the assumption of normality, then introduce
% normality to give the actual distribution for given expectation and variance.

\subsection{Bias of the MLE}
\begin{prop}
  The MLE is unbiased.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Bias}_\Bv(\Bvh) &= \E{\Bvh - \Bv} \\
    &= \Ebh - \E{\Bv} \by{linearity of expectation} \\
    &= \Ebh - \Bv \by{$\Bv$ is constant} \\
    &= \E{\mathbb{E}[\Bvh|\Xm]} - \Bv \by{law of total expectation} \\
    &= \E{\Bv} - \Bv \by{distribution of $\Bvh|\Xm$ is normal} \\
    &= \Bv - \Bv \by{$\Bv$ is constant} \\
    &= 0. \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\subsection{MLE of the Error Variance}
\begin{prop}
  The maximum likelihood estimator of the variance is
  \[ \hat{\sigma}^2_\mathrm{MLE} = \tfrac{1}{n} \hat{\ve}^\T \hat{\ve} 
    = \tfrac{1}{n} (\Yv - \Xm\Bvh)^\T (\Yv - \Xm\Bvh). \]
\end{prop}

\begin{proof}
  To maximize the likelihood with respect to the variance, we take the gradient
  of~\Cref{eq:likelihood} w.r.t.\ $\sigma^2$
  \begin{align*}
    \nabla_{\sigma^2} &= -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \\
    \implies \nabla_{\sigma^2}(\hat{\sigma}^2) &= -\frac{n}{2\hat{\sigma}^2} 
      + \frac{1}{2(\hat{\sigma}^2)^2}(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) = 0 \\
    \implies \frac{n}{2\hat{\sigma}^2} &= \frac{1}{2(\hat{\sigma}^2)^2}(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv) \\
    \implies \hat{\sigma}^2 &= \tfrac{1}{n}(\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv).
  \end{align*}
  The value of $\Bv$ can be estimated by the maximum likelihood estimator for
  $\Bv$, $\Bvh$, since we have already shown that $\Bvh$ is optimal regardless
  of the value of $\sigma^2$. Thus,
  \[ \hat{\sigma}^2_\mathrm{MLE} = \tfrac{1}{n}(\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh). \qedhere \]
\end{proof}

\subsection{An Unbiased Estimator of the Variance}
\begin{prop}
  The maximum likelihood estimator of the variance is biased, given the design.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Bias}_{\sigma^2}(\hat{\sigma}_\mathrm{MLE}^2) &= \E{\hat{\sigma}_\mathrm{MLE}^2 - \sigma^2} \\
    &= \E{\hat{\sigma}_\mathrm{MLE}^2} - \E{\sigma^2} \by{linearity of expectation} \\
    &= \E{\hat{\sigma}_\mathrm{MLE}^2} - \sigma^2 \by{$\sigma^2$ is constant} \\
    &= \E{\E{\left. \tfrac{1}{n}(\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh) \right| \Xm}} - \sigma^2. \by{law of total expectation}
  \end{alignat*}
  We have already shown in the~\nameref{app:prediction_error} that the norm of
  the residuals, under the assumption of normality, is 
  \[ (\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh) = \sigma^2 (n - p) \]
  where $\rank \Xm = p$. Thus,
  \begin{align*}
    \mathrm{Bias}_{\sigma^2}(\hat{\sigma}_\mathrm{MLE}^2) &= \E{\E{\left .\tfrac{1}{n}
        \sigma^2 (n - p) \right| \Xm}} - \sigma^2 \\
    &= \frac{n-p}{n} \E{\E{\left. \sigma^2 \right| \Xm}} - \sigma^2 \\
    &= \frac{n-p}{n} \E{\sigma^2} - \sigma^2 \\
    &= \frac{n-p}{n} \sigma^2 - \sigma^2 \numberthis \label{eq:Es2} \\
    &= \frac{(n-p)\sigma^2 - n\sigma^2}{n} \\
    \implies \mathrm{Bias}_{\sigma^2}(\hat{\sigma}_\mathrm{MLE}^2) &= -\frac{p}{n} \sigma^2 \ne 0. \qedhere
  \end{align*}
\end{proof}
Although the MLE of $\sigma^2$ is \emph{consistent} ($\mathrm{Bias} \to 0$ as $n
\to \infty$), it has non-zero bias for finite $\sigma^2$, $p$, $n$ and $p \le n$.

\begin{prop}
  An unbiased estimator of the variance is
  \[ \hat{\sigma}^2 = \frac{1}{n-p} (\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh). \]
\end{prop}

\begin{proof}
  To eliminate the bias in our estimator, we would like the expectation of the MLE
  estimator to be equal to $\sigma^2$. We see from~\Cref{eq:Es2} that the
  expectation of the MLE estimator is
  \[ \E{\hat{\sigma}^2_\mathrm{MLE}} = \frac{n-p}{n} \sigma^2. \]
  To remove the bias, define the estimator
  \[ \hat{\sigma}^2 = \lambda \hat{\sigma}^2_\mathrm{MLE}, \]
  where $\lambda$ is a constant parameter to be determined, such that
  \[ \E{\hat{\sigma}^2} = \sigma^2. \]
  The bias is then
  \begin{align*}
    \mathrm{Bias}_{\sigma^2}(\hat{\sigma}^2) &= \E{\hat{\sigma}^2} - \sigma^2 \\
    &= \sigma^2 - \sigma^2 \\
    &= 0.
  \end{align*}
  The parameter $\lambda$ is found by computing the expectation
  \begin{align*}
    \E{\hat{\sigma}^2} &= \E{\E{\left. \hat{\sigma}^2 \right| \Xm}} \\
    &= \E{\E{\left. \lambda \hat{\sigma}^2_\mathrm{MLE} \right| \Xm}} \\
    &= \lambda \E{\E{\left. \hat{\sigma}^2_\mathrm{MLE} \right| \Xm}} \\
    &= \lambda \frac{n-p}{n} \sigma^2 = \sigma^2 \\
    \implies \lambda &= \frac{n}{n-p}.
  \end{align*}
  Thus, our unbiased estimator is
  \begin{align*}
    \hat{\sigma}^2 &= \frac{n}{n-p} \hat{\sigma}^2_\mathrm{MLE} \\
    &= \frac{n}{n-p} \cdot \frac{1}{n}(\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh) \\
  \implies \hat{\sigma}^2  &\coloneqq \frac{1}{n-p} (\Yv - \Xm\Bvh)^\T(\Yv - \Xm\Bvh). \qedhere
  \end{align*}
\end{proof}

Given $\Xm$, the conditional distribution of the unbiased estimator is
\[ \frac{(n-p)\hat{\sigma}^2}{\sigma^2} \Big| \Xm \sim \chi^2_{n-p} \]
by Cochran's theorem, under the assumption that the errors are \iid\ Gaussian.

\subsection{The 2-D Case}
Assume $p = 2$ such that 
\begin{equation} \label{eq:Xm}
   \Xm = \begin{bmatrix} 1 & X_1 \\ \vdots & \vdots \\ 1 & X_n \end{bmatrix}.
\end{equation}
We no longer assume that $\Xm$ has a density. % WHY??
Let $\Bv = (a, b)^\T$ so the regression is
\[ Y_i = a + bX_i + \ve_i, \quad i = 1, \dots, n. \]

\subsubsection{The Least Squares Estimators}
The least squares estimators (LSEs) for $(a, b) = (\hat{a}, \hat{b})$ are defined as 
\[ (\hat{a}, \hat{b})_\mathrm{LSE} = \argmin_{a, b \in \R} \avg{i}{n}{(Y_i - (a + bX_i))^2}. \]
The estimators can be shown to be % show in Appendix?
\begin{align*}
  \hat{b} &= \ddfrac{\avg{i}{n}{X_iY_i} - \left(\avg{i}{n}{X_i}\right)\left(\avg{i}{n}{Y_i}\right)}
                    {\avg{i}{n}{X_i^2} - \avg{i}{n}{Y_i^2}} \\
  \hat{a} &= \avg{i}{n}{Y_i} - \hat{b}\avg{i}{n}{X_i}.
\end{align*}

\subsubsection{Consistency of the LSE}
\begin{prop}
  The least squares estimators $(\hat{a}, \hat{b})$ are consistent:
  \[  \Prob{\left|(\hat{a}, \hat{b}) - (a, b)\right| > \epsilon} \Plim 0. \]
\end{prop}

\begin{proof}
  To show that the estimators are consistent, we first need expressions for the
  true parameters. The definition of the true LSE parameters is
  \[ (a, b)_\mathrm{LSE} = \argmin_{a, b \in \R} \E{(Y - (a + bX))^2}. \]
  To find the minima, we differentiate with respect to each parameter, and set
  each expression equal to 0.
  \begin{align*}
    \nabla_a &= \E{-2(Y - (a + bX))} = 0, \text{ and} \\
    \nabla_b &= \E{-2X(Y - (a + bX))} = 0 \\
    \iff \E{Y} &= a + b\E{X}, \text{ and} \numberthis \label{eq:EA} \\
    \iff \E{XY} &= a\E{X} + b\E{X^2}. \numberthis \label{eq:EB}
  \end{align*}
  By subtracting $\E{X} \times \text{\Cref{eq:EA}}$ from \Cref{eq:EB}, we
  eliminate $a$ from the expression to find $b$
  \[ \E{XY} - \E{X}\E{Y} = b(\E{X^2} - \E{X}^2), \]
  which implies
  \begin{align*}
    \implies b &= \frac{\E{XY} - \E{X}\E{Y}}{\E{X^2} - \E{X}^2} \\
               &= \frac{\Cov{X, Y}}{\Var{X}}.
  \end{align*}
  Substituting this expression for $b$ into \Cref{eq:EA} gives
  \[ a = \E{Y} - b\E{X}. \]

  We see the convergence of the expressions for $(\hat{a}, \hat{b})$ as follows
  \begin{alignat*}{3}
    \avg{i}{n}{X_i} \Plim \E{X} \by{LLN} \\
    \avg{i}{n}{X_i^2} \Plim \E{X^2} \by{CMT} \\
    \avg{i}{n}{X_iY_i} \Plim \E{XY} \by{LLN}
  \end{alignat*}
  and likewise for $Y_i$ and $Y_i^2$. Thus, by the continuous mapping theorem,
  the LSEs converge in probability to
  \begin{align*}
    \hat{b} &\Plim \frac{\E{XY} - \E{X}\E{Y}}{\E{X^2} - \E{Y^2}} = b \\
    \hat{a} &\Plim \E{Y} - b\E{X} = a. \qedhere
  \end{align*}
\end{proof}

\subsubsection{Asymptotic Normality of the Estimators} \label{sec:2dnormal}
Assume the design is independent of the error, $X_1 \indep \ve_1$, and the error
is normally distributed $\ve_1 \sim \N{0}{\sigma^2}$.

\begin{prop}
  The least squares estimator is asymptotically normal
  \[ ((\hat{a}, \hat{b}) - (a, b)) \Dlim \N{0}{\sigma^2 (\Xm^\T\Xm)^{-1}}. \]
\end{prop}

\begin{proof}
  By the Central Limit Theorem and the consistency of our estimators,
  \[ ((\hat{a}, \hat{b}) - (a, b)) \Dlim \N{0}{\Var{(\hat{a}, \hat{b})}}. \]
  In \Cref{sec:dist_mle}, we showed that, given the design,
  \[ \Var{\Bvh|\Xm} = \sigma^2 (\Xm^\T\Xm)^{-1}. \qedhere \]
\end{proof}

\begin{tcolorbox}
  \emph{Note.}
  The unconditional variance is given by the \emph{law of total variance}
  \begin{align*}
    \Var{\Bvh} &= \Var{\E{\Bvh|\Xm}} + \E{\Var{\Bvh|\Xm}} \\
              &= \Var{\Bv} + \E{\sigma^2 (\Xm^\T\Xm)^{-1}} \\
              &= \E{\sigma^2 (\Xm^\T\Xm)^{-1}} \\
    \implies \Var{\Bvh} &= \sigma^2 \E{(\Xm^\T\Xm)^{-1}}.
  \end{align*}
  This variance is identical to that of the $\Bvh$ derived assuming \emph{fixed}
  design. Thus, we have shown that the variance of the \emph{fixed} design
  estimator is a consistent, unbiased estimate of the variance of the
  \emph{random} design estimator.
\end{tcolorbox}

To give the asymptotic variance in terms of the moments of $X$, we compute the
matrix product $\Xm^\T\Xm$, and invert it. The matrix $\Xm$ is defined in
\Cref{eq:Xm}.
{\everymath{\displaystyle}
\begin{align*}
  \Xm^\T \Xm &=
  \begin{bmatrix} 
    1   & \cdots & 1   \\
    X_1 & \cdots & X_n
  \end{bmatrix}^\T
  \begin{bmatrix} 
    1      & X_1    \\
    \vdots & \vdots \\
    1      & X_n
  \end{bmatrix} \\
  &=
  \begin{bmatrix}
    n               & \sumi{i}{n} X_i   \\
    \sumi{i}{n} X_i & \sumi{i}{n} X_i^2
  \end{bmatrix}
  =
  n
  \begin{bmatrix}
    1              & \avg{i}{n} X_i   \\
    \avg{i}{n} X_i & \avg{i}{n} X_i^2
  \end{bmatrix}
  =
  n
  \begin{bmatrix}
    1      & \Xnbar   \\
    \Xnbar & \Xsqbar
  \end{bmatrix}.
\end{align*}
}
The inverse of a $2 \times 2$ matrix multiplied by a constant is
\[ (a A)^{-1} = \frac{1}{a} A^{-1} = \frac{1}{a \det A} 
  \begin{bmatrix}
     A_{2,2} & -A_{1,2} \\
    -A_{2,1} &  A_{1,1}
  \end{bmatrix} 
\]
where $\det A = A_{1,1}A_{2,2} - A_{1,2}A_{2,1}$.
The determinant is
\begin{align*}
  \det \Xm^\T \Xm &= 1 \times \Xsqbar - \Xnbar \times \Xnbar \\
  &= \Xsqbar - \Xnbar^2 \eqqcolon S_x^2.
\end{align*}
where $S_x^2$ is the \emph{sample variance} of the design.
The entire inverse is
\[ (\Xm^\T \Xm)^{-1} = \frac{1}{n S_x^2}
  \begin{bmatrix}
    \Xsqbar & -\Xnbar \\
    -\Xnbar & 1
  \end{bmatrix}
\]
so the conditional variance of the estimator is
{\everymath{\displaystyle}
\[ \Var{\Bvh|\Xm} = \frac{\sigma^2}{n S_x^2}
  \begin{bmatrix}
    \phantom{-}\avg{i}{n} X_i^2 & -\avg{i}{n}X_i \\
    -\avg{i}{n}X_i   & 1
  \end{bmatrix},
\] which is asymptotically (by the Law of Large Numbers and Continous Mapping
Theorem)
\[ \Var{\Bvh|\Xm} \Plim \frac{\sigma^2}{n \Var{X}}
  \begin{bmatrix}
    \phantom{-}\E{X^2}  & -\E{X} \\
    -\E{X}              & 1
  \end{bmatrix}.
\]
}

\subsubsection{A Hypothesis Test}
Consider the hypotheses
\begin{align*}
  H_0 &\colon b > 0, \\
  H_1 &\colon b \le 0.
\end{align*}
The moments of $X$ and the error variance $\sigma^2$ are \emph{not} known.

\begin{prop}
  Let $\alpha \in (0, 1)$. A hypothesis test with asymptotic level $\alpha$ is
  given by
  \[ \delta_\alpha = \indic{T_n > q_\alpha}, \]
  where
  \[ T_n = \sqrt{n}\ddfrac{\hat{b}}{\sqrt{\frac{\hat{\sigma}^2}{S_x^2}}} \sim t_{n-2}, \]
  and $q_\alpha$ is the $(1-\alpha)$-quantile of Student's $t$-distribution
  with $n-2$ degrees of freedom.
\end{prop}

\begin{proof}
  By the Central Limit Theorem, the conditional distribution
  \[ (\Bvh - \Bv) \Dlim \N{0}{\sigma^2 (\Xm^\T\Xm)^{-1}}. \]
  Since we are only concerned with testing the slope $b$, we would like to
  transform both the estimator, $\Bvh$, and the asymptotic covariance matrix,
  $\Var{\Bvh|\Xm}$, to isolate the parameter $b$.
  \begin{lemma} \label{lem:F_test}
    Given the hypotheses
    \begin{align*}
      H_0 &\colon G\Bv = \lambda, \\
      H_1 &\colon G\Bv \ne \lambda,
    \end{align*}
    where $G \in \R^{k \times p}$ is a matrix with $\rank G = k \le p$, and
    $\lambda \in \R^k$, the test statistic
    \[ S_n = \frac{(G\Bvh - \lambda)^\T (G (\Xm^\T \Xm)^{-1} G^\T)^{-1} (G\Bvh - \lambda)}
      {k \hat{\sigma}^2} \sim F_{k, n-p},\]
    under $H_0$. A proof is given in the \nameref{app:F_test}.
  \end{lemma}

  In our particular case, let
  \[ G = \begin{bmatrix} 0 & 1 \end{bmatrix} \quad (k = 1, p = 2) \]
  such that
  \[ G\Bv = \begin{bmatrix} 0 & 1 \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = b, \]
  and let $\lambda = 0$.
  Then, 
  \[ G\Bvh - \lambda = \hat{b}, \]
  and
  \begin{align*}
    (G (\Xm^\T \Xm)^{-1} G^\T)^{-1} &=
    \left(
    \begin{bmatrix} 0 & 1 \end{bmatrix}
    \left(
    \frac{1}{n S_x^2}
    \begin{bmatrix}
      \phantom{-}\Xsqbar  & -\Xnbar \\
      -\Xnbar              & 1
    \end{bmatrix}
    \right)
    \begin{bmatrix} 0 \\ 1 \end{bmatrix}
    \right)^{-1} \\
    &=
    n S_x^2
    \left(
    \begin{bmatrix} 0 & 1 \end{bmatrix}
    \begin{bmatrix}
      \phantom{-}\Xsqbar  & -\Xnbar \\
      -\Xnbar              & 1
    \end{bmatrix}
    \begin{bmatrix} 0 \\ 1 \end{bmatrix}
    \right)^{-1} \\
    &= n S_x^2.
  \end{align*}
  The test statistic is then
  \[ S_n = \frac{n S_x^2 \hat{b}^2}{\hat{\sigma}^2} \sim F_{1, n-2}. \]
  Since we would like to differentiate between positive and negative parameter
  values, we take the positive square root and define
  \[ T_n \coloneqq \sqrt{S_n} = \sqrt{n} \ddfrac{\hat{b}}{\sqrt{\frac{\hat{\sigma}^2}{S_x^2}}}. \]
  Since $F_{1, n-2} = t_{n-2}^2$, the test statistic, conditioned on $\Xm$, is
  distributed as
  \[ T_n | \Xm = \sqrt{n} \ddfrac{\hat{b}}{\sqrt{\frac{\hat{\sigma}^2}{S_x^2}}} 
    \sim t_{n-2}. \]

  Our test is then
  \[ \delta_\alpha = \indic{T_n > q_\alpha} \]
  where $q_\alpha$ is the $(1-\alpha)$-quantile of Student's $t$-distribution
  with $n-2$ degrees of freedom.
\end{proof}

% Appendix: move general hypothesis testing under normality assumption here?
\subsection*{Appendix to Problem \thesection} \label{app:F_test}

\begin{proof}[Proof of \Cref{lem:F_test}]
  Let $G \in \R^{k \times p}$ be a matrix such that $\rank G = k \le p$,
  and $\lambda \in \R^k$ be a vector. Consider the hypotheses
  \begin{align*}
    H_0 &\colon G\Bv = \lambda, \\
    H_1 &\colon G\Bv \ne \lambda.
  \end{align*}
  Since $\Bvh$ is normally distributed,
  \begin{alignat*}{3}
    \Bvh - \Bv &\sim \N{0}{\sigma^2 (\Xm^\T \Xm)^{-1}}, \\
    G(\Bvh - \Bv) &\sim \N{0}{\sigma^2 G (\Xm^\T \Xm)^{-1} G^\T}. \by{\Cref{thm:Bx}}
  \end{alignat*}
  Under the null hypothesis,
  \[ \frac{G\Bvh - \lambda}{\sqrt{\sigma^2}} \sim \N{0}{G (\Xm^\T \Xm)^{-1} G^\T}. \]

  \begin{lemma} \label{lem:norm2chi}
    If $X \in \R^k \sim \N{0}{\Sigma}$, then $X^\T \Sigma^{-1} X \sim \chi^2_k$.
  \end{lemma}

  By \Cref{lem:norm2chi},
  \[ \frac{(G\Bvh - \lambda)^\T (G (\Xm^\T \Xm)^{-1} G^\T)^{-1} (G\Bvh - \lambda)}{\sigma^2} 
    \sim \chi^2_k. \]
  To replace the unknown variance $\sigma^2$, multiply the expression by 
  $\sigma^2/\hat{\sigma}^2$ to get
  \[ \frac{(G\Bvh - \lambda)^\T (G (\Xm^\T \Xm)^{-1} G^\T)^{-1} (G\Bvh - \lambda)}{\hat{\sigma}^2} 
    \sim \ddfrac{\chi^2_k}{\left(\frac{\hat{\sigma}^2}{\sigma^2}\right)}. \]
  By Cochran's theorem, under the assumption that the errors are \iid\ Gaussian,
  \[ \frac{\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n-p}. \]
  If we divide each side again by $k$, our expression becomes
  \[ \frac{(G\Bvh - \lambda)^\T (G (\Xm^\T \Xm)^{-1} G^\T)^{-1} (G\Bvh - \lambda)}{k \hat{\sigma}^2} 
    \sim \ddfrac{\frac{\chi^2_k}{k}}{\frac{\chi^2_{n-p}}{n-p}}. \]
  \begin{definition} 
    Under the assumption $\chi^2_p \indep \chi^2_q$, Fisher's $F$-distribution
    with parameters $p, q \in \mathbb{N}$ is defined as
    \[ F_{p, q} \coloneqq \ddfrac{\frac{\chi^2_p}{p}}{\frac{\chi^2_q}{q}} \]
  \end{definition}
  Define the test statistic
  \[ S_n \coloneqq \frac{(G\Bvh - \lambda)^\T (G (\Xm^\T \Xm)^{-1} G^\T)^{-1} (G\Bvh - \lambda)}
    {k \hat{\sigma}^2} \]
  such that
  \[ S_n \sim F_{k, n-p}. \]
  Given $\Xm$, this distribution is \emph{pivotal}, so it does not rely on the
  unknown error variance, $\sigma^2$.
\end{proof}

\begin{theorem}
  If $T \sim t_n$ is a random variable drawn from a Student's
  $t$-distribution with $n$ degrees of freedom, then $T$ is related to
  Fisher's $F$-distribution by $T^2 = F_{1, n}$.
\end{theorem}

\begin{proof}
  Using the definition of a $\chi^2_k$ random variable as the sum of $k$
  standard normal random variables, we see that
  \begin{align*}
    t_n \coloneqq \ddfrac{\N{0}{1}}{\sqrt{\frac{\chi^2_n}{n}}} 
    = \ddfrac{\sqrt{\frac{\chi^2_1}{1}}}{\sqrt{\frac{\chi^2_n}{n}}} 
    &= \sqrt{\ddfrac{\frac{\chi^2_1}{1}}{\frac{\chi^2_n}{n}}} \\
    \implies t_n &= \sqrt{F_{1, n}} \\
    \implies t_n^2 &= F_{1, n}. \qedhere
  \end{align*}
\end{proof}

\clearpage
\section{Logistic Regression}
Consider \iid\ pairs $(\Xv_1, Y_1), \dots, (\Xv_n, Y_n)$, such that
\begin{itemize}
  \item $Y_i \in \{0, 1\}$ is a Bernoulli random variable,
  \item $\Xv_i \in \R^p$, and
  \item $\log \mleft(\ddfrac{\Prob{Y_i = 1 | \Xv_i}}{\Prob{Y_i = 0 | \Xv_i}} \mright) 
        = \Xv_i^\T \Bv$ for some $\Bv \in \R^p$.
\end{itemize}
Assume $\Xv_1$ has an unknown density, $f_X(x) = \Prob{\Xv_1 = x}$.

\subsection{The Parameter of the Output Variable}
\begin{prop}
  The probability that $Y_i = 1$, given $\Xv_i$, is the logistic function,
  \[ \Prob{Y_i = 1 | \Xv_i} = \frac{1}{1 + e^{-\Xv_i^\T \Bv}}. \]
\end{prop}

\begin{proof}
  Let $\Prob{Y_i = 1 | \Xv_i} = p_i$. Then, $\Prob{Y_i = 0 | \Xv_i} = 1 - p_i$,
  so our requirement on their ratio is
  \begin{align*}
    &\log \mleft(\ddfrac{\Prob{Y_i = 1 | \Xv_i}}{\Prob{Y_i = 0 | \Xv_i}} \mright) = \Xv_i^\T \Bv \\
    &\log \mleft(\frac{p_i}{1 - p_i}\mright) = \Xv_i^\T \Bv \\
    \implies& \frac{p_i}{1 - p_i} = e^{\Xv_i^\T \Bv} \\
    \implies& p_i = e^{\Xv_i^\T \Bv} - p_i e^{\Xv_i^\T \Bv} \\
    \implies& p_i (1 + e^{\Xv_i^\T \Bv}) = e^{\Xv_i^\T \Bv} \\
    \implies& p_i = \frac{e^{\Xv_i^\T \Bv}}{1 + e^{\Xv_i^\T \Bv}} \\
    \implies& p_i = \frac{e^{-\Xv_i^\T \Bv}}{e^{-\Xv_i^\T \Bv}} \times \frac{e^{\Xv_i^\T \Bv}}{1 + e^{\Xv_i^\T \Bv}} \\
    \implies& p_i = \Prob{Y_i = 1 | \Xv_i} = \frac{1}{1 + e^{-\Xv_i^\T \Bv}}. \qedhere
  \end{align*}
\end{proof}

\subsection{The Likelihood of the Model}
\begin{prop}
  The likelihood of the model is given by
  \[ \Li{\Xv, Y; \Bv} = \prod_{i=1}^n p_i(\Bv)^{Y_i} (1 - p_i(\Bv))^{1 - Y_i} f_X(\Xv_i). \]
\end{prop}

\begin{proof}
  The likelihood is defined as
  \begin{align*}
    \Li{\Xv, Y; \Bv} &\coloneqq \prod_{i=1}^n f_Y(Y_i) \\
                     &= \prod_{i=1}^n f_{Y|X}(Y_i | \Xv_i) f_X(\Xv_i) \\
    \implies \Li{\Xv, Y; \Bv} &= \prod_{i=1}^n p_i(\Bv)^{Y_i} (1 - p_i(\Bv))^{1 - Y_i} f_X(\Xv_i). \qedhere
  \end{align*}
\end{proof}

\subsection{The Maximum Likelihood Estimator}
\begin{prop}
  The MLE does not depend on the density of the design, $f_X$.
\end{prop}

\begin{proof}
  The maximum likelihood estimator of the parameter is
  \begin{align*}
    \Bvh_\mathrm{MLE} \coloneqq& \argmax_{\Bv \in \R^p} \Li{\Bv} \\
    \iff &\argmax_\Bv \prod_{i=1}^n p_i(\Bv)^{Y_i} (1 - p_i(\Bv))^{1 - Y_i} f_X(\Xv_i) \\
    \iff &\argmax_\Bv \prod_{i=1}^n p_i(\Bv)^{Y_i} (1 - p_i(\Bv))^{1 - Y_i} \times \prod_{i=1}^n f_X(\Xv_i) \\
    \intertext{Since the second term is not a function of $\Bv$,}
    \implies \Bvh_\mathrm{MLE} =& \argmax_\Bv \prod_{i=1}^n p_i(\Bv)^{Y_i} (1 - p_i(\Bv))^{1 - Y_i}
  \end{align*}
  is not a function of the density of $f_X$.
\end{proof}

We can further explore the likelihood expression and solve for the actual MLE.
Since
\[ \Bvh_\mathrm{MLE} \coloneqq \argmax_\Bv \Li{\Bv} 
                          \iff \argmax_\Bv \log \Li{\Bv} \]
we can work with the log-likelihood
\[ \Bvh_\mathrm{MLE} = \argmax_\Bv \sumi{i}{n} Y_i \log p_i(\Bv) + (1 - Y_i) \log (1 - p_i(\Bv)). \]
To maximize the log-likelihood, we take the gradient with respect to $\Bv_j$,
\begin{align*}
  \nabla_{\Bv_j} &= \sumi{i}{n} \left( \frac{Y_i}{p_i} - \frac{1 - Y_i}{1 - p_i} \right) \ppx{p_i}{\Bv_j} \\
  &= \sumi{i}{n} \left( \frac{Y_i(1 - p_i) - p_i(1 - Y_i)}{p_i(1 - p_i)} \right) \ppx{p_i}{\Bv_j} \\
  &= \sumi{i}{n} \left( \frac{Y_i - Y_i p_i - p_i + Y_i p_i}{p_i(1 - p_i)} \right) \ppx{p_i}{\Bv_j} \\
  &= \sumi{i}{n} \left( \frac{Y_i - p_i}{p_i(1 - p_i)} \right) \ppx{p_i}{\Bv_j}.
\end{align*}

\begin{prop}
  The derivative of the logistic function $p$ with respect to its argument is
  equal to $p(1 - p)$; \ie\ if
  \[ p(x) = \frac{1}{1 + e^{-x}}, \]
  then
  \[ \ddx{p}{x} = p(1 - p). \]
\end{prop}

\begin{proof}
  Let
  \[ p(x) = \frac{1}{1 + e^{-x}}, \quad x \in \R. \]
  Then, by the chain rule,
  \begin{align*}
    \ddx{p}{x} &= \frac{-1}{(1 + e^{-x})^2} \cdot -e^{-x} \\
    \implies \ddx{p}{x} &= \frac{e^{-x}}{(1 + e^{-x})^2}.
  \end{align*}
  The expression $1 - p$ is given by
  \begin{align*}
    1-p &= 1 - \frac{1}{1 + e^{-x}} \\
    &= \frac{1 + e^{-x}}{1 + e^{-x}} - \frac{1}{1 + e^{-x}} \\
    &= \frac{1 + e^{-x} - 1}{1 + e^{-x}} \\
    \implies 1 - p &= \frac{e^{-x}}{1 + e^{-x}},
  \end{align*}
  so the product $p(1-p)$ is
  \begin{align*}
    p(1-p) &= \frac{1}{1+e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}} \\
    \implies p(1-p) &= \ddx{p}{x} = \frac{e^{-x}}{(1 + e^{-x})^2}. \qedhere
  \end{align*}
\end{proof}
Using this fact, the derivative of $p_i(\Xv^\T\Bv)$ is
\begin{align*}
  \ppx{p_i}{\Bv_j} &= p_i(1 - p_i) \ppx{\Xv_i^\T\Bv}{\Bv_j} \\
  \implies \ppx{p_i}{\Bv_j} &= p_i(1 - p_i) X_{ij}.
\end{align*}
Then, our expression for the gradient of the log-likelihood is 
\begin{align*}
  \nabla_{\Bv_j} &= \sumi{i}{n} \left( \frac{Y_i - p_i}{p_i(1 - p_i)} \right) p_i(1 - p_i) X_{ij} \\
  \implies \nabla_{\Bv_j} &= \sumi{i}{n} \left( Y_i - p(\Xv_i; \Bv) \right) X_{ij} \\
  \implies \nabla_{\Bv_j} &= \left( \Yv - p(\Xv; \Bv) \right)^\T \Xv_j,
\end{align*}
where $\Yv, p(\Xm; \Bv), \Xv_j \in \R^n$.
Since $p$ is a transcendental function of $\Xv^\T\Bv$, we cannot find the zeros of
this equation exactly. In practice, its zeros are estimated by numerical means,
such as Newton's Method.

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
