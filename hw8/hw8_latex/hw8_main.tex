\documentclass[letterpaper, reqno]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[comma, compress, numbers, square]{natbib}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}  % for indicator function
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{mathrsfs}  % mathscr
\usepackage{mleftright}
\usepackage{enumerate}
\usepackage{array, multirow}
\usepackage[section, ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage[font=small, hypcap=true]{caption}                % link to top of figures and subfigures
\usepackage[font=small, hypcap=true, list=true]{subcaption}   % use for subfigures instead of {subfigure}

\usepackage{hyperref}
\usepackage{nameref}  % refer to unnumbered sections
\hypersetup{
    linktoc=all,    % link table of contents to sections
    colorlinks,
    allcolors=black,
    urlcolor=dblue,
  }

% Declare theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}{Proposition}[section]

\renewcommand{\qedsymbol}{\ensuremath{\blacksquare}}

% new environment for proofs of claims within proofs
\newenvironment{subproof}[1][\proofname]{%
  \renewcommand{\qedsymbol}{\ensuremath{\square}}%
  \begin{proof}[#1]%
}{%
  \end{proof}%
}

\numberwithin{equation}{section}

% lists as 1., 2., ...
\renewcommand{\labelenumi}{\theenumi.}

% Letter subsections
\renewcommand{\thesubsubsection}{\thesubsection(\alph{subsubsection})}

% number only specific equation in, say, align* environment
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}

% Algorithm
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}

%-------------------------------------------------------------------------------
%     {listings + color} package options
%-------------------------------------------------------------------------------
% Define colors for code
\usepackage{xcolor}
\definecolor{gray}{rgb}{0.6, 0.6, 0.6}
\definecolor{mauve}{rgb}{0.58, 0, 0.82}
\definecolor{dblue}{HTML}{0645AD}
\definecolor{lblue}{HTML}{1F77B4}

\usepackage{listings}
\lstset{
  language = Python,
  basicstyle = \scriptsize\ttfamily,
  numbers = left,
  numberstyle = \tiny\color{gray},
  stepnumber = 1,
  numbersep = 8pt,
  breaklines = true,
  keywordstyle = \bfseries\color{lblue},
  commentstyle = \color{gray},
  % stringstyle = \color{mauve}
}

% Place loose figures at actual top of page
% \makeatletter
%   \setlength{\@fptop}{0pt}
% \makeatother

%-------------------------------------------------------------------------------
% NO HYPHENATION
%-------------------------------------------------------------------------------
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

%==============================================================================
% General macros
%==============================================================================
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand{\T}{\top} % transpose symbol
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}} % use BOTH to cover all cases
\newcommand{\ith}{$i^\mathrm{th}$}
\newcommand{\jth}{$j^\mathrm{th}$}
% \newcommand{\norm}[1]{{\lVert {#1} \rVert}_2}

% Misc.
\newcommand{\ie}{\emph{i.e.\ }}
\newcommand{\eg}{\emph{e.g.\ }}

% Use for fractions with large terms
\newcommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}

% Derivatives
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ddx}[2]{\frac{\dd #1}{\dd #2}}
\newcommand{\dx}[1]{\,\dd #1} % for inside integration

% Probability helpers
\newcommand{\Prob}[1]{\mathbb{P}\mleft[#1\mright]}
\newcommand{\E}[1]{\mathbb{E}\mleft[#1\mright]}
\newcommand{\V}[1]{\mathbb{V}\mleft[#1\mright]}
\newcommand{\Var}[1]{\operatorname{Var}\mleft(#1\mright)}
\newcommand{\Cov}[1]{\operatorname{Cov}\mleft(#1\mright)}
\newcommand{\R}{\mathbb{R}}  % real numbers
\newcommand{\N}[2]{\mathcal{N}\mleft( #1, #2 \mright)}
\newcommand{\U}[2]{\mathcal{U}\mleft(\mleft[ #1, #2 \mright]\mright)}
\newcommand{\indep}{\perp \!\!\! \perp}  % "is independent from"
\newcommand{\nindep}{\centernot\indep}
\newcommand{\indic}[1]{\mathbbm{1}\!\mleft\{#1\mright\}} % indicator function
\newcommand{\iid}{i.i.d.}

\newcommand{\sumi}[2]{\sum_{#1=1}^{#2}}
\newcommand{\avg}[2]{\frac{1}{#2}\sumi{#1}{#2}}

\newcommand{\by}[1]{&\quad&\text{(#1)}}

\newcommand{\Alim}{\xrightarrow[n \to \infty]{\text{a.s.}}}
\newcommand{\Plim}{\xrightarrow[n \to \infty]{\mathbb{P}}}
\newcommand{\Dlim}{\xrightarrow[n \to \infty]{(d)}}

\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}

\newcommand{\Xnbar}{\overline{X}_n}
\newcommand{\Rnbar}{\overline{R}_n}
\newcommand{\Qnbar}{\overline{Q}_n}

% Vectors/Matrices
\newcommand{\Xm}{\vect{X}}
\newcommand{\Yv}{\vect{Y}}
\newcommand{\Bv}{\beta}
\newcommand{\Bvh}{\hat{\beta}}
\newcommand{\ve}{\varepsilon}
\newcommand{\Ebh}{\mathbb{E}[\Bvh]}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Ber}{Ber}


%%%% TITLE ----------------------------
\title[Homework 8 -- Problem \thesection]{18.650 Fundamentals of Statistics\\{\large Homework 8}}
\author{Bernie Roesler}
\date{\today}
%%%%

%%%%%%% BEGIN DOCUMENT ----------------
\begin{document}

\graphicspath{{./figures/}}

\maketitle

\section{Heteroscedastic Regression}
Let the characteristics $(X_i, y_i)$ of $n$ individuals $(i = 1, \dots, n)$ be
observed, where $y_i \in \R$ is the dependent variable, and $X_i \in \R^p$ is
the vector of deterministic explanatory variables. The goal is to estimate the
vector of coefficients $\Bv = (\beta_1, \dots, \beta_n)^\T$ in the linear
regression
\[ y_i = X_i^\T \Bv + \ve_i, \quad i = 1, \dots, n. \]
Assume that the model is heteroscedastic, \ie\ the error terms $\ve_i$
are not \iid, but $\ve = (\ve_1, \dots, \ve_n)^\T \in \R^n$ is Gaussian,
centered, with known invertible covariance matrix $\Sigma \in \R^{n \times n}$.
Let $\Xm \in \R^{n \times p}$ be the matrix
\[ \Xm = \begin{bmatrix} 
            \text{---} X_1^\T \text{---} \\
            \vdots \\ 
            \text{---} X_n^\T \text{---} 
          \end{bmatrix} 
\]
and $\Yv = (y_1, \dots, y_n)^\T \in \R^n$.
The linear regression is then
\[ \Yv = \Xm\Bv + \ve. \]

Consider the estimator
\[ \Bvh = \argmin_{\Bv \in \R^p} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv). \]

\subsection{The Estimator Under Homoscedasticity}
\begin{prop}
  If the errors are homoscedastic, \ie\ $\Sigma = \sigma^2 \vect{I}_n$, where $\sigma^2 > 0$, and
  $\vect{I}_n$ is the $n \times n$ identity matrix, then $\Bvh$ reduces to the least
  squares estimator
  \[ \beta_\mathrm{LSE} = (\Yv - \Xm\Bv)^\T(\Yv - \Xm\Bv). \]
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \Bvh &\coloneqq \argmin_\Bv (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \\
         &= \argmin_\Bv (\Yv - \Xm\Bv)^\T (\sigma^2 \vect{I}_n)^{-1} (\Yv - \Xm\Bv) \\
         &= \argmin_\Bv \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T \vect{I}_n (\Yv - \Xm\Bv) \by{inverse rules} \\
         &= \argmin_\Bv \frac{1}{\sigma^2} (\Yv - \Xm\Bv)^\T (\Yv - \Xm\Bv) \by{definition of identity} \\
    \implies \Bvh &= \argmin_\Bv (\Yv - \Xm\Bv)^\T (\Yv - \Xm\Bv) = \beta_\mathrm{LSE} \by{$\sigma^2 > 0$ and constant} \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\subsection{The Maximum Likelihood Estimator}
\begin{prop}
  The estimator $\Bvh$ is the maximum likelihood estimator.
\end{prop}

\begin{proof}
  If $\ve \sim \N{0}{\Sigma}$, then
  \[ f_Y(\Yv) = \frac{1}{\sqrt{(2\pi)^n \det \Sigma}}
      \exp \mleft[-\frac{1}{2} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright]. \]
  The likelihood is defined as
  \begin{align*}
    L(\Bv; \Sigma) &\coloneqq \prod_{i=1}^m f_Y(\Yv_i)  \\
    &= \frac{1}{\sqrt{(2\pi)^n \det \Sigma}}
      \exp \mleft[-\frac{1}{2} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright]
  \end{align*}
  To maximize the likelihood, we take the log-likelihood
  \begin{align*}
    \ell(\Bv; \Sigma) &= \log L(\Bv, \Sigma) \\
    &= \log \frac{1}{\sqrt{(2\pi)^n \det \Sigma}}
      \exp \mleft[-\frac{1}{2} (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright] \\
    &= -\frac{1}{2} \mleft[n \log 2\pi + \log \det \Sigma 
        + (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright] \\
    \intertext{When maximizing with respect to $\Bv$, we ignore terms that are
      not functions of $\Bv$,}
    \therefore \Bvh_\mathrm{MSE} &\coloneqq \argmax_\Bv \ell(\Bv; \Sigma) \\
    &= \argmax_\Bv -\frac{1}{2} \mleft[n \log 2\pi + \log \det \Sigma 
        + (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv) \mright] \\
    \implies \Bvh = \Bvh_\mathrm{MSE} &= \argmin_\Bv (\Yv - \Xm\Bv)^\T \Sigma^{-1} (\Yv - \Xm\Bv)   \qedhere
  \end{align*}
\end{proof}

\subsection{A Sufficient Condition on the Dependent Matrix}
\begin{prop}
  A sufficient condition on the dependent matrix $\Xm$ for the estimator $\Bv$ to
  be uniquely defined is
  \[ \rank \Xm = p \le n. \]
\end{prop}

\begin{proof}
  Given the data, the error in the linear regression
  \[ \Yv = \Xm\Bv + \ve \]
  is minimized by projecting $\Yv$ onto the column space of $\Xm$,
  $\mathscr{C}(\Xm)$, by multiplying each side by $\Xm^\T$,
  \[ \Xm^\T\Yv = \Xm^\T\Xm\Bvh \]
  such that
  \[ \Bvh = (\Xm^\T\Xm)^{-1} \Xm^\T \Yv. \]
  This expression is unique if the inverse $(\Xm^\T\Xm)^{-1}$ is unique and non-singular.
  Since
  \[ \rank \Xm^\T\Xm = \rank \Xm, \]
  for any matrix $X$, and $\Xm^\T \Xm \in \R^{p \times p}$, $\rank \Xm = p$ is
  a sufficient condition for the existence of the inverse, and thus for the
  uniqueness of $\Bvh$.
\end{proof}

\subsection{The Heteroscedastic Estimator}
\begin{prop}
  The optimal heteroscedastic estimator is
  \[ \Bvh = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Yv. \]
\end{prop}

\begin{proof}
  \begin{align*}
    \Bvh &\coloneqq \argmin_t (\Yv - \Xm t)^\T \Sigma^{-1} (\Yv - \Xm t) \\
    &= \Yv^\T \Sigma^{-1} \Yv + t^\T \Xm^\T \Sigma^{-1} \Xm t - 2t^\T
    \Xm^\T \Sigma^{-1} \Yv.
  \end{align*}
  The gradient with respect to $t$ is
  \begin{align*}
    \nabla_t &= 2 \Xm^\T \Sigma^{-1} \Xm t - 2 \Xm^\T \Sigma^{-1} \Yv \\
             &= 0 \quad \text{for } t = \Bvh \\
    \implies& \Xm^\T \Sigma^{-1} \Xm \Bvh = \Xm^\T \Sigma^{-1} \Yv \\
    \implies& \Bvh = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Yv
    \qedhere
  \end{align*}
\end{proof}

\begin{prop} \label{prop:BvN}
  The estimator is normally distributed, with mean and variance
  \[ \Bvh \sim \N{\Bv}{(\Xm^\T \Sigma^{-1} \Xm)^{-1}}. \]
\end{prop}

\begin{proof}
  From our expression for $\Bvh$, insert the expression for the data $\Yv
  = \Xm\Bv + \ve$
  \begin{align*}
    \Bvh &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Yv \\
         &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} (\Xm\Bv + \ve) \\
         &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} (\Xm^\T \Sigma^{-1} \Xm)\Bv 
          + (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \ve \\
         &= \Bv + (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \ve.
  \end{align*}
  Let
  \[ B = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \]
  be the error coefficient matrix, such that
  \[ \Bvh = \Bv + B\ve. \]

  \begin{theorem} \label{thm:Bx}
    If $X \sim \N{0}{\Sigma}$, then $a + BX \sim \N{a}{B \Sigma B^\T}$,
    for any constant vector $a$, matrix $B$ and invertible covariance matrix
    $\Sigma$.
  \end{theorem}

  Since the error is assumed to be normally distributed,
  $\ve \sim \N{0}{\Sigma}$, by Theorem~\ref{thm:Bx},
  \[ \Bvh \sim \N{\Bv}{B \Sigma B^\T}. \]
  The covariance matrix is then
  \begin{alignat*}{3}
    B \Sigma B^\T &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1} \Sigma
                    ((\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1})^\T \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T 
                    ((\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1})^\T \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T 
                    {\Sigma^{-1}}^\T \Xm {(\Xm^\T \Sigma^{-1} \Xm)^{-1}}^\T \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T 
                    {\Sigma^\T}^{-1} \Xm {(\Xm^\T \Sigma^{-1} \Xm)^\T}^{-1}
                    \by{${(A^{-1})}^\T = {(A^\T)}^{-1}$} \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} 
                     (\Xm^\T \Sigma^{-1} \Xm) (\Xm^\T \Sigma^{-1} \Xm)^{-1}
                    \by{$\Sigma^\T = \Sigma$} \\
                  &= (\Xm^\T \Sigma^{-1} \Xm)^{-1} 
  \end{alignat*}
  so the estimator $\Bvh$ is indeed distributed normally
  \[ \Bvh \sim \N{\Bv}{(\Xm^\T \Sigma^{-1} \Xm)^{-1}}. \qedhere \]
\end{proof}

\subsection{Bias and Quadratic Risk of the Estimator}
\begin{prop} \label{prop:bhat_nobias}
  The estimator $\Bvh$ is unbiased.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Bias}_\Bv(\Bvh) &\coloneqq \E{\Bvh - \Bv} \\
    &= \Ebh - \E{\Bv} \by{linearity of expectation} \\
    &= \Ebh - \Bv \\
    &= \Bv - \Bv \by{Proposition~\ref{prop:BvN}} \\
    &= 0 \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\begin{prop}
  The risk of the estimator $\Bvh$ is the trace of its covariance matrix.
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \mathrm{Risk}_\Bv(\Bvh) &\coloneqq \E{(\Bvh - \Bv)^\T(\Bvh - \Bv)} \\
    &= \E{\norm*{\Bvh - \Bv}_2^2} \\
    &= \E{\norm*{\Bvh - \Ebh + \Ebh - \Bv}_2^2} \\
    &= \E{\norm*{\Bvh - \Ebh}_2^2 
      + 2(\Bvh - \Ebh)^\T (\Ebh - \Bv)
      + \norm*{\Ebh - \Bv}_2^2} \\
    &= \E{\norm*{\Bvh - \Ebh}_2^2}
      + 2\E{(\Bvh - \Ebh)^\T (\Ebh - \Bv)} 
      + \E{\norm*{\Ebh - \Bv}_2^2}.
  \end{alignat*}
  The second term is identically zero.
  \[ 2\E{(\Bvh - \Ebh)^\T (\Ebh - \Bv)} = 2\E{(\Bvh - \Ebh)^\T}(\Ebh - \Bv) \]
  because $\Ebh - \Bv$ is deterministic, and 
  \[ \E{(\Bvh - \Ebh)^\T} = 0 \]
  by definition. The risk is then
  \[ \mathrm{Risk}_\Bv(\Bvh) = \E{\norm*{\Bvh - \Ebh}_2^2} + \E{\norm*{\Ebh - \Bv}_2^2}. \]
  The second term becomes
  \[ \E{\norm*{\Ebh - \Bv}_2^2} = \norm*{\Ebh - \Bv}_2^2 \]
  because the argument is deterministic. We recognize this expression as the
  norm of the bias.
  \[ \norm*{\Ebh - \Bv}_2^2 = \norm*{\mathrm{Bias}_\Bv(\Bvh)}_2^2. \]
  To evaluate the first term we apply the following theorem,
  \begin{theorem} \label{thm:normnorm}
    If $X \sim \N{0}{\Sigma}$, then $\norm*{X}_2^2 = \tr \Sigma$.
  \end{theorem}
  to see that
  \begin{align}
    \E{\norm*{\Bvh - \Ebh}_2^2} &= \E{\tr (\Xm^\T \Sigma^{-1} \Xm)^{-1}} \\
    &= \tr (\Xm^\T \Sigma^{-1} \Xm)^{-1} \\
    &= \tr \Var{\Bvh}.
  \end{align}
  \[ \therefore \mathrm{Risk}_\Bv(\Bvh) = \tr \Var{\Bvh} + \norm*{\mathrm{Bias}_\Bv(\Bvh)}_2^2. \]
  As shown by the proof of Proposition~\ref{prop:bhat_nobias}, $\Bvh$ is
  unbiased, so
  \begin{align*}
    \mathrm{Risk}_\Bv(\Bvh) &= \tr \Var{\Bvh}  \\
    \implies \mathrm{Risk}_\Bv(\Bvh) &= \tr (\Xm^\T \Sigma^{-1} \Xm)^{-1}.
    \qedhere
  \end{align*}
\end{proof}

\subsection*{Appendix}
\begin{quote}
  \emph{Q: What is the prediction error of the estimator?}
\end{quote}

\begin{prop}
  The prediction error, defined as the expected value of the sum of squared
  residuals, is
  \[ \E{\norm*{\Yv - \Xm\Bvh}_2^2} = \tr P' \Sigma P', \]
  where $P'$ is the projection matrix onto the nullspace of $\Xm^\T$,
  $\mathscr{N}(\Xm^\T)$.
\end{prop}

\begin{proof}
  The residuals can be expressed as
  \begin{align*}
    \Yv - \Xm\Bvh &= \Yv - \Xm B \Yv \\
    &= (\vect{I}_n - \Xm B) \Yv
  \end{align*}
  where
  \[ B = (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1}. \]
  Let $P \in \R^{n \times n}$ be the projection matrix that maps onto the column
  space of $\Xm$, $\mathscr{C}(\Xm)$:
  \[ P = \Xm B =  \Xm (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1}. \]
  Then, let $P' \in \R^{n \times n}$ be the projection matrix onto
  $\mathscr{N}(\Xm^\T)$, which is orthogonal to $\mathscr{C}(\Xm)$.
  \[ P' = \vect{I}_n - \Xm B 
        = \vect{I}_n - \Xm (\Xm^\T \Sigma^{-1} \Xm)^{-1} \Xm^\T \Sigma^{-1}. \]
  The residuals are then
  \[ \Yv - \Xm\Bvh = (\vect{I}_n - P)\Yv = P'\Yv. \]
  We can substitute the expression for the $\Yv$ to obtain
  \begin{align*}
    \Yv - \Xm\Bvh &= P' \Yv \\
    &= P' (\Xm\Bv + \ve) \\
    &= P'\Xm\Bv + P'\ve.
  \end{align*}
  The projection of $\Xm$ onto the nullspace of its own transpose $P'\Xm\Bv$ is
  always zero,
  \[ \implies \Yv - \Xm\Bvh = P'\ve. \]
  The norm of the residuals is then
  \begin{alignat*}{3}
    \norm*{\Yv - \Xm\Bvh}_2^2 = \norm*{P'\Yv}_2^2 &= \norm*{P'\ve}_2^2 \\
    &= \norm*{\N{0}{P'\Sigma P'^\T}}_2^2 \by{Theorem~\ref{thm:normnorm}} \\
    &= \norm*{\N{0}{P'\Sigma P'}}_2^2 \by{$P'^\T = P'$} \\
    &= \tr P'\Sigma P' \by{Theorem~\ref{thm:Bx}} \\
    \implies \norm*{\Yv - \Xm\Bvh}_2^2 &= \tr P'\Sigma P' \tag*{\qedhere}
  \end{alignat*}
\end{proof}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
