\documentclass[letterpaper, reqno]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[comma, compress, numbers, square]{natbib}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}  % for indicator function
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{mleftright}
\usepackage{enumerate}
\usepackage{array, multirow}
\usepackage[section, ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage[font=small, hypcap=true]{caption}                % link to top of figures and subfigures
\usepackage[font=small, hypcap=true, list=true]{subcaption}   % use for subfigures instead of {subfigure}

\usepackage{hyperref}
\usepackage{nameref}  % refer to unnumbered sections
\hypersetup{
    linktoc=all,    % link table of contents to sections
    colorlinks,
    allcolors=black,
    urlcolor=dblue,
  }

% Declare theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}{Proposition}[section]

\renewcommand{\qedsymbol}{\ensuremath{\blacksquare}}

% new environment for proofs of claims within proofs
\newenvironment{subproof}[1][\proofname]{%
  \renewcommand{\qedsymbol}{\ensuremath{\square}}%
  \begin{proof}[#1]%
}{%
  \end{proof}%
}

\numberwithin{equation}{section}

% lists as 1., 2., ...
\renewcommand{\labelenumi}{\theenumi.}

% Letter subsections
\renewcommand{\thesubsubsection}{\thesubsection(\alph{subsubsection})}

% number only specific equation in, say, align* environment
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}

% Algorithm
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}

%-------------------------------------------------------------------------------
%     {listings + color} package options
%-------------------------------------------------------------------------------
% Define colors for code
\usepackage{xcolor}
\definecolor{gray}{rgb}{0.6, 0.6, 0.6}
\definecolor{mauve}{rgb}{0.58, 0, 0.82}
\definecolor{dblue}{HTML}{0645AD}
\definecolor{lblue}{HTML}{1F77B4}

\usepackage{listings}
\lstset{
  language = Python,
  basicstyle = \scriptsize\ttfamily,
  numbers = left,
  numberstyle = \tiny\color{gray},
  stepnumber = 1,
  numbersep = 8pt,
  breaklines = true,
  keywordstyle = \bfseries\color{lblue},
  commentstyle = \color{gray},
  % stringstyle = \color{mauve}
}

% Place loose figures at actual top of page
% \makeatletter
%   \setlength{\@fptop}{0pt}
% \makeatother

%-------------------------------------------------------------------------------
% NO HYPHENATION
%-------------------------------------------------------------------------------
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

%==============================================================================
% General macros
%==============================================================================
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\newcommand{\T}{\top} % transpose symbol
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}} % use BOTH to cover all cases
\newcommand{\ith}{$i^\mathrm{th}$}
\newcommand{\jth}{$j^\mathrm{th}$}

% Misc.
\newcommand{\ie}{\emph{i.e.\ }}
\newcommand{\eg}{\emph{e.g.\ }}

% Use for fractions with large terms
\newcommand{\ddfrac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}

% Derivatives
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ddx}[2]{\frac{\dd #1}{\dd #2}}
\newcommand{\dx}[1]{\,\dd #1} % for inside integration

% Probability helpers
\newcommand{\Prob}[1]{\mathbb{P}\mleft[#1\mright]}
\newcommand{\E}[1]{\mathbb{E}\mleft[#1\mright]}
\newcommand{\V}[1]{\mathbb{V}\mleft[#1\mright]}
\newcommand{\Var}[1]{\operatorname{Var}\mleft(#1\mright)}
\newcommand{\Cov}[1]{\operatorname{Cov}\mleft(#1\mright)}
\newcommand{\R}{\mathbb{R}}  % real numbers
\newcommand{\N}[2]{\mathcal{N}\mleft( #1, #2 \mright)}
\newcommand{\U}[2]{\mathcal{U}\mleft(\mleft[ #1, #2 \mright]\mright)}
\newcommand{\indep}{\perp \!\!\! \perp}  % "is independent from"
\newcommand{\nindep}{\centernot\indep}
\newcommand{\indic}[1]{\mathbbm{1}\!\mleft\{#1\mright\}} % indicator function
\newcommand{\iid}{i.i.d.}

\newcommand{\sumi}[2]{\sum_{#1=1}^{#2}}
\newcommand{\avg}[2]{\frac{1}{#2}\sumi{#1}{#2}}

\newcommand{\by}[1]{&\quad&\text{(#1)}}

\newcommand{\Alim}{\xrightarrow[n \to \infty]{\text{a.s.}}}
\newcommand{\Plim}{\xrightarrow[n \to \infty]{\mathbb{P}}}
\newcommand{\Dlim}{\xrightarrow[n \to \infty]{(d)}}

\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}

\newcommand{\Xnbar}{\overline{X}_n}
\newcommand{\Rnbar}{\overline{R}_n}
\newcommand{\Qnbar}{\overline{Q}_n}

% Vectors/Matrices
\newcommand{\Xm}{\vect{X}}
\newcommand{\Bv}{\beta}
\newcommand{\Bvh}{\hat{\beta}}
\newcommand{\ve}{\varepsilon}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\DeclareMathOperator{\Ber}{Ber}
\DeclareMathOperator{\Exp}{Exp}
% \DeclareMathOperator{\Var}{Var}
% \DeclareMathOperator{\Cov}{Cov}

%%%% TITLE ----------------------------
\title[Homework 8 -- Problem \thesection]{18.650 Fundamentals of Statistics\\{\large Homework 8}}
\author{Bernie Roesler}
\date{\today}
%%%%

%%%%%%% BEGIN DOCUMENT ----------------
\begin{document}

\graphicspath{{./figures/}}

\maketitle

\section{Heteroscedastic Regression}
Let the characteristics $(X_i, y_i)$ of $n$ individuals $(i = 1, \dots, n)$ be
observed, where $y_i \in \R$ is the dependent variable, and $X_i \in \R^p$ is
the vector of deterministic explanatory variables. The goal is to estimate the
vector of coefficients $\Bv = (\beta_1, \dots, \beta_n)^\T$ in the linear
regression
\[ y_i = X_i^\T \Bv + \ve_i, \quad i = 1, \dots, n. \]
Assume that the model is heteroscedastic, \ie\ the error terms $\ve_i$
are not \iid, but $\ve = (\ve_1, \dots, \ve_n)^\T$ is Gaussian,
centered, with known invertible covariance matrix $\Sigma$. Let $\Xm \in \R^{n
  \times p}$ be the matrix
\[ \Xm = \begin{bmatrix} - X_1^\T - \\ \vdots \\ - X_n^\T - \end{bmatrix} \]
and $Y = (y_1, \dots, y_n)^\T$.

Consider the estimator
\[ \Bvh = \argmin_{\Bv \in \R^p} (Y - \Xm\Bv)^\T \Sigma^{-1} (Y - \Xm\Bv). \]

\subsection{The Estimator Under Homoscedasticity}
\begin{prop}
  If the errors are homoscedastic, \ie\ $\Sigma = \sigma^2 I_n$, where $\sigma^2 > 0$, and
  $I_n$ is the $n \times n$ identity matrix, then $\Bvh$ reduces to the least
  squares estimator
  \[ \beta_\mathrm{LSE} = (Y - \Xm\Bv)^\T(Y - \Xm\Bv). \]
\end{prop}

\begin{proof}
  \begin{alignat*}{3}
    \Bvh &\coloneqq \argmin_{\Bv \in \R^p} (Y - \Xm\Bv)^\T \Sigma^{-1} (Y - \Xm\Bv) \\
         &= \argmin_{\Bv \in \R^p} (Y - \Xm\Bv)^\T (\sigma^2 I_n)^{-1} (Y - \Xm\Bv) \\
         &= \argmin_{\Bv \in \R^p} \frac{1}{\sigma^2} (Y - \Xm\Bv)^\T I_n (Y - \Xm\Bv) \by{inverse rules} \\
         &= \argmin_{\Bv \in \R^p} \frac{1}{\sigma^2} (Y - \Xm\Bv)^\T (Y - \Xm\Bv) \by{definition of identity} \\
    \implies \Bvh &= \argmin_{\Bv \in \R^p} (Y - \Xm\Bv)^\T (Y - \Xm\Bv) = \beta_\mathrm{LSE} \by{$\sigma^2 > 0$ and constant} \qedhere
  \end{alignat*}
\end{proof}

\subsection{The Maximum Likelihood Estimator}
\begin{prop}
  The estimator $\Bvh$ is the maximum likelihood estimator.
\end{prop}

\begin{proof}
  If $\ve \sim \N{0}{\Sigma}$, then
  \[ f(Y) = \frac{1}{(2\pi)^\frac{n}{2} \det \Sigma^{\frac{1}{2}}}
      \exp \mleft[-\frac{1}{2} (Y - \Xm\Bv)^\T \Sigma^{-1} (Y - \Xm\Bv) \mright]. \]
  The likelihood is defined as
  \[ L(Y; \Xm, \Bv, \Sigma) = L(\Bv, \Sigma) =  \prod_{i=1}^n f(Y_i). \]
  To maximize the likelihood, we take the log-likelihood
  \begin{align*}
    \ell(\Bv, \Sigma) &= \log L(\Bv, \Sigma) \\
    &= \log \prod_{i=1}^n f(Y_i) \\
    &= \sumi{i}{n} \log f(Y_i) \\
    &= \log \frac{1}{(2\pi)^\frac{n}{2} \det \Sigma^{\frac{1}{2}}}
      \exp \mleft[-\frac{1}{2} (Y - \Xm\Bv)^\T \Sigma^{-1} (Y - \Xm\Bv) \mright] \\
    &= -\frac{n}{2} \log 2\pi - \frac{1}{2} \log \det \Sigma 
        - \frac{1}{2} (Y - \Xm\Bv)^\T \Sigma^{-1} (Y - \Xm\Bv) \\
    \intertext{When maximizing with respect to $\Bv$, we can ignore terms
      that are not functions of $\Bv$,}
    \therefore \Bvh_\mathrm{MSE} &\coloneqq \argmax_{\Bv \in \R^p} \ell(\Bv, \Sigma) \\
    &= \argmax_{\Bv \in \R^p} - \frac{1}{2} (Y - \Xm\Bv)^\T \Sigma^{-1} (Y - \Xm\Bv) \\
    \implies \Bvh = \Bvh_\mathrm{MSE} &= \argmin_{\Bv \in \R^p} (Y - \Xm\Bv)^\T \Sigma^{-1} (Y - \Xm\Bv)   \qedhere
  \end{align*}
\end{proof}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
